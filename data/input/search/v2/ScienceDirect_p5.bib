@article{LI20223959,
title = {Energy-aware service composition in multi-Cloud},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {7},
pages = {3959-3967},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001422},
author = {Jianmin Li and Ying Zhong and Shunzhi Zhu and Yongsheng Hao},
keywords = {Energy-aware, Multi-Cloud, Scheduling method, Service composition},
abstract = {Service composition is widely used in multiple scenarios to meet users’ various demands. In a multi-Cloud environment (MCE), a composite request (service request) needs atomic services (service candidates) located in multiple clouds with various functions. Service composition composes atomic services from multiple clouds together as a new service. Prior work focused on how to compose services and ignored energy consumption caused by the execution of atomic services. In this paper, we examine an energy-aware heuristic for service composition (EASC) under a multi-Cloud environment to reduce energy consumption from executing atomic services. To meet our requirements, we try to compose services in one cloud to reduce energy consumption for transferring files between atomic services. Beyond that, we also consider the influence of the split-point positions to energy consumption and other metrics. Simulation results show that our proposed method has shown good performance in reducing execution time and energy consumption.}
}
@article{IDREES2021101444,
title = {An approximate-computing empowered green 6G downlink},
journal = {Physical Communication},
volume = {49},
pages = {101444},
year = {2021},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2021.101444},
url = {https://www.sciencedirect.com/science/article/pii/S1874490721001816},
author = {Maryam Idrees and Mohammed {Manzar Maqbool} and Muhammad Khurram Bhatti and M. Mahboob Ur Rahman and Rehan Hafiz and Muhammad Shafique},
keywords = {Approximate computing, Low power, Digital filters, Adders, Multipliers, Communication software/hardware},
abstract = {Approximate computing (AC) is an emerging embedded computing paradigm whereby accurate arithmetic units (i.e., adders and multipliers) of a computing platform (e.g., CPU, FPGA, ASIC etc.) are replaced by their inexact counterparts. For the applications (e.g., voice, images etc.) where the error induced by inaccurate arithmetic units remains within tolerable limits, AC is a promising technique because it leads to the design of energy-efficient computing hardware that occupies less circuit area, and has lower latency as well. This work is the first to investigate the feasibility of the AC for single-antenna and dual-antenna 6G downlink. Specifically, we consider the AC-empowered transceiver design of a 6G downlink whereby the state-of-the-art approximate/inexact arithmetic units are leverage to implement the pulse shaping filters (at the base station (BS) side) and decoders/equalizers (at the user equipment (UE) side). For simulation purpose, images and randomly generated bits are transmitted using M-ary phase shift keying scheme. To quantify the loss in arithmetic accuracy due to the AC, bit error rate (BER), structural similarity index (SSIM) and correlation coefficient (CC) are utilized as performance metrics; while to quantify the energy-efficiency benefit of the proposed AC techniques, dynamic power and on-chip power are utilized as performance metrics. Monte-Carlo results indicate up to 87% savings in dynamic power and very reasonable arithmetic accuracy (with SSIM above 93% and a CC of 99%), due to the proposed AC techniques.}
}
@incollection{SHRIVASTAVA2006514,
title = {Sustainability Implications of Ubiquitous Computing Environment},
editor = {J.A. Mwakali and G. Taban-Wani},
booktitle = {Proceedings from the International Conference on Advances in Engineering and Technology},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {514-521},
year = {2006},
isbn = {978-0-08-045312-5},
doi = {https://doi.org/10.1016/B978-008045312-5/50056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008045312550056X},
author = {Manish Shrivastava and Donart A Ngarambe},
keywords = {Sustainable Development, Ubiquitous Computing, Ubiquitous Society},
abstract = {ABSTRACT
In ubiquitous computing environment, a person might interact with hundreds of computers at a time, each invisibly embedded in the environment and wirelessly communicating with each other. The vision of ubiquitous computing is to make computers available in everyday objects. It is a new kind of relationship of people to computers. As progress in ubiquitous computing increases, the significant opportunities and threats will also be involved towards social and environmental sustainability. There are many issues regarding the sustainability like: How will ubiquitously available computing systems affect the ecological balance? What happens to society when there are hundreds of invisible microcomputers to each other? What are the implications on social sustainability? This paper explores theoretical issues of social, environmental and ethical implications of Ubiquitous Computing Environment on Sustainable Development.}
}
@article{FONTANADENARDIN2021102858,
title = {On revisiting energy and performance in microservices applications: A cloud elasticity-driven approach},
journal = {Parallel Computing},
volume = {108},
pages = {102858},
year = {2021},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2021.102858},
url = {https://www.sciencedirect.com/science/article/pii/S0167819121001010},
author = {Igor {Fontana de Nardin} and Rodrigo {da Rosa Righi} and Thiago Roberto {Lima Lopes} and Cristiano {André da Costa} and Heon Young Yeom and Harald Köstler},
keywords = {Elasticity, Energy, Performance, Cloud computing, Microservices},
abstract = {Monolithic applications are a subject that includes several knowledge areas. Sometimes it can be a challenge to optimize CPU or IO requirements because it is not trivial to recognize the problem itself and improve it. There are many approaches to resolve this situation, where a trending one is the microservices. As a variant of the service-oriented architecture, microservices is a technique that arranges an application as a collection of loosely coupled services. This decomposition enables better software management in cloud-based environments since we can replicate each part individually using cloud elasticity to avoid execution bottlenecks. Also, since elasticity mitigates resource overprovisioning, it favors better energy consumption: the cloud owner can redistribute finite available resources among different tenants, and users can pay less to use the infrastructure. However, elasticity tuning is not trivial and depends on several factors, such as user experience, application architecture, and parameter modeling. Today, we observe a lack of initiatives in the literature that address both performance and energy perspectives to support the execution of microservices applications in the cloud. Concerning this context, this article introduces Elergy as a lightweight proactive elasticity model that provides resource reorganization for a cloud-based microservices application. Its differential approach appears in improving energy consumption by periodically handling the most appropriate amount of resources to execute an application while maintaining or yet improving the performance of CPU-bound applications. Elergy performs these functions proactively, in such a way of preventing future problems related to either resource under- or overprovisioning. The results showed energy consumption reduction and a competitive cost (application time x consumed resources) when comparing Elergy with a non-elastic scenario. Elergy obtained savings from 1.93% to 27.92% for energy consumption.}
}
@article{MAY20231085,
title = {Leveraging Dual-Energy Computed Tomography to Improve Emergency Radiology Practice},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {1085-1096},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0033838923001380},
author = {Craig May and Aaron Sodickson},
keywords = {Dual-energy CT, Emergency radiology, Intracranial hemorrhage, Occult fracture, Cholelithiasis, Pyelonephritis, Bowel ischemia, Gastrointestinal bleeding}
}
@article{SHAHSAVAR2023293,
title = {Multi-objective energy and exergy optimization of hybrid building-integrated heat pipe photovoltaic/thermal and earth air heat exchanger system using soft computing technique},
journal = {Engineering Analysis with Boundary Elements},
volume = {148},
pages = {293-304},
year = {2023},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2022.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S0955799722004854},
author = {Amin Shahsavar and Müslüm Arıcı},
keywords = {Energy analysis, Exergy analysis, Heat pipe, Multi-objective optimization, Photovoltaic/thermal system, Earth-air heat exchanger},
abstract = {This research is dedicated to the numerical assessment of the performance of a novel system that uses solar and geothermal energy at the same time, which is capable of heating/cooling the outside air before it enters the air conditioning system of a building and supplying the required electricity for the building. The desired system consists of a photovoltaic/thermal-heat pipe (PVT-heat pipe) unit and an earth-air heat exchanger (EAHE). To cool the outside air in hot seasons of the year, the air is passed through the EAHE system and the exhaust air from building is used to cool the photovoltaic panels. Outside air heating in cold seasons is also possible by passing air through the PVT-heat pipe and EAHE systems. The genetic algorithm-based two-objective optimization is used to determine the necessary conditions to simultaneously maximize the annual useful energy and exergy yields of the hybrid system. The performance metrics of the optimal system are compared with the corresponding values of the PVT-EAHE unit. Outcomes revealed that the annual useful energy output of the PVT-heat pipe-EAHE system (93925.6kWh) is less than the PVT-EAHE system (96448.6kWh), while the PVT-heat pipe-EAHE system is able to produce more exergy (10904.5kWh) than the PVT-EAHE system (10015.5kWh).}
}
@article{BOTTCHER2023995,
title = {Dual-Energy Computed Tomography in Cardiac Imaging},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {995-1009},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0033838923001306},
author = {Benjamin Böttcher and Emese Zsarnoczay and Akos Varga-Szemes and Uwe Joseph Schoepf and Felix G. Meinel and Marly {van Assen} and Carlo N. {De Cecco}},
keywords = {Dual-energy computed tomography, Cardiovascular imaging, Coronary plaque imaging, Myocardial perfusion imaging, Myocardial tissue characterization, Photon counting}
}
@article{PENG2023100877,
title = {Modeling and optimization of collaborative computing in regional multi-energy systems for energy Internet},
journal = {Sustainable Computing: Informatics and Systems},
volume = {39},
pages = {100877},
year = {2023},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100877},
url = {https://www.sciencedirect.com/science/article/pii/S221053792300032X},
author = {Yuhuai Peng and Jing Wang and Chunyang Hu and Yang Song and Qiming Li and Audithan Sivaraman},
keywords = {Multi-energy system modeling, Integrated planning and management, Collaborative computing, Task scheduling optimization, Energy Internet},
abstract = {Multi-energy systems (MES) exploit advanced physical information technology and innovative management pattern to achieve collaborative control of multiple heterogeneous energy. The refined control of MES bursts massive delay-sensitive computing tasks, requiring precise system modeling and collaborative computing strategies. However, the unbalanced spatial and temporal distribution of resources and real-time requirements of tasks further increase the difficulty of computing. To address these challenges, a precise modeling and optimization method of collaborative computing in MES is proposed. First, an integrated system framework for regional multi-energy systems (RMES) in northeastern China is designed. Then, the collaborative computing problem with energy and delay constraints in RMES is modeled. It is proved that the problem can be transformed into a joint convex and nonconvex optimization problem. Moreover, an improved simulated annealing-based joint resources optimization (ISA-JRO) scheme is proposed. Finally, extensive simulation results show that ISA-JRO significantly improves the computational resource utilization ratio and reduces the total cost in MES. ISA-JRO reduces at least 25 % of the total cost compared with the traditional methods.}
}
@article{LONG2022501,
title = {Energy-efficient VM opening algorithms for real-time workflows in heterogeneous clouds},
journal = {Neurocomputing},
volume = {483},
pages = {501-514},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.08.145},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221016088},
author = {Saiqin Long and Xin Dai and Tingrui Pei and Jiasheng Cao and Hiroo Sekiya and Young-June Choi},
keywords = {Dynamic voltage and frequency scaling (DVFS), Real-time workflows, Energy consumption, VM opening},
abstract = {Minimizing energy consumption is a critical challenge for real-time workflows, particularly in heterogeneous cloud computing systems. State-of-the-art algorithms aim to minimize the energy consumed for processing such applications by choosing virtual machines (VMs) to shut down from all opened VMs (i.e., VM merging). However, such VM merging through an “on-to-close” approach usually incurs high computational complexity. This paper proposes an energy-efficient VM opening (EEVO) algorithm that is capable of choosing VMs to turn on from all closed VMs while satisfying the real-time constraint of applications. Considering that there are slacks that can be eliminated or reduced between adjacently scheduled tasks after using the EEVO algorithm, a dynamic scaling down EEVO algorithm (DEEVO) is further proposed. DEEVO is implemented by scaling down the frequency of VMs executing each task based on the dynamic voltage and frequency scaling (DVFS) technique. Experimental results demonstrate that, with the above-mentioned improvements, DEEVO achieves lower energy consumption for real-time workflows than state-of-the-art algorithms do. In addition, DEEVO outperforms state-of-the-art algorithms in the computational efficiency of accomplishing task scheduling.}
}
@article{JIANG20231866,
title = {The Value of Dual-Energy Computed Tomography Angiography-Derived Parameters in the Evaluation of Clot Composition},
journal = {Academic Radiology},
volume = {30},
number = {9},
pages = {1866-1873},
year = {2023},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2022.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S107663322200664X},
author = {Jingxuan Jiang and Hongmei Gu and Minda Li and Ye Hua and Sijia Wang and Lisong Dai and Yuehua Li},
keywords = {Stroke, Computed tomography, Dual-energy, Angiography, Thrombus compositions},
abstract = {Objectives
We aimed to assess the value of dual-energy computed tomography angiography (DE-CTA) derived parameters as a quantitative biomarker of thrombus composition in acute ischemic stroke (AIS).
Methods
AIS patients who underwent DE-CTA before thrombectomy between August 2016 and September 2022 were included in this study. We assessed the relative proportion of red blood cells (RBCs) and the fibrin/platelet ratio (F/P) of the retrieved clots and categorized the clots as RBC-dominant (RBCs > F/P) or F/P-dominant (F/P > RBCs). The thrombus based parameters were measured on polyenergetic images (PEI), virtual monoenergetic (VM), virtual non-contrast (VNC), iodine concentration (IC), and effective atomic number (Zeff) images respectively, and the slope of the spectral Hounsfield unit curve (λHU) was calculated. These parameters were compared in the DE-CTA images of RBC- and F/P-dominant thrombi. The diagnostic performance of the parameters was analyzed using the ROC curve. Correlations between thrombus composition and DE-CTA-derived parameters were assessed.
Results
The retrieved clots in 54 of 88 patients (61.36%) were RBC-dominant. The RBC-dominant thrombi showed significantly higher VNC values and lower IC, λHU, and Zeff values than the F/P-dominant thrombi (p < 0.05). The CT density measured on IC images showed the largest AUC value (AUC, 0.94; sensitivity, 77.78%; specificity, 100.00%). The Spearman rank-order correlation coefficient values showed that CT density measured on IC images of the thrombus showed the strongest association with the proportion of RBCs (r = -0.64, p < 0.001) and F/P (r = 0.65, p < 0.001).
Conclusions
DE-CTA-derived parameters, especially the CT density measured on IC images, could be associated with thrombus composition and allow for personalized thrombectomy strategies.}
}
@article{IGNATIUS2023100449,
title = {Radiotherapy planning of spine and pelvis using single-energy metal artifact reduction corrected computed tomography sets},
journal = {Physics and Imaging in Radiation Oncology},
volume = {26},
pages = {100449},
year = {2023},
issn = {2405-6316},
doi = {https://doi.org/10.1016/j.phro.2023.100449},
url = {https://www.sciencedirect.com/science/article/pii/S2405631623000404},
author = {Daliya Ignatius and Zaid Alkhatib and Pejman Rowshanfarzad and Simon Goodall and Mounir Ibrahim and Andrew Hirst and Riley Croxford and Joshua Dass and Mahsheed Sabet},
keywords = {Single-Energy Metal Artifact Reduction (SEMAR), Metal artifacts, Computed Tomography (CT), Radiotherapy treatment planning, 3D printing},
abstract = {Metal artifacts produce incorrect Hounsfield units and impact treatment planning accuracy. This work evaluates the use of single-energy metal artifact reduction (SEMAR) algorithm for treatment planning by comparison to manual artifact overriding. CT datasets of in-house 3D-printed spine and pelvic phantoms with and without metal insert(s) and two treated patients with metal implants were analysed. CT number accuracy improved with the use of SEMAR filter: root mean square deviation (RMSD) from reference (without metal) reduced by 35.4 in spine and 98.8 in hip. The plan dose volume histograms (DVHs) and dosimetric measurements showed comparable results. SEMAR reconstruction improved planning efficiency.}
}
@article{2005203,
title = {557 Cost effective prediction of sustained response to IFN α-2B or Peg-IFN α-2B plus ribavirin computing infected cells dynamics by early ALT and HCV-RNA decline},
journal = {Journal of Hepatology},
volume = {42},
pages = {203},
year = {2005},
issn = {0168-8278},
doi = {https://doi.org/10.1016/S0168-8278(05)81968-3},
url = {https://www.sciencedirect.com/science/article/pii/S0168827805819683}
}
@article{LI2020110245,
title = {Assessment analysis of green development level based on S-type cloud model of Beijing-Tianjin-Hebei, China},
journal = {Renewable and Sustainable Energy Reviews},
volume = {133},
pages = {110245},
year = {2020},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2020.110245},
url = {https://www.sciencedirect.com/science/article/pii/S1364032120305347},
author = {Ye Li and Yiyan Chen and Qun Li},
keywords = {Green development Cloud model Comprehensive evaluation Entropy method Index system Regional synergy},
abstract = {As the environment suffers from various forms of destruction, especially irreversible destruction, the idea of green development is getting more and more attention by all countries over the world. Nevertheless, for the coexistence of fuzziness of evaluation grade thresholds' selecting and evaluation indexes’ random error, the existing research on green development evaluation has not been involved. Therefore, aiming at this practical problem, this study proposes an evaluation method based on S-type cloud model, which be able to deal with the uncertainties of fuzziness and randomness concurrently. Furthermore, this method is adopted to assess the green development level of Beijing-Tianjin-Hebei (BTH) region, including 13 cities in China. The evaluation index system is established in terms of five dimensions: living environment, pollutant treatment and utilization, ecological efficiency, economic growth and innovative potential. The empirical results show that the overall level of pollutant treatment and utilization in Beijing-Tianjin-Hebei region is high while it is seemed to lack coordination between cities in other dimensions. This evaluation method proposed by this study can play the following key role: providing a new entry point for researches of green development, and also providing reference or guidance for the government in formulating public policies on green development.}
}
@article{YU2022124619,
title = {A novel real-time energy management strategy based on Monte Carlo Tree Search for coupled powertrain platform via vehicle-to-cloud connectivity},
journal = {Energy},
volume = {256},
pages = {124619},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124619},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222015225},
author = {Xiao Yu and Cheng Lin and Peng Xie and Sheng Liang},
keywords = {Energy management, Monte Carlo tree search, Vehicle-to-cloud connectivity, Electric vehicle, Coupled powertrain platform},
abstract = {To improve the performance and efficiency of the energy management strategy used in electric vehicles equipped with a dual-motor coupled powertrain platform, this study proposes a systematic real-time search approach via vehicle-to-cloud (V2C) connectivity to reduce the battery degradation and electrical consumption by control working mode and split torque. To be specific, the Monte Carlo Tree Search (MCTS) is employed to search for optimal control sequence in the velocity feasible range in the cloud platform, considering battery loss and electric cost. The logic of time and velocity range updating is proposed as the solution for abrupt traffic changes. To evaluate the effectiveness of the proposed method, a rule-based and an online DP (Dynamic Programming) -based strategy is developed as the baseline approach. Meanwhile, the assessment conditions include standard cycles following power noise and real-world driving cycles. Finally, actual vehicle and hardware-in-the-loop (HIL) experimental results demonstrate that the proposed method significantly outperforms other strategies, the average total cost is 0.36 USD/km, and the improvements are 12.9% and 11.4% compared to the rule-based and online DP-based approaches, respectively.}
}
@article{LONG2022111848,
title = {A review of energy efficiency evaluation technologies in cloud data centers},
journal = {Energy and Buildings},
volume = {260},
pages = {111848},
year = {2022},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.111848},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822000196},
author = {Saiqin Long and Yuan Li and Jinna Huang and Zhetao Li and Yanchun Li},
keywords = {Data center, Energy efficiency, Energy consumption, Evaluation, Metrics},
abstract = {The energy consumption by data centers is expanding in tandem with the rapid rise of the digital economy. Data centers, as high-energy-consumption organizations, have garnered extensive attention from society in order to accomplish energy conservation and emission reduction. As a result, improving the energy efficiency of cloud data centers has become a major topic of research. Researchers are working hard to develop practical energy efficiency evaluation methodologies and metrics in order to attain this goal. This article summarizes data center energy efficiency evaluation methods, classifies existing energy efficiency evaluation metrics, examines the current state and challenges of data center energy efficiency evaluation, and makes recommendations for improving energy efficiency evaluation technology to assist cloud operators, decision-makers, and researchers in developing appropriate energy efficiency evaluation strategies. We give data center researchers a better grasp of energy efficiency evaluation and encourage them to combine theory and practice in energy efficiency evaluation and utilize more advanced metrics to assess data center energy efficiency. This is a critical step in the quest for the most advanced green technology, as well as a significant step toward reaching sustainable development goals.}
}
@article{SOTOUDEH2023141785,
title = {Benchmarking the computed proton solvation energy and absolute potential in non-aqueous solvents},
journal = {Electrochimica Acta},
volume = {443},
pages = {141785},
year = {2023},
issn = {0013-4686},
doi = {https://doi.org/10.1016/j.electacta.2022.141785},
url = {https://www.sciencedirect.com/science/article/pii/S0013468622019417},
author = {Mohsen Sotoudeh and Kari Laasonen and Michael Busch},
keywords = {Absolute potential, Proton solvation energy, Non-aqueous solvents, Water, Dft, Quantum chemistry},
abstract = {Proton solvation energies and absolute potentials are of critical importance in all areas of chemistry. But despite their relevance they are only known in water with a sufficient degree of accuracy while we still lack fundamental understanding in non-aqueous solvents. Here, we report an extensive benchmark for different DFT or ab-initio methods, the solvation models, and the choice of reference compounds for computing proton solvation energies and absolute potentials. Our computations indicate, that cationic acids (ammonium and iminium ions) allow for the most accurate prediction of these parameters in water while neutral acids (e.g. alcohols, carboxylic acids) display an unphysical correlation between their pKa and the proton solvation energy. The CCSD(T)/SMD computations are the most accurate method for predicting the proton solvation energy. For non-aqueous solvents, excellent error cancelation has been observed for all considered parameters. Furthermore, we report a fundamental flaw in solvation models for non-aqueous solvents, causing an unphysical correlation between the pKa and the proton solvation energy in DMSO. This work thoroughly evaluates the most critical parameters affecting the computed proton solvation energies using DMSO as a test case.}
}
@article{KESHARI2023102952,
title = {An intelligent energy efficient optimized approach to control the traffic flow in Software-Defined IoT networks},
journal = {Sustainable Energy Technologies and Assessments},
volume = {55},
pages = {102952},
year = {2023},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102952},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822010001},
author = {Surendra Kumar Keshari and Vineet Kansal and Sumit Kumar and Priti Bansal},
keywords = {SD-IoT, Border Nodes, Network Lifetime, Energy Consumption, Lion Swarm Optimization Algorithm},
abstract = {In modern society numerous digital devices play a very significant role in day-to-day life. Digital devices are well connected and easily accessible through multiple sensors and Internet of Things (IoT) devices. Due to the rapid growth of digital devices, large amount of data traffics are being generated, which induces network congestion. To deal with large amount of data traffic a programmable Software Defined IoT (SD-IoT) infrastructure is utilized. For efficient and sustainable network, the data must be transmitted through optimal path in such a way to as to minimize energy consumption. Here, the network is partitioned into clusters to find an optimal path. Finding an optimal path from a set of possible paths is an NP-complete problem. To solve this problem, we propose to find a set of optimal border nodes of each cluster with other clusters in the network, so as to reduce the number of possible paths between clusters. The set of optimal border nodes will be selected in such a way so that they have maximum energy and minimum distances. This paper proposes an intelligent approach to find the set of optimal border nodes using Lion Swarm Optimization algorithm (LSOA). Once a set of optimal border nodes are obtained, an optimal path can be generated using a routing mechanism. The performance of the proposed work is analyzed in terms of packet delivery ratio, average latency, network lifetime and energy consumptions. The results show that the border nodes selected using LSOA finds better routes as compared to the border nodes selected using other state-of-the-art metaheuristics algorithm thereby, increases suitability of the network by energy conservation.}
}
@article{HUANG2023110668,
title = {Pancreatic fat fraction in dual-energy computed tomography as a potential quantitative parameter in the detection of type 2 diabetes mellitus},
journal = {European Journal of Radiology},
volume = {159},
pages = {110668},
year = {2023},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2022.110668},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X22005186},
author = {Shiqi Huang and Yuhong Liang and Xixi Zhong and Qunzhi Luo and Xinqun Yao and Zhuo Nong and Yi Luo and Lian Luo and Wei Jiang and Xiangyun Qin and Yaping Lv},
keywords = {Pancreas, Dual-energy CT, Fats, Type 2 diabetes mellitus},
abstract = {Purpose
To investigate the clinical value of measuring pancreatic fat fraction using dual-energy computed tomography (DECT) in association with type 2 diabetes mellitus (T2DM).
Materials and methods
This retrospective study included patients who underwent abdominal DECT between September 2021 and July 2022. The fat fractions in the head, body, and tail of the pancreas were calculated using fat maps generated from unenhanced DECT images, and CT values were measured at the same locations. The intraclass correlation coefficient (ICC) was used to analyze the reproducibility of measurements from two observers. Diagnostic performance was assessed using receiver operating characteristic curves.
Results
Seventy-eight patients, including 45 T2DM patients and 33 controls, were enrolled. The fat fractions of the pancreas were significantly higher in the T2DM group than in the control group (pancreatic head: 8.4 ± 6.3 % vs 5.1 ± 3.9 %; pancreatic body: 4.8 ± 4.0 % vs 2.7 ± 3.9 %; and pancreatic tail: 5.3 ± 3.2 % vs 2.7 ± 2.9 %, all p < 0.05). And the CT values of the pancreas were significantly lower in the T2DM group than in the control group (pancreatic head: 41.1 ± 8.5 HU vs 45.7 ± 4.6 HU; pancreatic body: 44.4 ± 5.0 HU vs 47.4 ± 3.7 HU; and pancreatic tail: 44.5 ± 5.0 HU vs 47.6 ± 3.2 HU, all p < 0.05). The fat fraction of the pancreatic tail was the best indicator for distinguishing T2DM patients from the controls (area under the curve: 0.716 (95 % CI: 0.601, 0.832), sensitivity: 64.4 % (95 % CI: 48.7 %, 77.7 %), and specificity: 78.8 % (95 % CI: 60.6 %, 90.4 %)).
Conclusion
The DECT fat fractions of the pancreas could be a valuable additional parameter in the detection of T2DM.}
}
@article{BRAVODIAS2022108504,
title = {Using building thermal mass energy storage to offset temporary BIPV output reductions due to passing clouds in an office building},
journal = {Building and Environment},
volume = {207},
pages = {108504},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.108504},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321008994},
author = {João {Bravo Dias} and Guilherme {Carrilho da Graça}},
keywords = {Demand flexibility, Thermal energy storage, Solar photovoltaic energy, Thermal mass, Cloud shading, Low energy buildings},
abstract = {Ongoing concerns about electrical grid stability and existing economic interests create unattractive conditions for grid-injection of renewable energy produced in building integrated photovoltaic systems (BIPV). As a result, BIPV often fails to use the full potential of the building envelope and site. An effective approach to increase BIPV size is maximizing self-consumption of the generated renewable energy through building demand flexibility and energy storage. Unfortunately, in cooling dominated office buildings with BIPV, shading by passing clouds can increase grid demand as the building maintains its energy demand in a moment when BIPV output is reduced. This paper analyses the effectiveness of turning off the heating ventilation and air conditioning system (HVAC) to offset temporary reductions in BIPV output due to passing clouds. Analysis of cloud duration in different climates shows that, on average, clouds last 20 min and occur predominately in the afternoon. When HVAC is off due to cloud shading, the building thermal mass limits indoor temperature increase to 2 °C in the first 50 min. Simulation based analysis of HVAC control-based energy flexibility shows that it is possible to maintain acceptable thermal comfort while reducing HVAC grid energy demand by 60% during the cooling season.}
}
@article{HAGHNEGAHDAR2022112439,
title = {Enhancing dynamic energy network management using a multiagent cloud-fog structure},
journal = {Renewable and Sustainable Energy Reviews},
volume = {162},
pages = {112439},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112439},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122003458},
author = {Lida Haghnegahdar and Yu Chen and Yong Wang},
keywords = {Cloud-fog computing, Information technology, Smart grid, Electric vehicle (EV), Multiagent structure, Energy network, Communications},
abstract = {Smart Grid benefits from information and communications technology (ICT) to integrate data from different sources across the network. The growing market for electric vehicles (EV) and electric devices is increasing energy demand. To manage a large amount of complex intelligent equipment, EV charging and discharging, and data-intensive devices, a reliable modern smart grid with a service-oriented, secure, efficient, and cost-effective system is compelling. Cloud computing is a promising approach to achieve that objective as monitoring power grid data flow and data center management are the key. This paper proposes an optimization model for energy management within the power cloud. Our energy system model enables accessible services to computing resources and real-time data stream processing within an integrated environment. Using digital technology, the proper implementation of our model reduces costs and energy consumption and improves the smart grid reliability for customers and energy providers in a distributed manner. This paper presents the implementation of a three-procedure optimization algorithm within a novel Multi-Agent Cloud-fog Structure (MACS) to meet the requirements raised by smart grid communication and distribution. Our model promotes reliable energy consumption adjustments by end-users who can choose power supplied by solar, wind, geothermal, biomass, or other renewable sources or from non-renewable sources in ways that reveal opportunities for demand-supply balance and energy saving.}
}
@article{DECARVALHO2022103762,
title = {Mapping of regulatory actors and processes related to cloud-based energy management environments using the Apriori algorithm},
journal = {Sustainable Cities and Society},
volume = {80},
pages = {103762},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103762},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722000932},
author = {Patrícia Stefan {de Carvalho} and Julio Cezar Mairesse Siluk and Jones Luís Schaefer},
keywords = {Energy management, Energy cloud regulation, Energy regulation process, Apriori algorithm},
abstract = {Energy Cloud (EC) is the future proposal for energy management but the point is that there is no regulatory framework for EC. Therefore, knowing how the energy regulatory environment works and the relationship between actors and processes in this environment will contribute to the proposition of a new, well-structured regulatory system. The objective of this article is to identify the processes and actors which compose the regulation level of energy systems, to establish the basic relationships between these actors and processes, outlining the guidelines for the establishment and/or modification of policies, laws, and regulations related to the transition of energy management systems to the EC. The method used to achieve the objective was a systematic literature review (SLR) and the Apriori algorithm. SLR identified 7 main processes and 21 secondary processes, totaling 28 regulatory processes (outlined and presented through a mental map), being established through Apriori a network of dependencies between these processes with 37 direct links. 23 actors were identified that are structured in a network with 28 direct and dependent connections. The connections between processes and actors can serve as a starting point for creating a roadmap for the development of new regulations considering the implementation of EC.}
}
@article{CHAUDHRY2020100357,
title = {Thermal-benchmarking for cloud hosting green data centers},
journal = {Sustainable Computing: Informatics and Systems},
volume = {25},
pages = {100357},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2019.100357},
url = {https://www.sciencedirect.com/science/article/pii/S2210537919300290},
author = {Muhammad Tayyab Chaudhry and M. Hasan Jamal and Zeeshan Gillani and Waqas Anwar and Muhammad Salman Khan},
keywords = {Cloud computing, Green data centers, Thermal benchmarking, Workload modeling},
abstract = {Thermal efficient usage of cloud hosting data center servers saves cooling energy and helps establish green cloud data centers. To achieve this goal, the data centers must be stress-tested to avail thermal data related to server utilization. The inherent limitations of cloud computing limit the control of cloud data center owner over the workload execution of cloud services except for the infrastructure and therefore thermal-aware workload modeling or thermal benchmarking of cloud infrastructure can fill this gap. Thermal-benchmarking techniques, through manipulation of server utilization, reveal the thermal profiles and thermal statistics of the servers that can be useful for thermal efficient data center management. This paper presents a generic approach to thermal-benchmarking and profiling of cloud hosting data center servers. We propose workload models to generate customizable thermal benchmarks for stress testing of data center servers. Additionally, we use workload traces from Alibaba cloud to generate thermal statistics to show that the proposed thermal benchmarking approach is applicable to any data center workload trace for any data center server.}
}
@article{WANG2023126585,
title = {Coordinated optimal scheduling of integrated energy system for data center based on computing load shifting},
journal = {Energy},
volume = {267},
pages = {126585},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.126585},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222034727},
author = {Jiangjiang Wang and Hongda Deng and Yi Liu and Zeqing Guo and Yongzhen Wang},
keywords = {Data center (DC), Integrated energy system (IES), Load shifting, Computing task, Optimum scheduling, Multi-objective optimization},
abstract = {High-efficiency uninterruptable power supplies and sustainable energy systems are essential to realize data centers more sustainable and environmentally friendly. Considering the coupling between computing tasks and power consumption, this paper proposes a coordinated optimization model of operational scheduling of integrated energy system (IES) for data center according to computing load transfer. An IES consisting of solar photovoltaic arrays, gas engine, and organic Rankine cycle is proposed and optimized to obtain the optimum component capacities. Then, the computing tasks of servers with task shifting are modeled to characterize the arrival time, execution time, and deadline of different tasks. The operation scheduling optimization model of components in the IES with the computing task transfer is proposed to minimize the operating costs of data center and maximize the user satisfaction level of computing tasks. The impacts of objective weights on the dispatch strategies of the IES and computing tasks are discussed. The optimization results in a case study demonstrate that the data center can effectively realize the transfer of electricity and cooling loads by transferring computing tasks to improve the IES performances. The optimum dispatch strategies with task transfer saves operational cost and fuel consumption by 2.76% and 2.56%, respectively, and the carbon dioxide emission is reduced by 6.31% in non-heating season. Their benefits in heating season are 3.47%, 4.11%, and 2.17%, respectively.}
}
@article{KAMBADAKONE2023xv,
title = {Unveiling the Spectrum: Dual-Energy Computed Tomography and Beyond!},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {xv-xvi},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0033838923001677},
author = {Avinash Kambadakone and Daniele Marin}
}
@article{WANG2023611,
title = {Online optimization of intelligent reflecting surface-aided energy-efficient IoT-edge computing},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {611-625},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004125},
author = {Zhongyang Wang and Du Xu},
keywords = {Edge computing, IoT, Online optimization, Intelligent Reflecting Surface, WPT},
abstract = {With the tremendous developments of Internet of Things (IoT), IoT devices and applications are imposing progressively higher requirements on computing, communication, and power services. Edge computing in combination with wireless power transmission (WPT) presents a viable solution. By empowering wireless access points (APs) with the capability of computation and WPT, APs can act as both edge computing servers and power stations. To improve the efficiency of data transmission and WPT, the Intelligent Reflective Surface (IRS) technique is adopted to passively beamforming thereby improving the channel condition of the wireless links. In such an IoT-edge system, efficiency and stability are the main concerns in designing a strategy of workload scheduling and resource allocation. In this paper, we propose an online algorithm named LSDR (Lyapunov optimization in combination with semi-definite relaxation) to minimize the total energy consumption while maintaining the system’s stable operation. The system’s stability implies that unprocessed data on the AP is limited and the battery level of each IoT sensor should not be depleted. Without requiring prior knowledge of the statistics of the IoT-edge system, the algorithm can balance the trade-off between system stability and total energy consumption. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate the effectiveness and efficiency of the algorithm.}
}
@article{LU20223167,
title = {Dynamic offloading for energy-aware scheduling in a mobile cloud},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part B},
pages = {3167-3177},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S131915782200115X},
author = {Junwen Lu and Yongsheng Hao and Kesou Wu and Yuming Chen and Qin Wang},
keywords = {Mobile cloud computing, Energy consumption, Offloading, Tradeoff},
abstract = {Mobile cloud computing (MCC) brings rich computational resources to mobile users, network operators, and cloud computing providers. The battery capacity of mobile devices poses several complex challenges, hence it is necessary to save energy by offloading applications to the remote cloud resources, especially when the scheduling is in a dynamic mobile cloud computing environment. To make a tradeoff decision involving energy consumption, deadline, and the system load, we proposed an iterated greedy taboo-mechanism algorithm (IGTMA) to solve the above issues in MCC environment. Compared to state-of-art approaches such as Adaptive First Come First Served (AFCFS), Minimize Execution Time (MINET), and tradeoff decisions for code offloading (TRADEOFF), the simulation experiment results show that our proposed IGTMA reduces energy consumption and enhances the number of finished jobs.}
}
@article{SABOURI2023e699,
title = {Pelvis Treatment Plan Dose Comparison for Proton Therapy Using Single Energy and Dual Energy Computed Tomography Simulation Methods},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {117},
number = {2, Supplement },
pages = {e699},
year = {2023},
note = {ASTRO 2023 65th Annual Meeting},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.06.2182},
url = {https://www.sciencedirect.com/science/article/pii/S0360301623066166},
author = {P. Sabouri and A. Koroulakis and D. Cusatis and K. Lehman and P. Wohlfahrt and J. Shah and J.K. Molitoris and S. Mossahebi},
abstract = {Purpose/Objective(s)
In proton therapy, plan robustness is ameliorated by expanding the irradiation volume beyond the target, in order to mitigate setup and proton range uncertainties (RU). A byproduct of this expansion is elevated doses to surrounding organs at risk (OARs) which reduce some of the benefits associated with proton therapy. Dual-energy CT (DECT) has been shown in multiple phantom and animal tissue studies to reduce RU without compromising plan robustness. This study quantifies dosimetric differences between single-energy CT (SECT) and DECT for pelvis patients treated with intensity modulated proton therapy (IMPT).
Materials/Methods
Under IRB approval, SECT and DECT scans from 25 IMPT pelvis patients were acquired using a CT scanner. Clinical plans were generated on the SECT images using robust optimization settings of 3.5% RU and 5 mm setup uncertainty in a treatment planning system. Subsequently, the clinical plans were recomputed on the corresponding DECT generated SPR-map images. For each patient, target coverage, mean and maximum OAR doses were compared for both image sets.
Results
Comparison of two plans showed systematic differences in target minimum dose (D99%) and V100%. Variations as high as 3.1% with DECT were observed in D99% indicating target under-dosage. On average, use of SECT overestimated V100% by 2.4% when compared to DECT. Since all clinical plans were optimized robustly to meet V95% coverage, higher agreement (<1%) was achieved for V95% (99.9±0.2%), and D95% (99.6±0.3%). DECT relative to SECT indicated slightly higher OAR maximum doses for femoral heads (2.3±2.78%), penile bulb/external genital (1.33±4.00%) and large bowel (0.5±1.25%). Similarly, higher mean dose was observed to rectum (2.08±4.88%), bladder (1.41±0.25%), femoral heads (1.18±6.61%) and penile bulb/external genital (1.78±5.74%) for DECT relative to SECT.
Conclusion
Our results show a disparity between evaluated SECT and DECT target and OAR dosimetric parameters. Given the higher accuracies in proton range estimation associated with the use of DECT, its use can imply a potential for more conformal proton plans as well as higher TPS computed target coverage and OAR dose accuracy, both of which enhance overall treatment quality.}
}
@article{BIRUDU2023105867,
title = {A negative capacitance FET based energy efficient 6T SRAM computing-in-memory (CiM) cell design for deep neural networks},
journal = {Microelectronics Journal},
volume = {139},
pages = {105867},
year = {2023},
issn = {0026-2692},
doi = {https://doi.org/10.1016/j.mejo.2023.105867},
url = {https://www.sciencedirect.com/science/article/pii/S0026269223001805},
author = {Venu Birudu and Siva Sankar Yellampalli and Ramesh Vaddi},
keywords = {Computing-in-memory (CiM), Deep neural networks (DNNs), Energy efficiency, Negative capacitance FETs (NCFETs), SRAM, VLSI/Hardware accelerators},
abstract = {An Energy-Efficient Computing-in-Memory (CiM) cell design utilizing a Negative Capacitance (NC) FET has been proposed to support computing architectures for Deep Neural Networks (DNNs). The NCFET device characteristics for CiM architectures have been studied to determine an optimal device performance window by changing the thickness of ferroelectric layer (Tfe). The performance metrics such as read margin (RM), write margin (WM), read energy and write energy of NCFET 6 T SRAM cell are analyzed with varying Tfe at two different supply voltages 0.3 V and 0.5 V respectively. NCFET based SRAM cell design achieves higher RM and WM at Tfe of 3 nm and lower energy consumption at 1 nm Tfe as compared with the baseline SRAM cell design at both VDD = 0.3 V and VDD = 0.5 V respectively. 6 T NCFET based CiM cell design for performing basic input-weight product operation (IWP) has been demonstrated and performance comparison is done with baseline CMOS design at VDD = 0.3 V and VDD = 0.5 V. In comparison with the baseline CMOS CiM cell design, NCFET based SRAM CiM design achieves ∼2.59x and 1.62x lower energy consumption at VDD = 0.3 V and VDD = 0.5 V respectively with an optimal Tfe window of 1–3 nm.}
}
@article{JAMEEL2021102480,
title = {Green sonochemical synthesis platinum nanoparticles as a novel contrast agent for computed tomography},
journal = {Materials Today Communications},
volume = {27},
pages = {102480},
year = {2021},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2021.102480},
url = {https://www.sciencedirect.com/science/article/pii/S2352492821004724},
author = {Mahmood S. Jameel and Azlan Abdul Aziz and Mohammed Ali Dheyab and Baharak Mehrdel and Pegah Moradi Khaniabadi and Bita Moradi Khaniabadi},
keywords = {, Ultrasonic irradiation, Omnipaque, Colloidal stability, Hounsfield},
abstract = {Computed tomography (CT) imaging can be enhanced with the use of contrast agents. However, the conventionally used clinical small-molecule agents (Omnipaque) are encumbered by an array of constraints that include potentially toxic and unwanted side effects. Therefore, this study presents a facile plant-assisted synthesis of platinum nanoparticles (Pt NPs) that employs ultrasonic irradiation to improve the Hounsfield (HU) values for CT imaging. This “one-step one-pot”, eco-friendly and simple approach produced highly stable, biocompatible and ultrasmall Pt NPs colloidal with a mean diameter of 3.8 nm. The extract from Prosopis farcta (P. farcta) fruits served as both a reducing agent and stabilizer. The cell viability test demonstrated that HEK-293 cells remained viable after being subjected to high concentrations of Pt NPs. The X-ray attenuation of the as-synthesized Pt NPs (HU = 355) exceeds those of commercial NPs and conventionally prepared NPs. Thus, this research gives new insights into the use of plants to prepare novel contrast agents for molecular imaging.}
}
@article{EFTEKHARI2021100219,
title = {Statistical optimization, soft computing prediction, mechanistic and empirical evaluation for fundamental appraisal of copper, lead and malachite green adsorption},
journal = {Journal of Industrial Information Integration},
volume = {23},
pages = {100219},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100219},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000194},
author = {Mohammad Eftekhari and Mohammad Gheibi and Hossein Azizi-Toupkanloo and Zahra Hossein-Abadi and Majeda Khraisheh and Amir Mohammad Fathollahi-Fard and Guangdong Tian},
keywords = {Aerated autoclave concrete, Chitosan, Removal, Adsorption isotherms, Adsorption kinetics},
abstract = {This paper describes an efficient and low cost material for treatment of Cu2+, Pb2+ and malachite green from aquatic environments. Aerated Autoclave Concrete (AAC) was used as a low cost bed and modified by chitosan to synthesize of AACCH composite. Then, it was analyzed by the Energy Dispersive X-ray spectroscopy (EDX), X-Ray Fluorescence (XRF), Field Emission Scanning Electron Microscopy (FE-SEM) and Fourier Transform Infrared spectrophotometry (FT-IR) analysis. The key factors influencing on the removal percentage among each run including, sample's pH, amounts of adsorbent and contact time were optimized by the Central Composite Design in Response Surface Methodology (CCD-RSM). Based on the obtained results, the optimum conditions for the removal of both Cu2+ and Pb2+ are pH 5.8, amounts of adsorbent: 14 mg and 58 min contact time. Also, for MG, its maximum removal percentage obtained at pH 9.7, 13 mg adsorbent and 55 min removal time. In addition, the variations of adsorptive behaviors were scrutinized by Adaptive Network-based Fuzzy Inference System (ANFIS) based on Sugeno model with focusing on removal percentage predictions. Also, to evaluate the adsorption mechanism, Dubinin–Radushkevich (D-R), Langmuir, Temkin, Freundlich (two parameter equations) and Sips, Khan and Toth isotherms (Three parameter equations) were appraised and the outcomes demonstrate that the adsorptive reaction of Pb2+, Cu2+ and MG governed by the Freundlich isotherm with the maximum adsorption capacities of 78.12, 56.82 and 833.33 mg g−1, correspondingly. Likewise, outputs of the D-R and Temkin isotherms show that the physiosorption process is the main interaction of analytes with AACCH composite. The consequences of the kinetic modeling illustrated that the adsorptive reaction of Cu2+, Pb2+ and MG followed by the pseudo second order adsorption kinetic. The geometry kinetic computing depicted that the adsorption/desorption rates don't have any interfering during reaction process. The advantages such as high efficiency, low cost, reusability and green aspect, make AACCH composite as a high performance adsorbent for decontamination of Pb2+, Cu2+ and MG from water resources.}
}
@article{SEHGAL20224635,
title = {Green software: Refactoring approach},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {7},
pages = {4635-4643},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820305164},
author = {Rajni Sehgal and Deepti Mehrotra and Renuka Nagpal and Ramanuj Sharma},
keywords = {Code smells, Refactoring, Power consumption, Joulemeter, Java applications},
abstract = {An energy efficient information and communication system is a need of the day. Information technology related industries are making efforts to reduce power consumption by improvising both the hardware infrastructure and software systems. The hardware is driven by software; hence, design and development of software may have a significant impact energy need of the overall system. Many times it is observed that complex software performs useless tasks leading to power consumption, referred to as energy leak. The solution of energy leaks for mobile applications has attracted the interest of researchers, but still handling energy leaks issues for applications developed using Java-based technology need to be explored. While designing software during maintenance phase, code smells are introduced which not only reduce performance of the software also may lead to execution of useless code thus enhancing the energy leak. The common practice of the software industry to remove the code smells is use of refactoring strategy. The basic idea of this study is to understand the impact of refactoring on total energy consumption. To explore this hypothesis a set of Java based applications are selected, code smell present in these applications are discovered and a suitable refactoring strategy is applied to reduce the code smell. Power consumption is estimated using an open source tool Microsoft Joulemeter, it monitors energy usage by machine resources and estimates power consumption. The energy consumption before and after refactoring is recorded and statistical t-test is performed to validate the proposed concept.}
}
@article{HOU2022593,
title = {Post-machining allowance optimization of directed energy deposited impeller blades using point cloud registration},
journal = {Manufacturing Letters},
volume = {33},
pages = {593-601},
year = {2022},
note = {50th SME North American Manufacturing Research Conference (NAMRC 50,2022)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2022.07.074},
url = {https://www.sciencedirect.com/science/article/pii/S2213846322001055},
author = {Liang Hou and Jing Guo and Yun Chen and Shuyuan Chen and Yuan Li and Xiangjian Bu},
keywords = {Directed energy deposition, on-machine measurement, allowance optimization, free-form part, hybrid additive and subtractive manufacturing},
abstract = {Complex free-form parts manufactured by directed energy deposition (DED) have problems of uneven allowance distributions and severe stair-case effects. In order to optimize allowance for post-machining in a hybrid additive and subtractive machine centre, a point cloud registration method is proposed for allowance optimization to match the on-machine measured point cloud of the deposited part with the point cloud of the theoretical model. Firstly, the deposition path is used to construct the inspection path and obtain the maximum and minimum envelops of the deposited part. Secondly, the allowance distribution for the uniform and positive criterion is solved using the pattern search algorithm, after the datum planes are overlapped. Finally, a free-form blade of a centrifugal impeller is selected as a case study for allowance optimization. The proposed method is also compared with an allowance optimization method using genetic algorithm. The results show that the proposed method is more accurate and efficient for rapid allowance optimization for DED complex parts.}
}
@article{PAUL2022110880,
title = {A novel energy balance approach for a verifiable and accurate solution of radiation extinction in purely absorbing particle clouds},
journal = {Journal of Computational Physics},
volume = {451},
pages = {110880},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110880},
url = {https://www.sciencedirect.com/science/article/pii/S0021999121007750},
author = {Immanuvel Paul and Ali Mani},
keywords = {Particle clouds, Radiation transport, Beer-Bouguer law, Absorption coefficient},
abstract = {We consider the problem of radiation transport through purely absorbing particle clouds. The gold-standard solution of particle-resolved Monte Carlo ray-tracing method to this problem is computationally expensive and therefore solving the radiation transport equation (specifically, the Beer-Bouguer law (BB-law)) on a Eulerian mesh is often preferred. While the absorption coefficient in the real problem is infinite, the BB-law approximates it to be a finite number in the form of number density through a set of assumptions. For particle clouds that do not obey these assumptions, the BB-law predicts an incorrect exponential decay. Also, when the number density is computed using the nearest-neighbor approach, the BB-law solution diverges when the Eulerian mesh size becomes closer to or smaller than the particle size. This numerical divergence is due to the homogenization error. Although the filtering strategy for number density minimizes the homogenization error, it still converges to an incorrect exponential decay for particle clouds that break the BB-law constraints and the cost of the filtering is equivalent to that of the gold-standard solution. In this study, we develop a novel, highly accurate, verifiable and cost-effective solution to the radiation transport equation on a Eulerian domain using an energy balance approach where we derive an expression for the absorption coefficient as a function of particle and Eulerian mesh sizes. We apply our new method to Poisson and turbulent particle clouds that violate all the BB-law constraints and show that the solution is converging upon the mesh refinement, and eventually, we recover the same gold-standard solution for a much cheaper computational cost.}
}
@article{HUSSAIN2022211,
title = {Deadline-constrained energy-aware workflow scheduling in geographically distributed cloud data centers},
journal = {Future Generation Computer Systems},
volume = {132},
pages = {211-222},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.02.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000619},
author = {Mehboob Hussain and Lian-Fu Wei and Amir Rehman and Fakhar Abbas and Abid Hussain and Muqadar Ali},
keywords = {Energy cost, Electricity price, Task scheduling, DVFS, Geographically distributed cloud data centers},
abstract = {The energy cost of cloud data centers is increasingly concerned worldwide; the minimization of energy cost is becoming an urgent problem. Considering data centers are geographically distributed, electricity prices are different in each data center. Consequently, it is also critical to assign workflow tasks to the geographically distributed data centers because data required by tasks is usually conserved in the given data center. So, as electricity prices and data transmission times change, it becomes a big challenge to minimize energy costs when scheduling workflow tasks to heterogeneous servers in cloud data centers. A DEWS (Deadline-constrained Energy-aware Workflow Scheduling) algorithm is proposed in this paper, which consists of task sequencing, VND-based data center searches, task sequence adjustment, and VM searching with Dynamic Voltage Frequency Scaling (DVFS). The DVFS method is included in the optimization procedure to cut down the additional energy cost of service providers. The experimental results show that the proposed algorithm outperforms the compared algorithms and reduces energy cost by 5%–20%.}
}
@article{ZHAO202169,
title = {Power optimization with less state transition for green software defined networking},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {69-81},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20310578},
author = {Yong Zhao and Xingwei Wang and Chuangchuang Zhang and Qiang He and Min Huang},
keywords = {SDN, Power-saving, Device state transition, Network monitoring, Link congestion},
abstract = {The power consumed by the Internet industry has been growing sharply in recent years, which accounts for a considerable proportion of the overall power consumption. To reduce high power consumption in the network, numerous proposals on power-saving are proposed. However, they usually reduce power consumption by dynamically sleeping and awakening the network device according to the current traffic fluctuation. When the traffic fluctuate sharply, the network device will swing between sleep and active state frequently, which consumes a considerable power and causes a series of side effects, such as packet losses, short sustainable device service life and synchronization among devices. In this paper, based on Software Defined Networking (SDN), we propose a Power Optimization with Less State Transition (POLST) framework, which mainly achieves the following two functions: (1) Real-time network monitoring with high accuracy and low overhead; (2) Power optimization with less device state transition. Then, we formulate optimization problem of minimizing the network power consumption, while considering power consumed by device state transition. To solve it, we propose a Power Aware Routing with Less State Transition (PARLST) algorithm. Finally, simulation results show that compared with the benchmarks, POLST has excellent performance in both network monitoring and network power optimization. More specifically, in the network monitoring, POLST can achieve up to 97.2% measurement accuracy with low monitoring overhead. In the power optimization, in comparison to the benchmarks, POLST can achieve up to 72.2% energy efficiency, 56.6% decrease in Switch State Transition (SST), 53.5% decrease in Link State Transition (LST).}
}
@article{LYU2023105787,
title = {Fluids flow in granular aggregate packings reconstructed by high-energy X-ray computed tomography and lattice Boltzmann method},
journal = {Computers & Fluids},
volume = {253},
pages = {105787},
year = {2023},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2023.105787},
url = {https://www.sciencedirect.com/science/article/pii/S0045793023000129},
author = {Qifeng Lyu and Anguo Chen and Jie Jia and Amardeep Singh and Pengfei Dai},
keywords = {Fluids flow, Granular aggregates, X-CT, LBM, Permeability, Porosity},
abstract = {Properties of fluids flow in granular aggregates are important for the design of pervious infrastructures used to alleviate urban water-logging problems. Here in this work, five groups of aggregates packing with similar average porosities but varying particle sizes were scanned by a high-energy X-ray computed tomography (X-CT) facility. The structures of the packings were reconstructed. Porosities were calculated and compared with those measured by the volume and mass of infilled water in the packing. Then pore networks were extracted and analyzed. Simulations of fluids flow in the packings were performed by using a lattice Boltzmann method (LBM) with BGK (Bhatnagar-Gross-Krook) collision model in the pore-network domain of the packings. Results showed wall effect on the porosity of aggregates packing was significant and the influence increased with the aggregate sizes. In addition, Poisson law and power law can be used to fit the coordination number and coordination volume of the packing's pore network, respectively. Moreover, the mass flow rates of fluids in the aggregates were affected by the porosities. On the two-dimensional slices, the mass flow rate decreased when the slice porosity increased. But for the three-dimensional blocks, the average mass flow rate increased with the volume porosity. And the permeability of the aggregates packing showed correlating change trend with the average pore diameter and fitting parameters of coordination volumes, when the sizes of aggregates changed. Though the limitation of merging interfaces causing fluctuation and discontinuity on micro parameters of fluid flow existed, the methods and results here may provide knowledge and insights for numerical simulations and optimal design of aggregate-based materials.}
}
@article{AISHWARYA2022100616,
title = {Design of Energy-Efficient Induction motor using ANSYS software},
journal = {Results in Engineering},
volume = {16},
pages = {100616},
year = {2022},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2022.100616},
url = {https://www.sciencedirect.com/science/article/pii/S2590123022002869},
author = {M. Aishwarya and R.M. Brisilla},
keywords = {Induction Motor, Electric Vehicle, Motor Design, Materials, ANSYS},
abstract = {The transportation sector is the second largest energy consumer and is growing every year. Electric Vehicles (EVs) driven by electric motor reduces the usage of fossil fuels. Induction Motors (IMs) are particularly gaining attention in the EV world for their advantages over traditional motors, such as flexibility in control, low material cost, and superior ventilation and cooling. However, machine designers face difficulties in designing EV IM with higher efficiency, power factor, and high torque with good performance and operational features. The Squirrel Cage Induction Motor (SCIM) can provide a compact design through the choice of proper materials. This paper proposes choice of material and design of IM for EV application. The design and analysis process uses the ANSYS RMxprt and Maxwell simulation software. The proposed model for EV is designed with 415 V, 50 Hz, and 5 HP output power. The design process focuses on comparative analysis of the core and winding materials using Finite Element Analysis (FEA) in ANSYS Maxwell. The performance of designed motor is analysed under both healthy and faulty conditions. The results obtained using ANSYS Maxwell indicate that the design is effective and accurate.}
}
@article{YAN2022107688,
title = {Energy-aware systems for real-time job scheduling in cloud data centers: A deep reinforcement learning approach},
journal = {Computers and Electrical Engineering},
volume = {99},
pages = {107688},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107688},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622000106},
author = {Jingchen Yan and Yifeng Huang and Aditya Gupta and Anubhav Gupta and Cong Liu and Jianbin Li and Long Cheng},
keywords = {Deep reinforcement learning, Energy-aware scheduling, Deep Q-learning, Task scheduling, Cloud computing, Data centers, QoS},
abstract = {With the advantages such as high-performance, low-maintenance, and reliability, more and more companies are moving their computing infrastructures to the cloud. In the meantime, with the increasing number of users continuously submitting jobs to cloud, energy consumed by the current cloud data centers has become a major concern for cloud service providers, due to financial and environmental reasons. In this paper, we propose a deep reinforcement learning (DRL) approach to handle real-time jobs. Specifically, we focus on allocating incoming jobs to appropriate virtual machines (VMs) in a way that energy consumption can be optimized while high quality of service (QoS) can be achieved. We give the detailed design and implementation of our approach, and our experimental results demonstrate that the proposed method can achieve better performance in job success rate and average response time with less energy consumption than the current approaches, in the presence of different real-time cloud workloads.}
}
@article{YIN2022124177,
title = {Risk assessment of photovoltaic - Energy storage utilization project based on improved Cloud-TODIM in China},
journal = {Energy},
volume = {253},
pages = {124177},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124177},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222010805},
author = {Yu Yin and Jicheng Liu},
keywords = {Photovoltaic, Energy storage, Utilization (PVESU), Risk assessment, Cloud-TODIM (Cloud-an acronym in Portuguese for interative multi-criteria decision making), TOPSIS(Technique for order of preference by similarity to ideal solution), Evaluation index system},
abstract = {“Photovoltaic + energy storage” is considered as one of the effective means to improve the efficiency of clean energy utilization. In the era of energy sharing, the “photovoltaic - energy storage - utilization (PVESU)" model can create a more favorable market environment. However, the various uncertainties in the construction of the PVESU project have become the main obstacles to the development of the PVESU model. This paper aims to evaluate the risk level of China's PVESU projects through an improved Cloud-TODIM (Cloud-an acronym in Portuguese for Interative Multi-criteria Decision Making) method. First of all, 18 critical risk factors are identified using the constructed five-dimensional risk analysis model. Secondly, the improved method is used to calculate the weight of the comprehensive criteria and rank 14 PVESU alternatives. Finally, the accuracy and feasibility of the hybrid method are verified by sensitivity analysis tests concluding the loss decay coefficient adjustment and comparison analysis with the Cloud-TOPSIS(Cloud-Technique for Order of Preference by Similarity to Ideal Solution) method. Meanwhile, in terms of energy storage, some suggestions are made for the future development of China's PVESU project. This study can also provide insightful enlightenment for PVESU project investors, risk management professionals and decision makers.}
}
@article{FATTAHI2023106750,
title = {Soft-linking a national computable general equilibrium model (ThreeME) with a detailed energy system model (IESA-Opt)},
journal = {Energy Economics},
volume = {123},
pages = {106750},
year = {2023},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2023.106750},
url = {https://www.sciencedirect.com/science/article/pii/S0140988323002487},
author = {Amirhossein Fattahi and Frédéric Reynès and Bob {van der Zwaan} and Jos Sijm and André Faaij},
keywords = {Bottom-up models, Top-down models, Soft-linking},
abstract = {Top-down CGE models are used to assess the economic impacts of climate change policies. However, these models do not represent the technologies and sources of greenhouse gas emissions as detailed as bottom-up energy system models. Linking a top-down CGE model with a bottom-up energy system model assures macroeconomic consistency while accounting for a detailed representation of energy and emission flows. While there is ample literature regarding the linking process, the corresponding details and underlying assumptions are barely described in detail. The present paper describes a step-by-step soft-linking process and its underlying assumptions, using the Netherlands as a case study. This soft-linking process increases the Dutch energy demand levels in 2050 by 19.5% on average compared to assumed exogenous levels. Moreover, the GDP in 2050 reduces by 5.5% compared to the baseline economic scenario. Furthermore, we identified high energy prices as the primary cause of this GDP reduction in the soft-linking process.}
}
@article{KRZYWANIAK2023396,
title = {Dynamic GPU power capping with online performance tracing for energy efficient GPU computing using DEPO tool},
journal = {Future Generation Computer Systems},
volume = {145},
pages = {396-414},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.03.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001267},
author = {Adam Krzywaniak and Paweł Czarnul and Jerzy Proficz},
keywords = {Energy-aware computing, High-performance computing, Green computing, Machine learning, GPU energy optimization},
abstract = {GPU accelerators have become essential to the recent advance in computational power of high-performance computing (HPC) systems. Current HPC systems’ reaching an approximately 20–30 mega-watt power demand has resulted in increasing CO2 emissions, energy costs and necessitate increasingly complex cooling systems. This is a very real challenge. To address this, new mechanisms of software power control could be employed. In this paper, a dynamic new method of limiting software power is introduced on one of the latest NVIDIA GPUs: a software tool called the Dynamic Energy-Performance Optimiser (DEPO). DEPO minimizes the energy consumption of the CUDA based GPU workloads, with respect to one of the three given metrics: minimum of energy (E), Energy-Delay product (EDP) and Energy-Delay sum (EDS). The tool gathers power measurements from NVIDIA Management Library (NVML). Measuring the application progress at runtime is based on CUDA Profiling Tools Interface (CUPTI) kernel-counting. We have evaluated the DEPO tool on the NVIDIA RTX A4500 and A100 GPUs with machine learning workloads. Depending on the application (training of neural networks: Resnet152, Densenet161, VGG-19 or a GEMM benchmark) for the E target metric, we were able to obtain energy savings exceeding 22% for both NVIDIA A100 and RTX A4500 GPUs while the performance drop has never been higher than 20%. Using one of the bi-objective EDP or EDS metrics allowed finding configurations resulting in 15% or 18% of energy saved with only 8% of performance loss. For most of the experiments the percentage-wise performance penalty is lower than the energy savings. This demonstrates its potential for energy consumption reduction in HPC systems with GPU accelerators.}
}
@article{AGYEKUM202231073,
title = {A 3E, hydrogen production, irrigation, and employment potential assessment of a hybrid energy system for tropical weather conditions – Combination of HOMER software, shannon entropy, and TOPSIS},
journal = {International Journal of Hydrogen Energy},
volume = {47},
number = {73},
pages = {31073-31097},
year = {2022},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2022.07.049},
url = {https://www.sciencedirect.com/science/article/pii/S0360319922030646},
author = {Ephraim Bonah Agyekum and Jeffrey Dankwa Ampah and Sandylove Afrane and Tomiwa Sunday Adebayo and Ebenezer Agbozo},
keywords = {Hybrid energy system, Techno-economic analysis, Ghana, HOMER Pro software, Hydrogen production},
abstract = {The increasing threat to environmental sustainability as a result of greenhouse gas (GHG) emissions from fossil fuel base power plants has necessitated the need to find sustainable energy sources to meet the world's energy demands. This study focuses on assessing the potential of a hybrid power plant for the production of electricity, hydrogen for the production of fertilizer for agricultural activities, farmland irrigation, environmental impact as well as its employment potential in northern Ghana. The Shannon entropy weight and TOPSIS multi-criteria decision-making approach were adopted to rank and identify the optimal configuration out of five possible options for the study area. Results from the simulation show that the winning system, i.e., Hydro + Battery system would generate a total electricity of 1,095,679 kWh/year. A cost of electricity of 0.06 $/kWh with an operating cost (OC) of $18,318 was recorded for the winning system. The total produced hydrogen by the optimum configuration is 8816 kg/year at a levelized cost of hydrogen (LCOH) of 4.47 $/kg. The quantity of low-carbon fertilizer that can be produced from the produced hydrogen is also assessed. The optimum configuration also recorded an employment potential of 4 persons in 25 years. A total GHG equivalence of 383.49 metric tons of CO2 equivalent indicating the level of emissions that will be avoided should the optimum system be used to meet the demands specified in this study was obtained.}
}
@article{KONJAANG2022103400,
title = {Energy-efficient virtual-machine mapping algorithm (EViMA) for workflow tasks with deadlines in a cloud environment},
journal = {Journal of Network and Computer Applications},
volume = {203},
pages = {103400},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103400},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000595},
author = {J. Kok Konjaang and John Murphy and Liam Murphy},
keywords = {Energy consumption, Execution Cost, Execution makespan, Workflow scheduling},
abstract = {Processing large scientific applications generates a huge amount of data, which makes running experiments in the cloud computing environment very expensive and energy-consuming. To find an optimal solution to the workflow scheduling problem, several approaches have been presented for scheduling workflow on cloud resources. However, more efficient approaches are needed to improve cloud service delivery. In this paper, an energy-efficient virtual machine mapping algorithm (EViMA) is proposed to improve resource management in the cloud computing environment to achieve effective scheduling that reduces cloud data center energy consumption, execution makespan, and execution cost. This ensures that the requirements of cloud users are met, and improves the quality of services offered by cloud providers. Our proposed mechanism considers the heterogeneity of scheduling from both cloud users’ and workflow applications’ perspectives. Through simulation experiments on real workflow datasets, the proposed EViMA can provide better solutions for both cloud users and cloud providers by reducing energy consumption, execution makespan, and execution cost better than the state-of-the-art.}
}
@article{KHAN2022320,
title = {Workload forecasting and energy state estimation in cloud data centres: ML-centric approach},
journal = {Future Generation Computer Systems},
volume = {128},
pages = {320-332},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004155},
author = {Tahseen Khan and Wenhong Tian and Shashikant Ilager and Rajkumar Buyya},
keywords = {ML-centric approach, Workload prediction, Energy state estimation, Resource management, Distributed data centre},
abstract = {Resource management in data centres continues to be a critical problem due to increased infrastructure complexity and dynamic workload conditions. Workload and energy consumption prediction are crucial for efficient resource management decisions in cloud data centres. Existing solutions only consider forecasting the usage of virtual machine resources such as CPU and memory; they do not consider provisioned resources (CPU and memory) and disk, network transmission rates, which significantly affect the energy consumption of the host as well. VM-level energy consumption can be estimated for automated energy management decisions in modern data centres. However, it is not easy to measure energy for VM devices such as CPU, memory, and disk at the software level. In this way, we propose an ML-based model to predict load and energy to aid resource management decisions. For modelling workload predictions, we investigated several distinctive ML algorithms such as Linear Regression (LR), Ridge Regression (RR), ARD Regression (ARDR), ElasticNet (EN) and deep learning (DL) algorithm like Gated Recurrent Unit (GRU). The model’s predictions are measured using standard evaluation metrics like root mean square error (RMSE). We have discovered that GRU has performed very well by accomplishing the most negligible RMSE value for all the workload performances based on experimental results. For energy state estimation, we propose four diverse clustering algorithms, including, semi-supervised affinity propagation based on transfer learning (TSSAP), CLA based on transfer learning (TCLA), kmeans based on transfer learning (TKmeans), P-teda based on transfer learning (TP-teda) to discover similar groups of VMs dependent on features that may influence energy consumption as opposed to estimating it for each VM. The TSSAP has acquired promising clustering accuracy with 87.48% and 53.80% in identifying the VM classes which have been calculated using standard metric such as micro-precision for the chosen workload in compassion to affinity propagation (AP) and the average of other proposed clustering algorithms respectively.}
}
@article{LI2019100853,
title = {Green resource allocation for user access in cloud-based networks},
journal = {Physical Communication},
volume = {37},
pages = {100853},
year = {2019},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2019.100853},
url = {https://www.sciencedirect.com/science/article/pii/S1874490718307390},
author = {Shidang Li and Juan Zhao and Weiqiang Tan and Xuanlin Zhu and Hui Zhong},
keywords = {Power control, User access, Power consumption, Bisection search},
abstract = {In this paper, we investigate green power control for user access in cloud-based cellular networks. The optimization problem is formulated as a non-convex and a non-linear mixed-integer fractional form considering the circuit power consumption. In order to solve the considered problem, we transform the considered optimization problem into a tractable subtractive form problem, where the solution can be obtained through three-stage optimization. The outer layer only refers one-dimension search for a parameter which can be obtained through bisection search. In the second stage, a non-fractional power control problem remains to solve, and we exploit a power update function to update the transmit power. The third stage performs user access. Simulation results indicate that our proposed algorithm archives better performance than the traditional algorithm.}
}
@article{ZHANG2022106245,
title = {Quantification of aerosol and cloud effects on solar energy over China using WRF-Chem},
journal = {Atmospheric Research},
volume = {275},
pages = {106245},
year = {2022},
issn = {0169-8095},
doi = {https://doi.org/10.1016/j.atmosres.2022.106245},
url = {https://www.sciencedirect.com/science/article/pii/S0169809522002319},
author = {Yanqing Zhang and Yi Gao and Liren Xu and Meigen Zhang},
keywords = {Cloud radiation effect, Aerosol direct effect, Aerosol indirect effect, WRF-Chem},
abstract = {The promotion of renewable energy as a substitute for fossil fuels is the key solution to achieve the goals established during the United Nations Climate Change Conference in Glasgow (COP26) based on which member countries agreed to phase down coal power and achieve net-zero carbon emissions. Among various renewable energy sources, solar energy is an attractive option that will have a significant effect on the future energy supply and energy use. Therefore, we selected the period of 2016–2020 during which the aerosol concentration gradually decreased due to strict pollutant control measures to evaluate solar energy simulations based on the Weather Research Forecast-Chemistry (WRF-Chem) model. We also analyzed the contributions of the aerosol direct effect (ADE), aerosol indirect effect (AIE), and cloud radiation effect (CRE) to solar energy trends by conducting sensitivity experiments. The results show that the WRF-Chem model performs well for the 2 m temperature (T2), cloud fraction, PM2.5, solar energy trends during 2016–2020. There are regional and seasonal differences in the contributions of ADE, AIE, and CRE to solar energy trends, with a decrease in ADE contributions and an increase in CRE contributions from north to south in China, and the AIE contribution being relatively slight. On an annual scale, ADE is the main contributor to the increase in solar energy trends in the Beijing-Tianjin-Hebei (89%) and Fenwei Plains (83.9%) from 2016 to 2020, which is related to the horizontal distribution of PM2.5. In the Yangtze River Delta and other regions, ADE and CRE contributed equally to the increase in solar energy trends, about 40%. In the Pearl River Delta and Sichuan Basin, the contribution of CRE is larger than that of AIE and ADE, the Pearl River Delta region is the largest contributor of CRE to the annual solar energy trends among the five major urban agglomerations, with a contribution of 78.4%, and Sichuan basin is the only region where CRE has a negative contribution to the annual solar energy trends (−59.1%). On the seasonal scale, the contribution of CRE is dominant except for the greater positive contribution of ADE to the solar energy trends in spring, summer, and autumn in Beijing-Tianjin-Hebei and in autumn in Fenwei Plain.}
}
@article{SAKAI2023838,
title = {Effects of scan parameters on the accuracies of iodine quantification and hounsfield unit values in dual layer dual-energy head and neck computed tomography: A phantom study conducted in a hospital in Japan},
journal = {Radiography},
volume = {29},
number = {5},
pages = {838-844},
year = {2023},
issn = {1078-8174},
doi = {https://doi.org/10.1016/j.radi.2023.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1078817423001268},
author = {Y. Sakai and T. Shirasaka and K. Hioki and S. Yamane and E. Kinoshita and T. Kato},
keywords = {Dual-energy computed tomography, Dual layer computed tomography, Head and neck imaging, Scan parameter},
abstract = {Introduction
No study has investigated scan parameters in head and neck dual layer dual-energy computed tomography (DL-DECT). This study aimed to select the appropriate scan parameters in head and neck imaging by evaluating the scan parameter effects on the accuracies of CT numbers and conduct iodine quantification in DL-DECT.
Methods
A multi-energy phantom was scanned using a dual layer CT (DLCT) scanner. Reference materials of iodine, blood, calcium, and adipose were used. A helical scan was performed by using reference and several protocols. Iodine density and virtual monochromatic images (VMIs) at the energy of 50, 70, and 100 keV were reconstructed. The iodine concentrations and CT numbers in each protocol were measured. Moreover, the absolute percentage errors (APEs) of iodine quantifications and CT numbers (reference vs. each protocol) were compared. Equivalence was observed when APEs between reference and each protocol was within 5%. Statistical analysis was performed using appropriate software.
Results
The APEs between the high-tube-voltage and reference protocol were 23.7, 14.0, 8.8, and 8.1% for iodine reference materials with concentrations equal to 2, 5, 10, and 15 mg/ml, respectively. At 50 keV, APEs between the high-tube-voltage and reference protocols were greater than 5% except for calcium and adipose. At 100 keV, APEs between the high-tube-voltage and reference protocols were greater than 5% except for blood and calcium.
Conclusions
The high-tube-voltage protocol improved the accuracies of the measurement for iodine quantification and CT numbers. Additionally, the scanning parameters except for tube voltage had no effect on accuracies of iodine quantitation and CT numbers in the DLCT scanner.
Implications for practice
The use of the high-tube-voltage protocol will be recommended for more accurate material decomposition in head and neck DL-DECT.}
}
@article{MILLER20151,
title = {Introduction to special issue on Computing for a Greener Water/Energy/Emissions Nexus},
journal = {Sustainable Computing: Informatics and Systems},
volume = {8},
pages = {1-2},
year = {2015},
note = {Special Issue on Computing for a Greener Water/Energy/Emissions Nexus; edited by Carol J. Miller.andSpecial Issue on Green Mobile Cloud Computing (Green MCC); edited by Danielo G. Gomes, Rafael Tolosana-Calasanz, and Nazim Agoulmine.},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2015.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2210537915000517},
author = {Carol J. Miller}
}
@article{GENNARO2023110002,
title = {Modelling double skin façades (DSFs) in whole-building energy simulation tools: Validation and inter-software comparison of naturally ventilated single-story DSFs},
journal = {Building and Environment},
volume = {231},
pages = {110002},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110002},
url = {https://www.sciencedirect.com/science/article/pii/S036013232300029X},
author = {Giovanni Gennaro and Elena {Catto Lucchino} and Francesco Goia and Fabio Favoino},
keywords = {Inter-software comparison, Empirical validation, Double skin façade, Naturally ventilated cavities, EnergyPlus, TRNSYS, IDA-ICE, IES-VE},
abstract = {Building energy simulation (BES) tools offer the possibility to integrate double skin façade (DSF) technologies into whole building simulation through dedicated modules or possible workarounds. However, the reliability of such tools in predicting the dynamic heat and mass transfer processes within the DSFs is still to be determined. Therefore, this paper aims to assess the performance of four popular BES tools (i.e. EnergyPlus, TRNSYS, IDA-ICE and IES-VE) in predicting the thermal behaviour of one-storey naturally ventilated DSF in three different ventilation modes. To evaluate their capability to predict thermophysical quantities, we compared the simulation results with experimental data. The results show that it is not possible to identify a tool that outperforms the others for all the analysed quantities, especially for the cavity air temperature, which is the least accurate parameter in all software due to underestimation of the daytime peak. IES-VE seems to be most accurate for Supply Air and Thermal Buffer modes when shading is deployed, while EnergyPlus appears most accurate for Outdoor Air Curtain mode. When it comes to surface temperatures and transmitted solar radiation, TRNSYS appears to be the best-performing software. In addition, this study investigated the challenges that designers may face when modelling a naturally ventilated DSF using whole-building simulation tools. Moreover, the investigation elucidates the challenges that have a more significant effect on the performance of the BES tools in order to reinforce their reliability.}
}
@article{PENATRUJILLO20231069,
title = {Pediatric Applications of Dual-Energy Computed Tomography},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {1069-1083},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S003383892300132X},
author = {Valeria Peña-Trujillo and Sebastian Gallo-Bernal and Erik L. Tung and Michael S. Gee},
keywords = {Dual-energy CT (DECT), Pediatrics, Radiation dose, High-pitch CT}
}
@article{GUO2023852,
title = {Use of dual energy computed tomography versus conventional techniques for preoperative localization in primary hyperparathyroidism: Effect of preoperative calcium and parathyroid hormone levels},
journal = {The American Journal of Surgery},
volume = {225},
number = {5},
pages = {852-856},
year = {2023},
issn = {0002-9610},
doi = {https://doi.org/10.1016/j.amjsurg.2023.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S000296102300017X},
author = {Michael Guo and Daniel B. Lustig and Debon Lee and Neraj Manhas and Sam M. Wiseman},
keywords = {Primary hyperparathyroidism, Parathyroidectomy, Dual energy computed tomography, Sestamibi, Pre-operative planning, Image localization, Calcium, Parathyroid hormone},
abstract = {Background
We aimed to investigate the association of preoperative calcium and parathyroid hormone (PTH) levels with sensitivity and accuracy of dual energy computed tomography (DECT), single-photon emission CT with 99mTc-sestamibi (CT-MIBI), and ultrasound (US) for pre-operative localization primary hyperparathyroid (PHP) patients.
Methods
Patients undergoing parathyroidectomy for PHP at a tertiary care facility who underwent DECT, CT-MIBI and US between 2012 and 2021 were stratified by preoperative calcium and PTH levels.
Results
Of 278 patients, those with high calcium and PTH levels had a higher sensitivity and accuracy with DECT (87.7%, 85.2%) compared to CT-MIBI (82.3%, 79.0%), and US (61.7%, 53.1%). DECT was more sensitive and accurate than other preoperative localization techniques in subgroups with normal PTH (DECT sensitivity 60.9%, accuracy 52.1%) and normal calcium levels (41.7%, 33.3%).
Conclusion
Preoperative calcium and PTH were associated with sensitivity and accuracy of pre-operative localization in PHP. DECT was sensitive and accurate for preoperative localization compared to other first-line imaging techniques.}
}
@article{RAJPUT2023105795,
title = {Local bit line 8T SRAM based in-memory computing architecture for energy-efficient linear error correction codec implementation},
journal = {Microelectronics Journal},
volume = {137},
pages = {105795},
year = {2023},
issn = {0026-2692},
doi = {https://doi.org/10.1016/j.mejo.2023.105795},
url = {https://www.sciencedirect.com/science/article/pii/S0026269223001088},
author = {Anil Kumar Rajput and Manisha Pattanaik},
keywords = {In-memory computing, Energy-efficiency, SRAM, Error Correction Code (ECC), Hamming code},
abstract = {Memory reliability is a critical issue in SRAM-based In-Memory Computing (IMC) architecture. The rapid advance in transistor technology makes SRAM more sensitive to soft errors. Various developments have recommended different methods to prevent store data corruption using Hamming Code. In order to encode or decode the information, a piece of Error Correction Code (ECC) hardware is necessary, which results in an area and energy overhead. In this paper, an IMC-based linear Error Correction Codec (IMC-ECC) hardware for parity check bit generation (encoding) and syndrome vector generation (decoding) is implemented by using the proposed Transmission Pass Gate (TPG) Local Bit-line (LB-8T) SRAM structure based IMC architecture. The local bit line structure resolves the half-select issues that occur for bit-interleave architecture. The TPG improves the write stability by delivering both strong VDD and GND. Furthermore, the proposed IMC-ECC scheme, along with bit-interleave architecture, makes the IMC architecture more resilient to multi-bit soft error. The proposed IMC-ECC scheme is validated by implementing a 4 Kb LB-8T SRAM array in 65 nm CMOS technology and Hamming code (7, 4) as an example. The proposed IMC-ECC scheme achieves the average energy consumption of 0.294 pJ/bit and 0.075 pJ/bit for the encoding and decoding process, respectively, at a supply voltage of 1 V.}
}
@article{LI2023357,
title = {An energy fault and consumption optimization strategy in wireless sensor networks with edge computing},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {1},
pages = {357-367},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822004219},
author = {Guozhi Li and Yan Tong and Ge Zhang and Yue Zeng},
keywords = {Artificial intelligence, Causality reasoning graph, Energy hole and consumption balance (EHCB) problem, NS-2},
abstract = {With the operation of wireless sensor networks (WSNs), energy-constrained sensor nodes and inadequate energy constraint methods are becoming obsolete, as they reduce the efficiency of data transmission. The appearance of edge computing (EC) and causality graph provide a new opportunity for energy fault analysis and assessment in WSNs. As a result, by selecting a reliable data transmission path and averaging the energy consumption, we present an energy fault and consumption optimization (EFCO) algorithm for solving the energy hole and consumption balance (EHCB) problem in wireless sensor networks with edge computing (ECWSNs). Specifically, we first build a novel four-layer network architecture by using edge computing technology and causality graph theory. Then, the energy fault cost (EFC) in ECWSNs is formulated as an optimization problem that is constrained by the energy allocation of relay nodes. Furthermore, we propose a causal reasoning algorithm to deduce the single-value fault status probability of relay nodes. Finally, we utilize version 2 of a network simulator (NS-2) to evaluate the fault derivation and energy allocation efficiency of the EFCO algorithm in ECWSNs.}
}
@article{XIE2023100130,
title = {An energy-efficient resource allocation strategy in massive MIMO-enabled vehicular edge computing networks},
journal = {High-Confidence Computing},
volume = {3},
number = {3},
pages = {100130},
year = {2023},
issn = {2667-2952},
doi = {https://doi.org/10.1016/j.hcc.2023.100130},
url = {https://www.sciencedirect.com/science/article/pii/S2667295223000284},
author = {Yibin Xie and Lei Shi and Zhenchun Wei and Juan Xu and Yang Zhang},
keywords = {Vehicular edge computing, Massive MIMO, Resource allocation, Energy-efficient},
abstract = {The vehicular edge computing (VEC) is a new paradigm that allows vehicles to offload computational tasks to base stations (BSs) with edge servers for computing. In general, the VEC paradigm uses the 5G for wireless communications, where the massive multi-input multi-output (MIMO) technique will be used. However, considering in the VEC environment with many vehicles, the energy consumption of BS may be very large. In this paper, we study the energy optimization problem for the massive MIMO-based VEC network. Aiming at reducing the relevant BS energy consumption, we first propose a joint optimization problem of computation resource allocation, beam allocation and vehicle grouping scheme. Since the original problem is hard to be solved directly, we try to split the original problem into two subproblems and then design a heuristic algorithm to solve them. Simulation results show that our proposed algorithm efficiently reduces the BS energy consumption compared to other schemes.}
}
@article{XU2022106397,
title = {Reconstructing all-weather daytime land surface temperature based on energy balance considering the cloud radiative effect},
journal = {Atmospheric Research},
volume = {279},
pages = {106397},
year = {2022},
issn = {0169-8095},
doi = {https://doi.org/10.1016/j.atmosres.2022.106397},
url = {https://www.sciencedirect.com/science/article/pii/S0169809522003830},
author = {Fubao Xu and Jianrong Fan and Chao Yang and Jiali Liu and Xiyu Zhang},
keywords = {Land surface temperature (LST), Cloud radiation effect, XGBoost, Passive microwave, Tibetan Plateau (TP)},
abstract = {Land surface temperature (LST) has been used in many applications as its strong relationships with land surface processes. However, the greatest limitation of the use of LST is the missing data caused by cloud contamination and weather conditions. In this study, we first used the XGBoost method to describe complex relationships of LST with surface characteristics from clear-sky pixels, and applied the model to retrieve hypothetical clear-sky LST under cloudy sky. Secondly, cloud radiative effect (CRE) on the LST was calculated based on energy balance using the reanalysis data. The models were applied to reconstruct the all-weather LST over the Tibetan Plateau (TP). The spatial patterns of reconstructed LST indicated that our model could produce completely spatial-seamless LST and depict the detailed information. The accuracy of the XGBoost LST was validated against the clear-sky MODIS LST (average R2 = 0.92, MAPE = 0.52, RMSE = 2.32 K). The CRE-EB LST was evaluated using data from six in-situ sites from the TP. The validation results were separated into three conditions: clear sky (RMSE = 3.01 K–3.52 K, R2 = 0.88–0.93, bias = −1.08 K-1.88 K), cloudy sky (RMSE = 3.31 K–4.06 K, R2 = 0.87–0.92, bias = −0.21 K-1.11 K), and overall (RMSE = 3.31 K–3.82 K, R2 = 0.88–0.93, bias = −0.42 K-1.24 K). Compared to existing all-weather LST datasets, the temporal variability of our LST data shows similar seasonal and daily changes, and the CRE-EB LST has advantages in terms of image quality and accuracies under cloudy condition. This study demonstrated the utility of proposed models to reconstruct all-weather LST.}
}
@article{ZUO2023312,
title = {Joint resource optimization and trajectory design for energy minimization in UAV-assisted mobile-edge computing systems},
journal = {Computer Communications},
volume = {203},
pages = {312-323},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2023.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0140366423000932},
author = {Bangfu Zuo and Yu Xu and Dingcheng Yang and Lin Xiao and Tiankui Zhang},
keywords = {UAV trajectory design, Mobile edge computing (MEC), Task partition},
abstract = {This paper studies an unmanned aerial vehicle (UAV)-assisted mobile-edge computing (MEC) system, where a rotary-wing UAV equipped with computing platforms is used to assist ground users (GUs) with insufficient computing resource. Each GU offloads part of the task to the UAV for computation through the uplink, and the remaining computation task is computed by itself. Then, UAV will return the results to the corresponding GU via downlink. Considering the results for each GU is not only used by itself, but may also be sent to other GUs for data fusion, we thus design two specific scenarios in this paper: (1) homologous UAV-assisted MEC scenario, i.e. UAV receives the computation task of GUs and then returns the results to the identical GUs, and (2) non-homologous UAV-assisted MEC scenario, i.e., UAV receives the computation task of source GUs and returns the results to the corresponding destination GUs. For these two scenarios, we propose an energy minimization problem by jointly optimizing task partition, the time slot length and UAV trajectory. In particular, we use the path discretization approach to convert the problem as a problem form which is finite variables. Since the problem is non-convex, we use successive convex approximation (SCA) techniques to tackle the non-convexity. Considering the initial trajectory has an important affect on the experimental result, then a specific trajectory initialization design via combining with the Pickup-and-Delivery Problem (PDP) is proposed. Numerical results proof our proposed design is superior compared with the benchmark cases.}
}
@article{TANG2023,
title = {Energy-optimal DNN model placement in UAV-enabled edge computing networks},
journal = {Digital Communications and Networks},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2023.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S235286482300038X},
author = {Jianhang Tang and Guoquan Wu and Mohammad Mussadiq Jalalzai and Lin Wang and Bing Zhang and Yi Zhou},
keywords = {UAV-Enabled edge computing, DNN model Placement, 6G networks, Inference tasks},
abstract = {Unmanned aerial vehicle (UAV)-enabled edge computing is emerging as a potential enabler for Artificial Intelligence of Things (AIoT) in the forthcoming sixth-generation (6G) communication networks. With the use of flexible UAVs, massive sensing data is gathered and processed promptly without considering geographical locations. Deep neural networks (DNNs) are becoming a driving force to extract valuable information from sensing data. However, the lightweight servers installed on UAVs are not able to meet the extremely high requirements of inference tasks due to the limited battery capacities of UAVs. In this work, we investigate a DNN model placement problem for AIoT applications, where the trained DNN models are selected and placed on UAVs to execute inference tasks locally. It is impractical to obtain future DNN model request profiles and system operation states in UAV-enabled edge computing. The Lyapunov optimization technique is leveraged for the proposed DNN model placement problem. Based on the observed system overview, an advanced online placement (AOP) algorithm is developed to solve the transformed problem in each time slot, which can reduce DNN model transmission delay and disk I/O energy cost simultaneously while keeping the input data queues stable. Finally, extensive simulations are provided to depict the effectiveness of the AOP algorithm. The numerical results demonstrate that the AOP algorithm can reduce 18.14% of the model placement cost and 29.89% of the input data queue backlog on average by comparing it with benchmark algorithms.}
}
@article{BOGOT2023,
title = {Distribution of Aeration and Pulmonary Blood Volume in Healthy, ARDS and COVID-19 Lungs: A Dual-Energy Computed Tomography Retrospective Cohort Study},
journal = {Academic Radiology},
year = {2023},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2023.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1076633223000326},
author = {Naama R. Bogot and Roee Steiner and Yigal Helviz and Chedva Weiss and Konstantin Cherniavsky and Olga Pichkhadze and Lorenzo Ball and Yigal Frank and Philip Levin and Paolo Pelosi and Ofer Benjaminov and Sharon Einav},
keywords = {Dual-energy computed tomography, COVID-19 lung disease, ARDS, lung aeration, pulmonary blood volume, aeration-blood-volume ratio},
abstract = {Rationale and Objectives
Few reports have studied lung aeration and perfusion in normal lungs, COVID-19, and ARDS from other causes (NC-ARDS) using dual-energy computed tomography pulmonary angiograms (DE-CTPA). To describe lung aeration and blood-volume distribution using DE-CTPAs of patients with NC-ARDS, COVID-19, and controls with a normal DE-CTPA (“healthy lungs”). We hypothesized that each of these conditions has unique ranges of aeration and pulmonary blood volumes.
Materials and Methods
This retrospective, single-center study of DE-CTPAs included patients with COVID-19, NC-ARDS (Berlin criteria), and controls. Patients with macroscopic pulmonary embolisms were excluded. The outcomes studied were the (1) lung blood-volume in areas with different aeration levels (normal, ground glass opacities [GGO], consolidated lung) and (2) aeration/blood-volume ratios.
Results
Included were 20 patients with COVID-19 (10 milds, 10 moderate-severe), six with NC-ARDS, and 12 healthy-controls. Lung aeration was lowest in patients with severe COVID-19 24% (IQR13%–31%) followed by those with NC-ARDS 40%(IQR21%–46%). Blood-volume in GGO was lowest in patients with COVID-19 [moderate-severe:-28.6 (IQR-33.1–23.2); mild: -30.1 (IQR-33.3–23.4)] and highest in normally aerated areas in NC-ARDS -37.4 (IQR-52.5–30.2-) and moderate-severe COVID-19 -33.5(IQR-44.2–28.5). The median aeration/blood-volume ratio was lowest in severe COVID-19 but some values overlapped with those observed among patients with NC-ARDS.
Conclusion
Severe COVID-19 disease is associated with low total aerated lung volume and blood-volume in areas with GGO and overall aeration/blood volume ratios, and with high blood volume in normal lung areas. In this hypothesis-generating study, these findings were most pronounced in severe COVID disease. Larger studies are needed to confirm these preliminary findings.}
}
@article{LEE2023102787,
title = {Scale-CIM: Precision-scalable computing-in-memory for energy-efficient quantized neural networks},
journal = {Journal of Systems Architecture},
volume = {134},
pages = {102787},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102787},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122002727},
author = {Young Seo Lee and Young-Ho Gong and Sung Woo Chung},
keywords = {Digital-based computing-in-memory, Quantized neural networks, Precision-scalable computation},
abstract = {Quantized neural networks (QNNs), which perform multiply-accumulate (MAC) operations with low-precision weights or activations, have been widely exploited to reduce energy consumption. QNNs usually have a trade-off between energy consumption and accuracy depending on the quantized precision, so that it is necessary to select an appropriate precision for energy efficiency. Nevertheless, the conventional hardware accelerators such as Google TPU are typically designed and optimized for a specific precision (e.g., 8-bit), which may degrade energy efficiency for other precisions. Though an analog-based computing-in-memory (CIM) technology supporting variable precision has been proposed to improve energy efficiency, its implementation requires extremely large and power-consuming analog-to-digital converters (ADCs). In this paper, we propose Scale-CIM, a precision-scalable CIM architecture which supports MAC operations based on digital computations (not analog computations). Scale-CIM performs binary MAC operations with high parallelism, by executing digital-based multiplication operations in the CIM array and accumulation operations in the peripheral logic. In addition, Scale-CIM supports multi-bit MAC operations without ADCs, based on the binary MAC operations and shift operations depending on the precision. Since Scale-CIM fully utilizes the CIM array for various quantized precisions (not for a specific precision), it achieves high compute-throughput. Consequently, Scale-CIM enables precision-scalable CIM-based MAC operations with high parallelism. Our simulation results show that Scale-CIM achieves 1.5∼15.8 × speedup and reduces system energy consumption by 53.7∼95.7% across different quantized precisions, compared to the state-of-the-art precision-scalable accelerator.}
}
@article{MATERWALA2022205,
title = {Energy-SLA-aware genetic algorithm for edge–cloud integrated computation offloading in vehicular networks},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {205-222},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22001327},
author = {Huned Materwala and Leila Ismail and Raed M. Shubair and Rajkumar Buyya},
keywords = {Computation offloading, Edge–cloud computing, Energy-efficiency, Evolutionary genetic optimization algorithm, Quality of service (QoS), Vehicular Ad Hoc Networks (VANET)},
abstract = {Vehicular Ad Hoc Networks (VANET) is an emerging technology that enables a comfortable, safe, and efficient travel experience by providing mechanisms to execute applications related to traffic congestions, road accidents, autonomous driving, and entertainment. The mobile vehicles in VANET are characterized by low computational and storage capabilities. In such scenarios, to meet applications’ performance requirements, requests from vehicles are offloaded to edge and cloud servers. The high energy consumption of these servers increases operating costs and threatens the environment. Energy-aware offloading strategies have been introduced to tackle this problem. Existing works on computation offloading focus on optimizing the energy consumption of either the IoT devices/mobile/vehicles and/or the edge servers. This paper proposes a novel offloading algorithm that optimizes the energy of edge–cloud integrated computing platforms based on Evolutionary Genetic Algorithm (EGA) while maintaining applications’ Service Level Agreement (SLA). The proposed algorithm employs an adaptive penalty function to incorporate the optimization constraints within EGA. Comparative analysis and numerical experiments are carried out between the proposed algorithm, random and genetic algorithm-based offloading, and no offloading baseline approaches. On average, the results show that the proposed algorithm saves 2.97 times and 1.37 times more energy than the random and no offloading algorithms respectively. Our algorithm has 0.3% of violations versus 52.8% and 62.8% by the random and no offloading approaches respectively. While the energy-non-SLA-aware genetic algorithm saves, on average, 1.22 times more energy than our approach, however, it violates SLAs by 159 times more than our proposed approach.}
}
@article{WU2022123235,
title = {Developing a holistic fuzzy hierarchy-cloud assessment model for the connection risk of renewable energy microgrid},
journal = {Energy},
volume = {245},
pages = {123235},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.123235},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222001384},
author = {Zhongqun Wu and Chan Yang and Ruijin Zheng},
keywords = {Renewable energy microgrid, Grid-connected risk, Risk assessment model},
abstract = {Grid-connection of renewable energy microgrids (GCREM) is an important form of promoting the use of clean and renewable energy (RE). However, GCREM will bring huge risks to the power grid. Accurate evaluation and control of risks are critical to the development of GCREM. The existing research is not deep and comprehensive enough to make a reliable evaluation on the risks. The current study proposes a new evaluation framework based on the unit decomposition of the system of GCREM, and then builds a holistic fuzzy hierarchy-cloud assessment model for the risks. The main contributions of this paper are as follows: (1) decomposes GCREM risks to each functional unit of the system, and clarifies the risk transmission between the units within the system; (2) improves the accuracy of weighting risk variables based on interval type-2 fuzzy method; (3) realizes the visibility of risk evaluation and a prior judgment on its effectiveness; (4) establishes a 5-dimensional risk evaluation system of GCREM for the first time, which achieved full coverage of risk variables without the overlap between dimensions. The empirical analyses show that our model can not only assess the overall risk level of GCREM, but also identify key risk sources.}
}
@article{SHI2022103749,
title = {A cloud-based energy management strategy for hybrid electric city bus considering real-time passenger load prediction},
journal = {Journal of Energy Storage},
volume = {45},
pages = {103749},
year = {2022},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.103749},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21014237},
author = {Junzhe Shi and Bin Xu and Xingyu Zhou and Jun Hou},
keywords = {Electric city bus, Cloud computing, Vehicle to infrastructure, Dynamic programming, Passenger load prediction},
abstract = {Electric city bus gains popularity in recent years for its low greenhouse gas emission, low noise level, etc. Different from a passenger car, the weight of a city bus varies significantly with different amounts of onboard passengers. After analyzing the importance of battery aging and passenger load effects on an optimal energy management strategy, this study introduces the passenger load prediction into the hybrid-electric city buses energy management problem, which is not well studied in the existing literature. The average model, Decision Tree, Gradient Boost Decision Tree, and Neural Networks models are compared in the passenger load prediction. The Gradient Boost Decision Tree model is selected due to its best accuracy and high stability. Given the predicted passenger load, a dynamic programming algorithm determines the optimal power demand for supercapacitor and battery by optimizing the battery aging and energy usage leveraging cloud techniques. Then, rule extraction is conducted on dynamic programming results, and the rule is real-time loaded to the vehicle onboard controller to handle prediction errors and uncertainties. The proposed cloud-based Dynamic Programming and rule extraction framework with the passenger load prediction show 4% and 11% lower bus operating costs in off-peak and peak hours, respectively. The operating cost by the proposed framework is less than 1% of the dynamic programming with the true passenger load information.}
}
@article{MEER20231097,
title = {Dual-Energy Computed Tomography and Beyond: Musculoskeletal System},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {1097-1110},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0033838923001343},
author = {Emtenen Meer and Mitulkumar Patel and Darren Chan and Adnan M. Sheikh and Savvas Nicolaou},
keywords = {Dual-energy CT, Musculoskeletal imaging, Gout, Collagen analysis}
}
@article{ZHAO2022156,
title = {An energy and carbon-aware algorithm for renewable energy usage maximization in distributed cloud data centers},
journal = {Journal of Parallel and Distributed Computing},
volume = {165},
pages = {156-166},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S074373152200079X},
author = {Daming Zhao and Jiantao Zhou},
keywords = {Carbon emission, Energy consumption, Renewable energy, Virtual machine placement},
abstract = {The vigorous development and the increasing popularity of cloud computing highlight the necessity of reducing data center energy consumption and the environmental impact of carbon dioxide emissions. For geographically distributed data centers, cloud servers are connected to the conventional power grid and in addition they are supported by an attached renewable energy source. Since the carbon footprint rate of energy consumption has dynamic differences in space, reducing energy consumption does not mean decrease carbon emission, which indicates that energy consumption and carbon footprint need to be synergistically optimized. In this paper, an energy and carbon-aware algorithm for virtual machine placement is proposed. The goal is to obtain a virtual machine allocation scheme that aims to achieve the trade-off between energy consumption and carbon emissions by improving renewable energy utilization. The experimental results show that the proposed approach is more energy-efficient and greener, which can also maximize the renewable energy utilization with 73.11% while ensuring the SLA violation with 0.2% in comparison to the baseline algorithms.}
}
@article{ANANTHAKRISHNAN2023963,
title = {Dual-Energy Computed Tomography: Integration Into Clinical Practice and Cost Considerations},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {963-971},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S003383892300129X},
author = {Lakshmi Ananthakrishnan and Naveen Kulkarni and Aran Toshav},
keywords = {Multi-energy CT, Workflow optimization, Dual-energy CT, Efficiency, Spectral}
}
@article{YANG2023178,
title = {Joint heterogeneity-aware personalized federated search for energy efficient battery-powered edge computing},
journal = {Future Generation Computer Systems},
volume = {146},
pages = {178-194},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001644},
author = {Zhao Yang and Shengbing Zhang and Chuxi Li and Miao Wang and Jiaying Yang and Meng Zhang},
keywords = {Federated learning, Heterogeneity-aware, Personalization, Energy efficient, Federated search, Battery-powered edge device},
abstract = {The limited battery capacity of edge devices has a significant impact on the deployment of Federated Learning (FL). As a result, maintaining computation sustainability is a critical issue for edge FL. Furthermore, the heterogeneities of deployed edge devices reduce FL efficiency and effectiveness, making FL computation sustainability more challenging to maintain. To address these issues raised by heterogeneities, we perform a joint heterogeneity-aware personalized federated search for energy-efficient edge computing. To achieve energy-efficient on-device inference and training, a one-training process is adopted to search for personalized partial network structures on each device. We begin by tailoring the network scale on each node based on the efficiency of model inference, which also serves as the search space for optimization. This strategy can mitigate the straggler problem and improve the energy efficiency of FL by guiding the efficient FL training process in each round. To further optimize the energy consumption of edge devices, we design a lightweight search controller during the search process. This controller meets the low energy consumption requirements of the edge devices and reduces their energy consumption during the search process. Finally, we introduce an adaptive search strategy to guarantee personalized training convergence on each device. By reducing the energy consumption of each training round and ensuring the training convergence of personalized models, we can significantly improve the energy efficiency of FL on battery-powered edge devices. Our framework can obtain competitive accuracy with state-of-the-art methods while improving inference efficiency by up to 1.43× and training energy efficiency by up to 2.63×.}
}
@article{JUNCKLAUSMARTINS2022100019,
title = {Systematic review of nowcasting approaches for solar energy production based upon ground-based cloud imaging},
journal = {Solar Energy Advances},
volume = {2},
pages = {100019},
year = {2022},
issn = {2667-1131},
doi = {https://doi.org/10.1016/j.seja.2022.100019},
url = {https://www.sciencedirect.com/science/article/pii/S2667113122000079},
author = {Bruno {Juncklaus Martins} and Allan Cerentini and Sylvio Luiz Mantelli and Thiago Zimmermann {Loureiro Chaves} and Nicolas {Moreira Branco} and Aldo {von Wangenheim} and Ricardo Rüther and Juliana {Marian Arrais}},
keywords = {Nowcasting, Cloud, Solar energy, Image processing, Ground, Photovoltaic,},
abstract = {Nowcasting of solar energy considering clouds is important for photovoltaic solar plants and distributed systems. Clouds present a challenge for modeling, due to constant changes in shape and size, and are dependent on local atmospheric conditions. Several methods are being used for the automatic assessment of clouds from the surface to predict solar power generation, assisted by camera, side sensors, etc. During our research we did not find a Systematic Literature Review on this topic. This review is intended to search the related scientific articles to find the state of the art in the area from the period of 2011–2020. We found 65 articles to review after the meta-analysis. We look for the main short-term forecasting methods used. The majority of articles rely on classical statistics approaches based on historical data. Yet recent articles show that this trend might be shifting towards Machine Learning approaches. Our analysis shows that most articles found are based on images captured by fish-eye lenses using a single camera. The most common forecasting techniques are Artificial Neural Networks and Convolutional Neural Networks, with the root mean squared error being the most predominant error metric used for model validation among both classical and Machine Learning approaches.}
}
@article{MENG2022,
title = {Green resource allocation for mobile edge computing},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000220},
author = {Anqi Meng and Guandong Wei and Yao Zhao and Xiaozheng Gao and Zhanxin Yang},
keywords = {Mobile edge computing, Green communications, Mixed-integer programming, Resource allocation},
abstract = {We investigate the green resource allocation to minimize the energy consumption of the users in mobile edge computing systems, where task offloading decisions, transmit power, and computation resource allocation are jointly optimized. The considered energy consumption minimization problem is a non-convex mixed-integer non-linear programming problem, which is challenging to solve. Therefore, we develop a joint search and Successive Convex Approximation (SCA) scheme to optimize the non-integer variables and integer variables in the inner loop and outer loop, respectively. Specifically, in the inner loop, we solve the optimization problem with fixed task offloading decisions. Due to the non-convex objective function and constraints, this optimization problem is still non-convex, and thus we employ the SCA method to obtain a solution satisfying the Karush-Kuhn-Tucker conditions. In the outer loop, we optimize the offloading decisions through exhaustive search. However, the computational complexity of the exhaustive search method is greatly high. To reduce the complexity, a heuristic scheme is proposed to obtain a sub-optimal solution. Simulation results demonstrate the effectiveness of the developed schemes.}
}
@article{DELUCIA2023207,
title = {Unlocking the potential of edge computing for hyperspectral image classification: An efficient low-energy strategy},
journal = {Future Generation Computer Systems},
volume = {147},
pages = {207-218},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23001802},
author = {Gianluca {De Lucia} and Marco Lapegna and Diego Romano},
keywords = {Hyperspectral classification, Edge Computing, Principal Component Analysis, GPU computing},
abstract = {Despite recent improvements, the computing capability of Edge Computing devices is still inferior to high-end servers, so special methodologies are required to consider the computing environment while developing algorithms. In the present work, we propose a hybrid technique to make the classification of Hyperspectral Images feasible and effective through a Convolutional Neural Network on low-power and high-performance sensor devices. More specifically, we combine two strategies: we initially use the Principal Component Analysis method to discard non-significant wavelengths and shrink the dataset; then, we apply a process acceleration method to boost performance by implementing a form of GPU-based parallelism. The experiments demonstrate the technique’s effectiveness in terms of performance and energy consumption: it enables correct classifications even with low-power devices often deployed on Unmanned Aerial Vehicles, where the network connection is unpredictable or erratic.}
}
@article{YOU2022252,
title = {MQDS: An energy saving scheduling strategy with diverse QoS constraints towards reconfigurable cloud storage systems},
journal = {Future Generation Computer Systems},
volume = {129},
pages = {252-268},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21004696},
author = {Xindong You and Dawei Sun and Xueqiang Lv and Shang Gao and Rajkumar Buyya},
keywords = {Reconfigurable disk, Quality of Service, Energy efficiency, Cloud storage, Scheduling algorithms},
abstract = {Nowadays, cloud storage systems are usually constructed by heterogeneous disks with reconfigurable speed for energy efficiency. Scheduling tasks with diverse QoS (Quality of Service) requirements to disks with different speeds is seldom considered in state-of-the-art mainstream disk scheduling algorithms. This paper proposes a multi-QoS disk scheduling strategy (MQDS), aiming to support energy-saving and diverse QoS constraints towards reconfigurable heterogeneous cloud storage systems. Three algorithms are designed in MQDS: Time Prior Disks Algorithm (TPDS) for time-sensitive tasks, Cost Prior Disks Algorithm (CPDS) for cost-sensitive tasks, and Benefit Function-based Disks Algorithm (BFDS) for tasks with time-varying QoS requirements. CloudSimDisk simulator is extended to evaluate the response time, energy consumption and cost expenditure performance of the proposed algorithms, for comparison with the extended Robin Round Disk scheduling algorithm (E-RRDS). Extensive experimental results demonstrate that the three proposed algorithms outperform E-RRDS. TPDS consumes shortest response time and CPDS has the least cost; and BFDS achieves balance between TPDS and CPDS, which provide more options to users. It is worth mentioning that all the algorithms in MQDS are more energy efficient than E-RRDS. As a whole, the proposed MQDS is energy efficient and able to accommodate diverse QoS constraints well.}
}
@article{ZHU2022108458,
title = {Cloud-edge collaborative distributed optimal dispatching strategy for an electric-gas integrated energy system considering carbon emission reductions},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {143},
pages = {108458},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2022.108458},
url = {https://www.sciencedirect.com/science/article/pii/S0142061522004677},
author = {Xu Zhu and Jun Yang and Xiangpeng Zhan and Yuanzhang Sun and Yuwei Zhang},
keywords = {Alternating direction method of multipliers, Integrated energy system, Distributed optimization, Carbon trading, Demand response},
abstract = {To reduce carbon emissions and improve the energy efficiency of energy systems, integrated energy systems (IESs) have been promoted in recent years. However, the data of different energy networks cannot be fully shared. The data privacyprotection is attracting more attention in demand side. It is difficult for centralized dispatching strategies, which must obtain all the data of energy systems during optimization, to be applied. To solve these issues, a cloud-edge collaborative distributed optimal dispatching strategy for electric-gas IESs is proposed in this paper. The centralized IES scheduling problem is reasonably divided into multiple subproblems. Considering information barrier, the cloud computing centers are separately set in different energy networks to solve energy flow optimization subproblems. Considering prosumer privacy protection, edge computing units are separately set in energy hubs to deal with cost and carbon emissions minimization of regional integrated energy systems (RIESs). Based on the proximal Jacobian alternating direction method of multipliers, the common optimization variables interactive iteration and parallel solution of multiple dispatching models are developed. The simulation results show that the proposed distributed optimization method can achieve the same accuracy as the centralized optimization method and improve problem-solvingefficiency in general.}
}
@article{NANDANWAR2023100654,
title = {Real-time computing of power flows and node voltages in electrical energy network using decision trees},
journal = {Cleaner Engineering and Technology},
volume = {15},
pages = {100654},
year = {2023},
issn = {2666-7908},
doi = {https://doi.org/10.1016/j.clet.2023.100654},
url = {https://www.sciencedirect.com/science/article/pii/S2666790823000599},
author = {Sonali Nandanwar and Narayan Prasad Patidar and M. Deva Brinda and Mohan Lal Kolhe},
keywords = {Sustainable electrical energy network, Power flows, Voltage security, Binary decision tree, Regression tree},
abstract = {In sustainable operation of electrical energy network, it is necessary to compute in real-time power flows and voltages at nodes for prioritizing power injection from clean energy resources. Intermittent renewable energy sources are likely to create voltage and power balancing issues and to maintain the voltage security of electrical network, real-time information of network power flows and bus voltages are required accurately and instantaneously. This paper presents an approach based on decision trees (DT) for real-time estimation of power flows within the electrical energy network and node voltages. A single tree structure is built for estimation of discrete (or categorical) as well as continuous values of line flows and node voltages of each line and node separately. A simple binary decision tree (BDT) and regression tree (RT) are used for estimation of discrete values and continuous values respectively. The training and testing patterns are generated by performing power flow analysis on an electrical energy network. Once the DT is trained, it estimates the line power flows and bus voltages with desired accuracy. The accuracy of the DT model is tested on a typical IEEE 30-bus system, using test patterns. Result shows that mean absolute error in case of line flow estimation for line number 1 and 10 are found to be 0.0028 p. u. and 0.0017 p. u. Also mean absolute error in case of bus voltage estimation for bus number 3 and 10 are found to be 0.0019 p. u. and 0.0016 p. u. Above results are suggestive of instantaneous estimationwith desired accuracy of line flow and bus voltages, which is the need of the hour for sustainable electrical energy network with integration of cleaner energy resources.Since, DT gives instantaneous result therefore suitable for real-time applications in sustainable electrical energy management system.}
}
@article{REDDY2022100776,
title = {Towards energy efficient Smart city services: A software defined resource management scheme for data centers},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100776},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100776},
url = {https://www.sciencedirect.com/science/article/pii/S221053792200107X},
author = {K. Hemant K. Reddy and Ashish K. Luhach and V. Vinoth Kumar and Sanjoy Pratihar and Deepak Kumar and Diptendu S Roy},
keywords = {Data Center Networking, Energy optimization, Multipath routing, SDN, ILP, Swarm intelligence, Heuristic approach},
abstract = {With the rapid advancements in the services computing paradigm, cloud computing has been a fundamental requirement for enabling practically all sophisticated applications and services, particularly for Smart cities. However curbincs energy dissipation in the data centers (DC) has been a key effort, albeit while fulfilling the quality of service (QoS) requirement of DCs are characterized by complex interconnections among their servers. Maintenance of these servers under dynamic scenarios while ensuring scalability and performance demands Software Defined Networks (SDNs) for easy and efficient resource management. This paper addresses the QoS requirement and energy-efficient operation for Software-Defined DCs for resource management by means of (i) selectively activating a subset of switches, (ii) propounding multi-path routing for all scheduled flows, (iii) aggregating data and routing structure to avoid network congestion and (iv) installation of appropriate forwarding rules across the network switches. These issues are collectively put together in the outline of ILP problem, but due to its computational complexity, a heuristic optimization approach called particle swarm intelligence is articulated. The particle swarm intelligence (PSI) algorithm is employed for feature selection to obtain the minimum cost of the generated energy while satisfying the network traffic demand. The presented simulation result lay the efficacy of the proposed algorithm.}
}
@article{WANG2022102596,
title = {Optimal configuration and pricing strategies for electric-heat cloud energy storage: A Stackelberg game approach},
journal = {Sustainable Energy Technologies and Assessments},
volume = {53},
pages = {102596},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102596},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822006464},
author = {Jianxi Wang and Zhou Xu and Yonghui Sun and Xinye Du and Rabea {Jamil Mahfoud} and Junjie Xiong},
keywords = {Cloud energy storage, Stackelberg game, Optimal configuration, Service pricing, Two-layer optimization},
abstract = {The economic model of cloud energy storage (CES) can help solving the problem of high cost of self-built energy storage. As a contribution to the field of integrated energy systems, the application mechanism of CES for both electric and heat energy systems is studied in this paper, where an optimal configuration and service pricing method of electric-heat CES model based on Stackelberg game approach are proposed. Firstly, a Stackelberg game framework is constructed to consider the interests of both CES provider and users. Secondly, five economic components including energy storage service fee are established, where the optimization models of CES provider and users are built with the objectives of maximizing net revenue and minimizing energy cost, respectively. By decoupling the charging demand of users and the charging plan of the CES provider, the proposed model allows the CES provider to have good flexibility in purchasing energy from the grid and heat network for storage, which in turn can reduce costs and increase revenue. After that, the game problem is constructed as a two-layer optimization model and transformed into a single-layer optimization model based on the Karush-Kuhn-Tucker (KKT) conditions, where the large-M method is used to deal with the nonlinear equation constraints. Finally, experimental case is designed for three types of loads, namely: industrial areas, residential areas and commercial areas, as four comparison scenarios are studied to verify the effectiveness of the proposed methods in enhancing the economy and balancing the interests of multiple subjects.}
}
@article{WANG2023107209,
title = {Machine learning-based identification of symptomatic carotid atherosclerotic plaques with dual-energy computed tomography angiography},
journal = {Journal of Stroke and Cerebrovascular Diseases},
volume = {32},
number = {8},
pages = {107209},
year = {2023},
issn = {1052-3057},
doi = {https://doi.org/10.1016/j.jstrokecerebrovasdis.2023.107209},
url = {https://www.sciencedirect.com/science/article/pii/S105230572300232X},
author = {Ling-Jie Wang and Pei-Qing Zhai and Li-Li Xue and Cai-Yun Shi and Qian Zhang and Hua Zhang},
keywords = {Carotid plaque, Ischaemic stroke, Transient ischaemic attack, Machine learning, Dual-energy computed tomography angiography},
abstract = {Objective
This study aimed to develop and validate a machine learning model incorporating both dual-energy computed tomography (DECT) angiography quantitative parameters and clinically relevant risk factors for the identification of symptomatic carotid plaques to prevent acute cerebrovascular events.
Methods
The data of 180 patients with carotid atherosclerosis plaques were analysed from January 2017 to December 2021; 110 patients (64.03±9.58 years old, 20 women, 90 men) were allocated to the symptomatic group, and 70 patients (64.70±9.89 years old, 50 women, 20 men) were allocated to the asymptomatic group. Overall, five machine learning models using the XGBoost algorithm, based on different CT and clinical features, were developed in the training cohort. The performances of all five models were assessed in the testing cohort using receiver operating characteristic curves, accuracy, recall rate, and F1 score.
Results
The shapley additive explanation (SHAP) value ranking showed fat fraction (FF) as the highest among all CT and clinical features and normalised iodine density (NID) as the 10th. The model based on the top 10 features from the SHAP measurement showed optimal performance (area under the curve [AUC] .885, accuracy .833, recall rate .933, F1 score .861), compared with the other four models based on conventional CT features (AUC .588, accuracy .593, recall rate .767, F1 score .676), DECT features (AUC .685, accuracy .648, recall rate .667, F1 score .678), conventional CT and DECT features (AUC .819, accuracy .740, recall rate .867, F1 score .788), and all CT and clinical features (AUC .878, accuracy .833, recall rate .867, F1 score .852).
Conclusion
FF and NID can serve as useful imaging markers of symptomatic carotid plaques. This tree-based machine learning model incorporating both DECT and clinical features could potentially comprise a non-invasive method for identification of symptomatic carotid plaques to guide clinical treatment strategies.}
}
@article{JARUNNARUMOL2023973,
title = {Neuroradiology Applications of Dual and Multi-energy Computed Tomography},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {973-985},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0033838923001355},
author = {Natthawut Jarunnarumol and Shahmir Kamalian and Michael H. Lev and Rajiv Gupta},
keywords = {Dual-energy CT, Multispectral CT, Neuroimaging, Virtual monochromatic, Material decomposition, Spectral imaging, Stroke, Head and neck imaging}
}
@article{KHEMILI2022130,
title = {Energy aware fuzzy approach for placement and consolidation in cloud data centers},
journal = {Journal of Parallel and Distributed Computing},
volume = {161},
pages = {130-142},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521002227},
author = {Wided Khemili and Jalel Eddine Hajlaoui and Mohamed Nazih Omri},
keywords = {Cloud data centers, Placement, Consolidation, Multiple access edge computing, Network function virtualization},
abstract = {Virtual Network Function (VNF) is one of the pillars of a Cloud network that separates network functions and their dedicated hardware devices, such as routers, firewalls, and load balancers, to host their services on virtual machines. The VNF is responsible for network services that run on virtual machines and can connect each of them alone or organize themselves into a single enclosure to use all the resources available in that enclosure. This flexibility allows physical and virtual resources to be used in a way that ensures control over power consumption, balance in resource use, and minimizing costs and latency. In order to consolidate VNF groups into a minimum number of Virtual Machine (VM) with estimation of the association relation to a measure of confidence under the context of possibility theory, we propose a new Fuzzy-FCA approach for VNF placement based on Formal Concept Analysis (FCA) and fuzzy logic in mixed environment based on cloud data centers and Multiple access Edge Computing (MEC) architecture. Thus, the inclusion of this architecture in the cloud environment ensures the distribution of compute resources to the end user in order to reduce end-to-end latency. To confirm the effectiveness of our solution, we compared it to one of the best algorithms studied in the literature, namely the MultiSwarm algorithm. The results of the series of experiments carried out show the feasibility and efficiency of our algorithm. Indeed, the harvested results confirm the capability of maximizing and balancing the use of resources, of minimizing the latency and the cost of energy consumption. The performance of our solution in terms of average latency represents 16%, a slight increase compared to MultiSwarm, and an average gain, in runtime, of 49%, compared to the same algorithm.}
}
@article{RIPPEL2023110645,
title = {Evaluation of run-off computed tomography angiography on a first-generation photon-counting detector CT scanner – Comparison with low-kVp energy-integrating CT},
journal = {European Journal of Radiology},
volume = {158},
pages = {110645},
year = {2023},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2022.110645},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X22004958},
author = {K. Rippel and J.A. Decker and R. Wudy and T. Trzaska and M. Haerting and T.J. Kroencke and F. Schwarz and C. Scheurig-Muenkler},
keywords = {Photon-counter detector CT, Energy-integrating detector CT, Run-off CTA of the lower extremities, Virtual monoenergetic imaging, Contrast medium reduction},
abstract = {Purpose
To assess the overall imaging performance (radiation dose and image quality) of a photon-counting detector CT (PCD-CT) in comparison with a state-of-the-art energy-integrating detector CT (EID-CT) in run-off CTAs.
Methods
Consecutive patients who underwent run-off CTA on a PCD-CT were included (PCD-CT cohort). A retrospective cohort of patients who had undergone run-off CTA on an EID-CT was matched for gender, body mass index, height, and age (EID-CT cohort). Virtual monoenergetic imaging (VMI) reconstructions for various keV settings (40–120 keV) were generated. CT values and noise were semiautomatically measured for 13 vascular segments of the abdomen, pelvis, and lower extremities. Signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR) were calculated for each segment. Subjective image quality was evaluated by two radiologists along the dimensions ‘vessel attenuation’, ‘vessel sharpness’, and ‘overall image quality’ using 5-point Likert scales.
Results
Forty patients (age 70.9 ± 9.8 years; 14 women) were included in the PCD-CT cohort and matched with a corresponding number of EID-CT patients. Overall, there was an inverse correlation of signal and noise but also of SNR and CNR with keV levels used for VMI reconstructions. SNR and CNR in the 40 – 60 keV range exceeded EID-CT levels significantly. Subjective image quality was substantially higher at lower keV levels and showed no significant difference to EID-CT.
Conclusion
Low keV VMI reconstructions of run-off CTA scans on a PCD-CT result in substantially higher SNR and CNR than 80 kVp and 100 kVp EID-CT acquisitions with equal subjective image quality.}
}
@article{REHMAN2023108333,
title = {Energy-efficient and reconfigurable complementary filter based on analog–digital hybrid computing with SnS2 memtransistor},
journal = {Nano Energy},
volume = {109},
pages = {108333},
year = {2023},
issn = {2211-2855},
doi = {https://doi.org/10.1016/j.nanoen.2023.108333},
url = {https://www.sciencedirect.com/science/article/pii/S2211285523001702},
author = {Shania Rehman and Muhammad Farooq Khan and Hee-Dong Kim and Sungho Kim},
keywords = {Analog-digital hybrid circuit, Drone, Memtransistor, Sensor fusion, Tin disulfide},
abstract = {Sensor fusion is a widely exploited technique that combines data from two or more sensors to improve the accuracy to a level that cannot be achieved using a single sensor alone. Algorithms for sensor fusion are generally executed on conventional digital computing platforms; however, these algorithms impose a burden on small electrical systems with limited battery capacities and computing resources. In this study, we demonstrated an analog–digital hybrid computing platform based on an SnS2 memtransistor for energy-efficient and reconfigurable sensor fusion with a complementary filter algorithm. We experimentally verified that the power consumption of our hybrid computing-based complementary filter is only half that of the traditional software-based complementary filter, even with the same accuracy.}
}
@article{YADAV2022976,
title = {High-resolution outgoing long wave radiation data (2014–2020) of INSAT-3D Imager and its comparison with Clouds and Earth’s Radiant Energy System (CERES) data},
journal = {Advances in Space Research},
volume = {70},
number = {4},
pages = {976-991},
year = {2022},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2022.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S0273117722004343},
author = {Ramashray Yadav and R.K. Giri and S.C. Bhan},
keywords = {OLR, Spatiotemporal, INSAT-3D, CERES, ITCZ, GSICS & DCDR},
abstract = {As a proxy of convection INSAT-3D satellite-derived product Outgoing Long Wave Radiation (OLR) data is available in both high temporal and spatial ranges over 40°N–40°S & 35°E–135°E. Daily gridded data set of 7 years of data (2014–2020) has been generated at 10 km × 10 km resolution and the same is compared with Clouds and Earth’s Radiant Energy System (CERES) instrument data taken from CERES as reference. Almost all the INSAT-3D data set generated is Global Space-based Inter-Calibration System (GSICS) corrected. The spatiotemporal consistency of the data set was statistically analyzed and found to be reasonably good agreement having a bias of ∼±5–6 W/m2 over above said domain. This inter-comparison is essential to get confidence in the data sets and release it further in the public domain for any scientific study. Again, this data set will be very useful in diagnosing the variations of convection at different scales (daily, weekly, monthly, annual, seasonal, intra-seasonal, etc.) & an important repository of Daily Climate Data Records (DCDR) for future studies. The specified domain of the present study is affected throughout the year with variable (weak, moderate, intense, or severe) spatiotemporal Inter-Tropical Convergence Zone (ITCZ) convection streams due to different types of weather activities (winter, pre-monsoon, monsoon, and post-monsoon) throughout the year. To visualize the importance of this high-resolution OLR data set a case study of Cyclone Amphan and Vayu is presented. The extremely severe intense convection (OLR departure −112 W/m2 with INSAT-3D new data set whereas −104 W/m2 in CERES data) was observed in both the data sets on 18th May-2020 at 13.7–16°N & 86.2–86.8°E during the super cyclonic stage of Amphan. A similar type of variation in the OLR has been noticed for Vayu Cyclone (OLR departure −94 W/m2 with INSAT-3D new data set whereas −86 W/m2 in CERES data). This information is very useful in impact-based forecasting and further future actions for disaster managers/decision-makers. The localized convective features during cyclone activity over the Indian Ocean region both in the Arabian Sea and the Bay of Bengal are well captured with this new data set and the difference in OLR of INSAT -3D and CERES -9 W/m2 and -12 W/m2 respectively.}
}
@article{JOHNSON2023112477,
title = {Mapping 3D grain and precipitate structure during in situ mechanical testing of open-cell metal foam using micro-computed tomography and high-energy X-ray diffraction microscopy},
journal = {Materials Characterization},
volume = {195},
pages = {112477},
year = {2023},
issn = {1044-5803},
doi = {https://doi.org/10.1016/j.matchar.2022.112477},
url = {https://www.sciencedirect.com/science/article/pii/S1044580322007598},
author = {Quinton C. Johnson and Peter Kenesei and Steve Petruzza and Jayden Plumb and Hemant Sharma and Jun-Sang Park and Elliott Marsden and Kristoffer Matheson and Michael W. Czabaj and Ashley D. Spear},
keywords = {Microstructure, Synchrotron radiation, X-ray CT, Crystallography, Cellular metal, Lattice structure},
abstract = {Open-cell metal foams are ultra-low-density cellular metals with complex hierarchical structures that span bulk, cell, ligament, and sub-ligament scales and give rise to desirable properties such as high strength-to-weight ratio and excellent energy absorption. Although literature suggests that intrinsic material structures at sub-ligament length scales (e.g., grains and precipitates) play an important role in mechanical behavior of open-cell metal foams, there are very few experimental measurements of such structures in three dimensions and for meaningful volumes of foam. This study seeks to map and track the three-dimensional (3D) grain and precipitate structures of an intact volume of open-cell aluminum foam by advancing microstructural characterization techniques that leverage X-ray micro-computed tomography (μCT) and far-field high-energy X-ray diffraction microscopy (FF-HEDM). A 6%-relative-density aluminum foam sample was mechanically tested in compression while μCTand FF-HEDM measurements were collected at interrupted loading states at beamline 1-ID of the Advanced Photon Source. A new scanning strategy and reconstruction algorithm were established to enable characterization of a foam volume with diameter approximately four times wider than the nominal width of the X-ray beam. The result is a set of maps that detail both the 3D grain and precipitate structures throughout the foam volume at successive strain steps. A novel grain tracking procedure was developed to track individual grains within the foam volume by accounting for the large rigid-body motions that individual ligaments can undergo during mechanical loading. The ability to track grains and precipitate structures in three dimensions throughout large bulk deformation of ultra-low-density polycrystalline materials enables new possibilities for validating numerical models and investigating local failure mechanisms. Furthermore, the methods and procedures developed in this study could be applied to other ultra-low-density structures, such as additively manufactured lattices.}
}
@article{CHEN2022271,
title = {A numerical model for the calculation of the minimum ignition energy of pure and mixture dust clouds},
journal = {Process Safety and Environmental Protection},
volume = {164},
pages = {271-282},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022005341},
author = {Tengfei Chen and Jo {Van Caneghem} and Jan Degrève and Filip Verplaetsen and Jan Berghmans and Maarten Vanierschot},
keywords = {Minimum ignition energy, Dust clouds, Dust mixtures, Theoretical model},
abstract = {A numerical model is developed for the minimum ignition energy (MIE) calculation of pure and mixture dust clouds, considering the influence of particle size distribution. The original particle number based size distribution is approximated with median sizes (number d50) of the subdivisions obtained by dividing the original distribution range. The MIE is calculated for 5 pure dusts, 2 mixtures of combustible dusts and 2 mixtures of combustible & inert dusts. For pure dust clouds, as the number of subdivisions and hence d50s increases, variation trend of the MIE as a function of dust concentration (MIE(conc)) gradually shifts from a “V” to a “U” or “bathtub” shape, along with an increase in the concentration where the dust cloud MIE (minimum MIE(conc) value) is reached. Furthermore, the calculated MIE fluctuates at lower number of d50s but gradually stabilizes as the d50 number further increases. The calculated MIE variation trends correspond well with the experimental data. However, due to uncertainties in the dust cloud formation and spark release in the experimental tests, differences in the definitions of the d50 in different studies and high idealization of the numerical model, there are deviations between the absolute experimental and calculated MIE values.}
}
@article{WANG2022118125,
title = {Stock price prediction for new energy vehicle enterprises: An integrated method based on time series and cloud models},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118125},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118125},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013094},
author = {Meng-xian Wang and Zhi Xiao and Hong-gang Peng and Xiao-kang Wang and Jian-qiang Wang},
keywords = {New energy vehicle, Stock price prediction, Prediction method, Time series, Cloud model},
abstract = {Vigorously developing new energy vehicles (NEVs) is a new strategic goal of the automotive industry nowadays. Stocks are important channels for NEV enterprises to raise funds, and stock price prediction is of great significance to equity financing, risk identification and policy formulation of NEV enterprises. Given this, this paper devotes to developing a novel prediction method by the integration of time series and cloud models to manage the stock price prediction of NEV enterprises. In this way, the concept and generator algorithm of time series clouds (TSCs) are presented, the time series techniques are introduced to model stock price data comprehensively, and the TSC model is established to detect the uncertainty of data changes. Finally, a case study for the stock price prediction of NEV enterprises is carried out to elucidate and testify the developed prediction method. The result analysis and discussion demonstrate that this method outperforms other models and can effectively support the stock price prediction of NEV enterprises.}
}
@article{SAMRIYA2022100746,
title = {Network intrusion detection using ACO-DNN model with DVFS based energy optimization in cloud framework},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100746},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100746},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000786},
author = {Jitendra Kumar Samriya and Rajeev Tiwari and Xiaochun Cheng and Rahul Kumar Singh and Achyut Shankar and Manoj Kumar},
keywords = {Network Intrusion detection system (NIDS), Deep neural network (DNN), Ant colony optimization (ACO), DVFS, Dimension reduction, NSL-KDD dataset},
abstract = {Recent technologies and innovations have encouraged users to adopt cloud-based environment. Network intrusion detection (NID) is an important method for network system administrators to detect various security holes. The performance of traditional NID methods can be affected when unknown or new attacks are detected. For instance, existing intrusion detection systems may have overfitting, low classification accuracy, and high false positive rate (FPR) when faced with significantly large volume and variety of network data. For that reason, this system has been agreed by many establishments to allure the users with its suitable features. Because of its design, it is exposed to malicious attacks. An Intrusion Detection System (IDS) is required to handle these issues which can detect such attacks accurately in a cloud environment. To analyze the IDS datasets some of the predominant choices are Deep learning and Machine learning (ML) algorithms. By adopting nature-inspired algorithms, the problems concerning the data quality and the usage of high-dimensional data can be managed. In this study the datasets KDD Cup 99 and NSL-KDD are used. The dataset is cleaned using the min-max normalization technique and it is processed using the 1-N encoding approach for achieving homogeneity. Dimensionality reduction is done using the Ant colony optimization (ACO) algorithm and further processing is done using the deep neural network (DNN). To minimize the energy consumption we have adopted the Dynamic Voltage and Frequency Scaling (DVFS) mechanism to the system. The main reason to set up this concept is to develop a balance between the energy consumption and the time of different modes of VMs or hosts. The proposed model is validated and compared with ACO and Principal component analysis (PCA)-based (Naïve Bayes) NB models, the experimental outcomes proved the superiority of the ACO-DNN model over the existing methods.}
}
@article{BAI2022205,
title = {Performance analysis of an energy-saving strategy in cloud data centers based on a MMAP[K]/M[K]/N1+N2 non-preemptive priority queue},
journal = {Future Generation Computer Systems},
volume = {136},
pages = {205-220},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002035},
author = {Xiaojun Bai and Shunfu Jin},
keywords = {Cloud data center, Energy-saving strategy, Two-threshold hysteresis mechanism, Marked Markovian arrival process, Non-preemptive priority queue, Pareto optimality},
abstract = {With the development of cloud computing, big data, artificial intelligence and other next generation information technologies, the scale of the data center industry worldwide is growing rapidly, resulting in a sharp increase in energy consumption. In order to efficiently reduce the idle energy consumption in cloud data centers, in this paper, we propose an energy-saving strategy based on two-threshold hysteresis cluster scheduling mechanism, which enables the reserved cluster to be dynamically switched on and off with load variation. In addition, the tasks to be processed are classified into real-time tasks and non-real-time tasks. To keep track of the two classes of tasks and model the correlated traffic in cloud data centers, we describe the arrival flow of tasks as a Marked Markovian Arrival Process (MMAP). Accordingly, we develop a non-preemptive priority queue as the system model to capture the working principle of the proposed strategy. By using the matrix-geometric solution and Gauss–Seidel method, the steady-state distribution of the system model is analyzed, and some key Quality of Service (QoS) metrics and Total Cost of Ownership (TCO) metrics are calculated. Results of numerical experiments show that, under the proposed energy-saving strategy, the overall power consumption in a small cloud data center can be reduced by an average of 30.20% under different scenarios, and the larger the cloud data center scale, the more obvious the energy saving effect. The results also confirm the impact of inter-class correlations of the input process on performance metrics. Furthermore, we identify the Pareto optimal solutions for trading off the overall power consumption and average waiting time of real-time tasks.}
}
@article{SCRIECIU2007678,
title = {The inherent dangers of using computable general equilibrium models as a single integrated modelling framework for sustainability impact assessment. A critical note on Böhringer and Löschel (2006)},
journal = {Ecological Economics},
volume = {60},
number = {4},
pages = {678-684},
year = {2007},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2006.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S092180090600468X},
author = {S. Serban Scrieciu},
keywords = {Computable general equilibrium models, Sustainability impact assessment, Policy appraisal},
abstract = {The search for methods of assessment that best evaluate and integrate the trade-offs and interactions between the economic, environmental and social components of development has been receiving a new impetus due to the requirement that sustainability concerns be incorporated into the policy formulation process. A paper forthcoming in Ecological Economics [Böhringer, C., Löschel, A., in press. Computable general equilibrium models for sustainability impact assessment: status quo and prospects, Ecological Economics.] claims that Computable General Equilibrium (CGE) models may potentially represent the much needed “back-bone” tool to carry out reliable integrated quantitative Sustainability Impact Assessments (SIAs). While acknowledging the usefulness of CGE models for some dimensions of SIA, this commentary questions the legitimacy of employing this particular economic modelling tool as a single integrating modelling framework for a comprehensive evaluation of the multi-dimensional, dynamic and complex interactions between policy and sustainability. It discusses several inherent dangers associated with the advocated prospects for the CGE modelling approach to contribute to comprehensive and reliable sustainability impact assessments. The paper warns that this reductionist viewpoint may seriously infringe upon the basic values underpinning the SIA process, namely a transparent, heterogeneous, balanced, inter-disciplinary, consultative and participatory take to policy evaluation and building of the evidence-base.}
}
@article{WANG1998277,
title = {Integrating hardware, software and mindware for sustainable ecosystem development: Principles and methods of ecological engineering in China1Paper presented at ICEE 96—International Conference on Ecological Engineering, Beijing, China, 7–11 October 1996.1},
journal = {Ecological Engineering},
volume = {11},
number = {1},
pages = {277-289},
year = {1998},
issn = {0925-8574},
doi = {https://doi.org/10.1016/S0925-8574(98)00044-5},
url = {https://www.sciencedirect.com/science/article/pii/S0925857498000445},
author = {Rusong Wang and Jingsong Yan},
keywords = {Hardware, Software, Mindware, Sustainable ecosystem development, Ecological cybernetics, Ecopolis},
abstract = {Grounded in human ecological philosophy, ecological engineering in China seeks to find an alternative way to realize sustainable development at ecosystem level through total metabolism of resources, systematic coupling of technologies and cultivation of people's behavior. Here the key is integration of ‘hardware’, ‘software’ and ‘mindware’. Eight design principles of ecological engineering based on eco-cybernetics are discussed, which fall into three categories: competition, symbiosis and self-reliance. The fundamental tasks of ecological engineering are to develop a sustainable ecosystem through the integrative planning of its structure, function and processes by encouraging totally functioning technology, systematically responsible institutions and ecologically vivid culture. A campaign of ecological engineering development in China is introduced, including 29 national comprehensive experimental communities for sustainable development, 51 pilot studies of eco-county development, and 100 ecological demonstration districts. Some fruitful theoretical and applied results have been gained and the case of Dafeng eco-county development is introduced.}
}
@article{LU2023538,
title = {Energy-efficient task scheduling for mobile edge computing with virtual machine I/O interference},
journal = {Future Generation Computer Systems},
volume = {148},
pages = {538-549},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002431},
author = {Baoshan Lu and Junli Fang and Xuemin Hong and Jianghong Shi},
keywords = {Mobile edge computing, Virtual machine, I/O interference, Task scheduling, Mixed-integer nonlinear programming, Karush–Kuhn–Tucker},
abstract = {Mobile edge computing (MEC) is expected to support the computation-intensive and delay-sensitive applications of mobile internet users. In this paper, we investigate the resource allocation of MEC with the effect of I/O interference among parallel virtual machines (VMs) while satisfying the quality of service (QoS) of tasks. Different from existing works, we propose a flexible task scheduling approach that combining parallel and sequential computing to minimize the computing energy consumption of MEC server. We formulate the task scheduling problem as a mixed-integer nonlinear programming (MINLP) and decompose it as a CPU resource allocation subproblem, a computing time slot subproblem, and a VM selection subproblem. We show the first subproblem is a convex problem and propose a CPU frequency allocation (CFA) algorithm based on the Karush–Kuhn–Tucker (KKT) conditions to obtain the optimal CPU frequency resource allocation. For the time slot allocation and VM selection subproblems, we propose the three step allocation (TSA) and urgency based adjusting (UBA) algorithms to obtain the near-optimal solutions, respectively. Simulation results show that compared with several time slot allocations and VM selections, the proposed TSA and UBA algorithms can save up to 21.7% and 95.8% of energy consumption, respectively.}
}
@incollection{TEWARY2023189,
title = {9 - Static Green’s function for elliptic equations formulated using a partial Fourier representation and applied to computing the thermostatic/electrostatic response of nanocomposite materials☆},
editor = {Vinod K. Tewary and Yong Zhang},
booktitle = {Modeling, Characterization, and Production of Nanomaterials (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {189-223},
year = {2023},
series = {Woodhead Publishing Series in Electronic and Optical Materials},
isbn = {978-0-12-819905-3},
doi = {https://doi.org/10.1016/B978-0-12-819905-3.00009-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128199053000099},
author = {Vinod K. Tewary and E.J. Garboczi},
keywords = {Conductivity, Elliptic equations, Graphene and phosphorene, Green’s functions, Laplace/Poisson equation, Nanocomposites, Partial Fourier transform},
abstract = {A computationally efficient representation of static Green’s functions (GFs) for elliptic partial differential equations is described. The equations of interest are the Laplace/Poisson equations for thermostatics and electrostatics. The representation is based upon the use of partial Fourier transforms (PFTs) in one or two Cartesian coordinates, depending upon the boundary conditions. The representation is particularly convenient for solving boundary value problems in which the Dirichlet condition is specified in one Cartesian direction and periodic boundary conditions in the others. The PFT leads to an exact analytical expression for the GF in the Dirichlet direction, the direction in which the Dirichlet condition is specified. The availability of an analytical result in one direction makes the method computationally very efficient. The periodic boundary conditions are satisfied by using the Born’s supercell approach for crystal lattices. In this chapter, the general PFT technique for 3D systems is described, followed by its specialization to modern 2D composite materials. The technique is applicable to inclusions of arbitrary shapes and concentration but, in this chapter, for illustration, we consider a cubic inclusion in a cuboid host in 3D as well as a square-shaped inclusion in a rectangular host in 2D. Numerical results are presented for the effective conductivity of 2D phosphorene and graphene containing metallic inclusions, which is a nanocomposite material system of contemporary industrial interest.}
}
@article{YANG2022,
title = {Cloud-Model-Based Feature Engineering to Analyze the Energy–Water Nexus of a Full-Scale Wastewater Treatment Plant},
journal = {Engineering},
year = {2022},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922001990},
author = {Shan-Shan Yang and Xin-Lei Yu and Chen-Hao Cui and Jie Ding and Lei He and Wei Dai and Han-Jun Sun and Shun-Wen Bai and Yu Tao and Ji-Wei Pang and Nan-Qi Ren},
keywords = {Wastewater treatment plants, Cloud-model theory, Data mining, Principal component analysis, -means clustering, Cloud-model-based energy-consumption analysis},
abstract = {Wastewater treatment plants (WWTPs) are important and energy-intensive municipal infrastructures. High energy consumption and relatively low operating performance are major challenges from the perspective of carbon neutrality. However, water–energy nexus analysis and models for WWTPs have rarely been reported to date. In this study, a cloud-model-based energy-consumption analysis (CMECA) of a WWTP was conducted to explore the relationship between influent and energy consumption by clustering its influent’s parameters. The principal component analysis (PCA) and K-means clustering were applied to classify the influent condition using water quality and volume data. The energy consumption of the WWTP is divided into five standard evaluation levels, and its cloud digital characteristics (CDCs) were extracted according to bilateral constraints and golden ratio methods. Our results showed that the energy consumption distribution gradually dispersed and deviated from the Gaussian distribution with decreased water concentration and quantity. The days with high energy efficiency were extracted via the clustering method from the influent category of excessive energy consumption, represented by a compact-type energy consumption distribution curve to identify the influent conditions that affect the steady distribution of energy consumption. The local WWTP has high energy consumption with 0.3613 kW·h·m−3 despite low influent concentration and volumes, across four consumption levels from low (I) to relatively high (IV), showing an unsatisfactory operation and management level. The average oxygenation capacity, internal reflux ratio, and external reflux ratio during high energy efficiency days recognized by further clustering were obtained (0.2924–0.3703 kg O2·m−3, 1.9576–2.4787 and 0.6603–0.8361, respectively), which could be used as a guide for the days with low energy efficiency. Consequently, this study offers a water–energy nexus analysis method to identify influent conditions with operational management anomalies and can be used as an empirical reference for the optimized operation of WWTPs.}
}
@article{SHEKHARYADAV2023102986,
title = {Energy efficient and optimized genetic algorithm for software effort estimator using double hidden layer bi-directional associative memory},
journal = {Sustainable Energy Technologies and Assessments},
volume = {56},
pages = {102986},
year = {2023},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102986},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822010347},
author = {Chandra {Shekhar Yadav} and Raghuraj Singh and Sambit Satpathy and S. {Baghavathi Priya} and B.T. Geetha and Vishal Goyal},
keywords = {Energy efficient, Optimization, Associative memory, Hidden layer},
abstract = {In software development, it's important to have an accurate assessment of effort, cost, energy, and time in order to plan and allocate resources in the best way possible. This makes it more likely that the software will work and lowers the risk that it won't. Bi-directional associative memory is used in the suggested method to figure out how long it will take to finish a project. This effort estimator was built in MATLAB and tested, verified, and trained on a large project dataset. In this work, a genetic algorithm called Double Hidden Layers Bi-directional Associative Memory is used to make a unique model for a software estimator (DHBAM). After doing several simulations, we found that the DHBAM architecture works better than the Single Hidden Layer Feed-Forward Neural Net (SHFNN) model for getting the best results. It has also been proven with the root mean square error (RMSE) method. In a previous study, the RMSE for the network design SHFNN 16-19-1 with a learning rate of 1.01 and a momentum of 0.70 after 1,000,000 iterations was 1.49074 × 10^-3. With a learning rate parameter of 1.05 and a momentum parameter of 0.6, the RMSE for the network design DHBAM 16-8-6-1 is now 1.241703 × 10^-3, which drops to 1.2238 × 10^-3 after 100 generations in 10,000 populations using the optimized based genetic algorithm (GA). Based on the results, it's clear that the proposed effort estimator does a better job than the ones that are already in use. Experiments show that the newly proposed optimized based genetic algorithm will help researchers, scientists, and businesses predict important traits more accurately and efficiently early on in the planning process.}
}
@article{WEI2023131805,
title = {The role of pores in micro-zone distribution of Cd in a tropical paddy soil: Results from X-ray computed tomography combined energy dispersive spectroscopy analysis},
journal = {Journal of Hazardous Materials},
volume = {457},
pages = {131805},
year = {2023},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2023.131805},
url = {https://www.sciencedirect.com/science/article/pii/S0304389423010889},
author = {Chaoxian Wei and Bigui Lin and Beibei Liu and Zhenli He and Qinfen Li},
keywords = {Cadmium, Undisturbed tropical soil, Micro-zone distribution, Air space pores, Water-holding pores},
abstract = {Accurate description of Cd micro-zone distribution and accumulation is the prerequisite for revealing Cd transfer and transformation processes. However, to date, the role of soil pores in the Cd micro-zone distribution characteristics in undisturbed soil is still unclear. In this study, the obvious heterogeneous distribution of Cd in and around the soil pores at the cross-sectional surface of the tropical undisturbed topsoil was visualized by the combination of X-ray micro-computed tomography and scanning electron microscope-energy dispersive spectroscopy. For both the air space and water-holding pores, the micro-zone distribution characteristics of Cd around the pores were dominated by pore sizes. For macropores and mesopores, Cd preferred to distribute in the micro-zone within 167.5–335 µm from pores. But for micropores, the highest content percentage of Cd was exhibited in the micro-zone within 67–167.5 µm from pores. The random forest model revealed that the occurrence of Fe (13.83%) and P (13.59%) contributed most to Cd micro-zone distribution around air space pores. While for water-holding pores, Fe occurrence (18.30%) contributed more significantly than P (11.92%) to Cd micro-zone distribution. Our study provided new insights into Cd retention mechanism, which is help for accessing Cd migration and transformation.}
}
@article{KIM20226465,
title = {Numerical analysis of shock energy dissipation in microbubble clouds},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {8},
pages = {6465-6475},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2021.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1110016821008097},
author = {Chanwoo Kim and Hyun {Sun Park} and Hang {Jin Jo} and Moohwan Kim},
keywords = {Microbubble, Shockwave, Shock–bubble interaction, Shock mitigation, Bubble dynamics},
abstract = {In this study, energy dissipation via shock–bubble interaction (SBI) is investigated. With the SBI model developed herein, shock propagation in a bubbly mixture where shockwaves move in a steady state with constant shock velocity and shock strength are obtained as forms of time histories of gas and liquid pressure, bubble radius, and gas temperature. Shock energy is dissipated through thermal heat transfer between the gas and the liquid during bubble oscillation after the shockwave passes in a certain location. The dissipation energy (DE) and damping time (DT) were obtained in the form of dimensionless functions with SBI initial values of the gas volume fraction, bubble radius, and shock strength. Finally, the dissipation power is estimated by combining the concepts of DE and DT that evaluates the ability of energy mitigation in a given SBI system.}
}
@article{MACK2003157,
title = {The use of head computed tomography in elderly patients sustaining minor head trauma},
journal = {The Journal of Emergency Medicine},
volume = {24},
number = {2},
pages = {157-162},
year = {2003},
issn = {0736-4679},
doi = {https://doi.org/10.1016/S0736-4679(02)00714-X},
url = {https://www.sciencedirect.com/science/article/pii/S073646790200714X},
author = {Lisa R Mack and Shu B Chan and Julio C Silva and Teresita M Hogan},
keywords = {head injury, minor, elderly, computed tomography (CT) scans},
abstract = {The study objectives were to ascertain historical and clinical criteria differentiating intracranial injury (ICI) in elderly patients with minor head trauma (MHT), and determine applicability of current head computed tomography (CT) scan indications in this population. A 12-month retrospective chart review was performed at a community teaching hospital with 34,000 annual Emergency Department (ED) visits. Included were patients ≥ 65 years old sustaining MHT with a Glasgow Coma Scale (GCS) score of 13–15 who had a CT scan performed during their hospital stay. Data included: injury mechanism, symptoms, signs, GCS, anticoagulation use or studies, presence of alcohol or drug, CT scan result, diagnosis, and outcome and intervention(s). There were 133 patients, with 19 (14.3%) suffering ICI. Four ICI patients required neurosurgical intervention. The mean age was 80.4 years and 66% were female. Four of 19 ICI patients (21%) had a GCS of 15, no neurologic symptoms, alcohol use or anticoagulation. Only 1 of 13 signs and symptoms correlated with ICI. In this study, no useful clinical predictors of intracranial injury in elderly patients with MHT were found. Current protocols based on clinical findings may miss 30% of elderly ICI patients. Head CT scan is recommended on all elderly patients with MHT.}
}
@article{RANI2021107943,
title = {Pareto based ant lion optimizer for energy efficient scheduling in cloud environment},
journal = {Applied Soft Computing},
volume = {113},
pages = {107943},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107943},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621008656},
author = {Rama Rani and Ritu Garg},
keywords = {Energy efficiency, Workflow scheduling, Discrete ant lion optimization, Multi objective optimization, Cloud computing},
abstract = {The tremendous growth of cloud-based data centres leads to significant amount of energy. Thus, the prime concern for the cloud service providers is to generate environment friendly solution by minimizing the energy consumption. Further, due to increased demand of scientific workflow applications, cloud providers face a challenging issue of efficient workflow scheduling by minimizing the makespan. To address the above-mentioned contradictory issues, we proposed a Pareto based multi-objective discrete ant lion optimization algorithm (PBMO-DALO) to solve workflow-scheduling problem in cloud data centres with minimizing the conflicting objectives of makespan and energy consumption simultaneously. Distinguished from the original ant lion optimization algorithm, the proposed algorithm involves new encoding scheme for ants and antlions, their random walk and selection of fitter antlion for trap building to address the discrete nature of the workflow scheduling problem. Subsequently, the PBMO-DALO uses the Pareto dominance and crowding distance approach to tackle optimization of multiple objectives and to achieve the optimal solutions. The simulation results indicate that the proposed PBMO-DALO algorithm overwhelms other competing algorithms and generates good trade-off solutions with better convergence and uniform diversity.}
}
@article{MOON2023171211,
title = {Tunable voltage polarity-dependent resistive switching characteristics by interface energy barrier modulation in ceria-based bilayer memristors for neuromorphic computing},
journal = {Journal of Alloys and Compounds},
volume = {963},
pages = {171211},
year = {2023},
issn = {0925-8388},
doi = {https://doi.org/10.1016/j.jallcom.2023.171211},
url = {https://www.sciencedirect.com/science/article/pii/S0925838823025148},
author = {Sola Moon and Kitae Park and Peter Hayoung Chung and Dwipak Prasad Sahu and Tae-Sik Yoon},
keywords = {Artificial synapse, Neuromorphic computing, Ceria, Gd-doped ceria, Tunable voltage polarity dependence},
abstract = {Synaptic characteristics with tunable dependence on the voltage polarity are demonstrated in ceria (CeO2) and Gd-doped ceria (GDC) bilayer memristors with respect to their stacking orders. Both Pt/GDC/CeO2/Pt and Pt/CeO2/GDC/Pt memristors with different oxide stacking orders exhibit analog, linear, and symmetric synaptic weights for potentiation and depression, paired-pulse facilitation, and short- and long-term plasticity (STP and LTP, respectively). Potentiation and depression behaviors are highly linear and symmetric thanks to the stacking of more oxygen-deficient GDC layer with CeO2, which facilitates the redistribution of oxygen vacancies for analog resistance change. These memristors have opposite dependence of synaptic weight updates on the polarity of potentiation and depression voltages for the stacking order of CeO2 and GDC, which are consistently interpreted by voltage-driven energy barrier modulation at the interface between CeO2 and GDC or with Pt electrodes via oxygen vacancy redistribution. Recognition simulation with modified handwritten digits patterns using a two-layer perceptron neural network exhibits accuracy of approximately 88% with achieved dynamic range, linearity, symmetry, and precision states. These synaptic characteristics are also demonstrated in a 32 × 32 crossbar array of GDC-top memristors. This verifies the potential of bilayer memristors for application in artificial synapse networks in neuromorphic computing systems.}
}
@article{MATERWALA2022238,
title = {Performance and energy-aware bi-objective tasks scheduling for cloud data centers},
journal = {Procedia Computer Science},
volume = {197},
pages = {238-246},
year = {2022},
note = {Sixth Information Systems International Conference (ISICO 2021)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.137},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921023619},
author = {Huned Materwala and Leila Ismail},
keywords = {Autonomous agents, cloud computing, energy-efficiency, evolutionary algorithm, genetic algorithm, quality of service, multi-objective optimization, performance},
abstract = {Cloud computing enables remote execution of users’ tasks. The pervasive adoption of cloud computing in smart cities’ services and applications requires timely execution of tasks adhering to Quality of Services (QoS). However, the increasing use of computing servers exacerbates the issues of high energy consumption, operating costs, and environmental pollution. Maximizing the performance and minimizing the energy in the cloud data center is challenging. In this paper, we propose a performance and energy optimization bi-objective algorithm to trade off the contradicting performance and energy objectives. An evolutionary algorithm-based multi-objective optimization is for the first time proposed using system performance counters. The performance of the proposed model is evaluated using a realistic cloud dataset in a cloud computing environment. Our experimental results achieve higher performance and lower energy consumption compared to a state-of-the-art algorithm.}
}
@article{FARHADIAN2020116035,
title = {Sulfonated chitosan as green and high cloud point kinetic methane hydrate and corrosion inhibitor: Experimental and theoretical studies},
journal = {Carbohydrate Polymers},
volume = {236},
pages = {116035},
year = {2020},
issn = {0144-8617},
doi = {https://doi.org/10.1016/j.carbpol.2020.116035},
url = {https://www.sciencedirect.com/science/article/pii/S0144861720302095},
author = {Abdolreza Farhadian and Mikhail A. Varfolomeev and Alireza Shaabani and Saeed Nasiri and Iskander Vakhitov and Yulia F. Zaripova and Vladimir V. Yarkovoi and Aleksander V. Sukhov},
keywords = {Hydrate inhibitor, Corrosion inhibitor, Flow assurance, High cloud point, High-pressure micro-differential scanning calorimetery, Environmentally friendly},
abstract = {In this work sulfonated chitosan (SCS) was introduced as a promising green kinetic methane hydrate and corrosion inhibitor to overcome the incompatibility problem between inhibitors. Evaluation of hydrate inhibition performance of SCS with high-pressure autoclave and micro-differential scanning calorimeter revealed that hydrate formation was delayed 14.3 ± 0.2 times and amount of hydrate formed was decreased to 30 % compared to water. The weight loss experiments showed that SCS provides corrosion inhibition efficiency of 95.6 ± 0.1 at 5000 ppm concentration. SCS is able to increase polarization resistance and decrease corrosion current density according to electrochemical measurements. Study of surface morphology by SEM-EDX and profilometer showed that SCSs suppress corrosion rate and reduce the surface roughness of carbon steel. Quantum chemical study confirmed that the pendant groups caused by chitosan modification interact with carbon steel surface. The findings of this research can provide new opportunities to develop biodegradable materials as KHIs/CIs for flow assurance in oil and gas pipelines.}
}
@article{NEHRA20231031,
title = {Dual-Energy, Spectral and Photon Counting Computed Tomography for Evaluation of the Gastrointestinal Tract},
journal = {Radiologic Clinics of North America},
volume = {61},
number = {6},
pages = {1031-1049},
year = {2023},
note = {Dual Energy CT and Beyond},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2023.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0033838923001379},
author = {Avinash K. Nehra and Bari Dane and Benjamin M. Yeh and Joel G. Fletcher and Shuai Leng and Achille Mileto},
keywords = {Dual-energy CT, Photon-counting detector CT, Virtual monoenergetic images, Iodine maps, Hepatocellular carcinoma, Pancreatic adenocarcinoma, Gastrointestinal bleeding, Peritoneal disease}
}
@article{LIU2021101930,
title = {Research on green renovations of existing public buildings based on a cloud model –TOPSIS method},
journal = {Journal of Building Engineering},
volume = {34},
pages = {101930},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.101930},
url = {https://www.sciencedirect.com/science/article/pii/S2352710220335622},
author = {Yang Liu and Hongyu Chen and Xian-jia Wang},
keywords = {Public buildings, Green renovation, Cloud model, TOPSIS},
abstract = {Energy saving and consumption reduction in the field of public buildings (PBs) are keys to achieving the target global temperature growth of the Paris Agreement. However, evaluations of the green renovation (GR) of existing PBs are challenged by the need to construct a unified dimensional model for renovations that are convenient, durable, resource saving and environmentally friendly. This paper proposes a TOPSIS decision method based on a cloud model for the evaluation of GR schemes of existing PBs that can overcome the ambiguity and complexity of scheme selection. The method and preparation processes are described, and the qualitative concepts and boundaries are defined. Then, evaluation models of GR for existing PBs that comprise energy savings and energy utilization, water savings and water resource utilization, land savings and both the outdoor environment and indoor environment are established. An integrated assessment is performed to identify the best plan for GR, and the corresponding evaluation indexes are determined. Finally, a case study in the main library of Huazhong University of Science & Technology (HUST) is examined. The most suitable scheme for the case study is to expand the library by connecting the vacant land space between Yifu Library and the old library. This case study illustrates the validity and practicability of the proposed method, laying a foundation for the selection of GR schemes.}
}