@article{WU2020116381,
title = {Renewable energy investment risk assessment for nations along China’s Belt & Road Initiative: An ANP-cloud model method},
journal = {Energy},
volume = {190},
pages = {116381},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2019.116381},
url = {https://www.sciencedirect.com/science/article/pii/S0360544219320766},
author = {Yunna Wu and Jing Wang and Shaoyu Ji and Zixin Song},
keywords = {Belt & Road initiative, Renewable energy investment, Risk assessment, Cloud model},
abstract = {With the active promotion of the “Belt & Road initiative”, renewable energy investment shows the market prospect of explosive growth in various countries. However, the overseas renewable energy investment risks faced by Chinese companies are synthesized, variegated and long-term, requiring comprehensive assessments and joint responses. To evaluate the risks in renewable energy investment for nations along “Belt & Road initiative”, this paper establishes an ANP-cloud framework in consideration of the randomness of information. Firstly, 32 risk factors covering technical, political, economic, resource, social/environmental risks and Chinese factors are identified and 54 nations are studied and classified into six groups. Then, the analytic network process (ANP) method is employed to determine four sets of weights based on grouping, so that mutual influence among factors can be taken into account. Furthermore, the overall risks are calculated through the cloud model according to their membership degree. The results indicate that political risks, economic risks and resource risks occupy the main determinant position during the overseas renewable energy investment while Chinese factors have certain effects on investment behaviors. Finally, recommendations are given for investors and decision-makers to help choose a more appropriate nation to invest in, strengthen risk prevention and control as well.}
}
@article{SAXENA2021248,
title = {A proactive autoscaling and energy-efficient VM allocation framework using online multi-resource neural network for cloud data center},
journal = {Neurocomputing},
volume = {426},
pages = {248-264},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.08.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220315873},
author = {Deepika Saxena and Ashutosh Kumar Singh},
keywords = {Differential evolution, Multi-resource prediction, Pareto-optimal, Power saving, Resource provisioning, Resource utilization},
abstract = {This work proposes an energy-efficient resource provisioning and allocation framework to meet dynamic demands of the future applications. The frequent variations in a cloud user’s resource demand leads to the problem of an excess power consumption, resource wastage, performance and Quality-of-Service (QoS) degradation. The proposed framework addresses these challenges by matching the application’s predicted resource requirement with resource capacity of VMs precisely and thereby consolidating entire load on the minimum number of energy-efficient physical machines (PMs). The three consecutive contributions of the proposed work are: (1) Online Multi-Resource Feed-forward Neural Network (OM-FNN) to forecast the multiple resource demands concurrently for the future applications, (2) autoscaling of VMs based on the clustering of the predicted resource requirements, (3) allocation of the scaled VMs on the energy-efficient PMs. The integrated approach successively optimizes resource utilization, saves energy and automatically adapts to the changes in future application resource demand. The proposed framework is evaluated by using real workload traces of the benchmark Google Cluster Dataset and compared against different scenarios including energy-efficient VM placement (VMP) with resource prediction only, VMP without resource prediction and autoscaling, and optimal VMP with autoscaling based on actual resource utilization. The observed results demonstrate that the proposed integrated approach achieves near-optimal performance against optimal VMP and outperforms rest of the VMPs in terms of power saving and resource utilization up to 88.5% and 21.12% respectively. In addition, OM-FNN predictor shows better accuracy, lesser time and space complexity over a traditional single-input and single-output feed-forward neural network (SISO-FNN) predictor.}
}
@article{2023S118,
title = {407 Monosodium Urate Crystal Depletion in Renal Transplant Recipients Treated With Pegloticase: PROTECT Serial Dual-Energy Computed Tomography Findings},
journal = {American Journal of Kidney Diseases},
volume = {81},
number = {4, Supplement 1},
pages = {S118-S119},
year = {2023},
note = {National Kidney Foundation 2023 Spring Clinical Meeting Abstracts},
issn = {0272-6386},
doi = {https://doi.org/10.1053/j.ajkd.2023.01.409},
url = {https://www.sciencedirect.com/science/article/pii/S0272638623004754}
}
@article{SONG2020429,
title = {Experimental investigations of the ignition delay time, initial ignition energy and lower explosion limit of zirconium powder clouds in a 20L cylindrical vessel},
journal = {Process Safety and Environmental Protection},
volume = {134},
pages = {429-439},
year = {2020},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2019.10.032},
url = {https://www.sciencedirect.com/science/article/pii/S0957582019315502},
author = {Xianzhao Song and Hao Su and Lifeng Xie and Bin Li and Yong Cao and Yongxu Wang},
keywords = {Zirconium powder, Dust-dispersion time, Maximum explosion pressure, Maximum rate of pressure rise, Explosion index, Concentration},
abstract = {Zirconium powder, a main potential dangerous source in nuclear industry, has been focused on its explosion characteristics in this paper. Experiments were performed in a 20 L cylindrical explosion vessel with the influences of initial ignition energy, ignition delay time and zirconium concentration. Results showed that the maximum explosion pressure of zirconium-air mixture rose with the increase of the initial ignition energy, and a peak value was obtained with the variation of the ignition delay time. Lower explosion limit with the initial ignition energy of 10 kJ is 10 g/m³. Furthermore, explosion severity parameters, including maximum explosion pressure, maximum rate of pressure rise and maximum explosion index of zirconium-air mixture, were also obtained with the coupling variation between initial ignition energy and dust concentration. Research results and data obtained are a supplement to the utilization and safety operation of zirconium powders.}
}
@article{KETANKUMAR2015108,
title = {A Green Mechanism Design Approach to Automate Resource Procurement in Cloud},
journal = {Procedia Computer Science},
volume = {54},
pages = {108-117},
year = {2015},
note = {Eleventh International Conference on Communication Networks, ICCN 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Data Mining and Warehousing, ICDMW 2015, August 21-23, 2015, Bangalore, India Eleventh International Conference on Image and Signal Processing, ICISP 2015, August 21-23, 2015, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S187705091501337X},
author = {Doshi Chintan Ketankumar and Gaurav Verma and K. Chandrasekaran},
keywords = {Cloud computing, Green cloud broker, Resource procurement, Mechanism design.},
abstract = {Cloud computing paradigm is emerging as the solution to all the infrastructure setup problems of IT industry. But the thriving demand of cloud infrastructure has increased the energy consumption of the data centers drastically. As the energy consumption of the data center rises, it leads us to high carbon emissions which are dangerous for the environment. In this paper, we propose a green cloud broker for resource procurement problem by considering the metrics of energy efficiency and environmental friendly operations of the cloud service provider. We use mechanism design methods to decide the allocation and payment for the submitted job dynamically. We perform experiments and show the results of comparisons of energy consumption and emission of greenhouse gases between the allocation decided by the proposed green cloud broker and a without taking the green metric into consideration.}
}
@article{GUO2022108678,
title = {Energy harvesting computation offloading game towards minimizing delay for mobile edge computing},
journal = {Computer Networks},
volume = {204},
pages = {108678},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108678},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621005491},
author = {Mian Guo and Qirui Li and Zhiping Peng and Xiushan Liu and Delong Cui},
keywords = {Mobile edge computing, Computation offloading, Energy harvesting},
abstract = {Mobile edge computing (MEC) has emerged for meeting the ever-increasing computation demands from mobile applications. Mobile users endowing with computation and energy harvesting (EH) capabilities, called EH devices, are desired in MEC systems. With computation capability, EH devices can switch between local computing and mobile-edge computing for better QoEs. With EH technologies, mobile users could survive perpetually for supporting long-term task processing. However, the heterogeneous computation resource among EH devices and edge servers, limited wireless resource as well as time-constrained harvestable energy challenge the computation offloading. This paper investigates a green MEC with EH devices and develops an effective EH computation offloading scheme towards minimizing delay for green MEC. We formulate and analyze the EH computation offloading problem with game theory. Then, an EH computation offloading game scheme, including the Lyapunov drift-based energy harvesting and computation offloading best response algorithms, is designed to find out optimal energy harvesting and computation offloading solutions to address the problem. The simulation results have been conducted to demonstrate the efficiency of the proposal.}
}
@article{POTU2022107603,
title = {Quality-aware energy efficient scheduling model for fog computing comprised IoT network},
journal = {Computers & Electrical Engineering},
volume = {97},
pages = {107603},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107603},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005371},
author = {NARAYANA POTU and SREEDHAR BHUKYA and CHANDRASHEKAR JATOTH and PREMCHAND PARVATANENI},
keywords = {IoT networks, Fog computing, Moving averages, Edge node computing, Resource scheduling, Heiken-Ashi patterns},
abstract = {The present phase of the IoT (Internet of Things) networks has no limits to perform loaded data transmissions and computational requirements, which is made possible by Fog Computing technology. However, excessive energy resources and considerable loss of quality in loaded data transmission remains a significant challenge to the Fog Computing comprised IoT networks. This manuscript endeavors to portray a novel scheduling algorithm that optimizes edge node and fog node scheduling processes. The proposed approach is a supervised learning approach, titled "Quality-aware Energy Efficient Scheduling Model for Fog computing comprised IoT networks. The statistical measures moving averages and Heiken-Ashi patterns (Heiken-Ashi has been used in conjunction with candlestick charts to predict the present scheduling opportunities easily) have been used as a base for the proposed scheduling model. The experimental study signifies the performance of the proposed model and is scaled against the contemporary models.}
}
@article{RM202016,
title = {Load balancing of energy cloud using wind driven and firefly algorithms in internet of everything},
journal = {Journal of Parallel and Distributed Computing},
volume = {142},
pages = {16-26},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520300356},
author = {Swarna Priya R.M. and Sweta Bhattacharya and Praveen Kumar Reddy Maddikunta and Siva Rama Krishnan Somayaji and Kuruva Lakshmanna and Rajesh Kaluri and Aseel Hussien and Thippa Reddy Gadekallu},
keywords = {Green communication, Energy cloud, Internet of everything (IoE), Wind driven algorithm, Firefly algorithm},
abstract = {The smart applications dominating the planet in the present day and age, have innovatively progressed to deploy Internet of Things (IoT) based systems and related infrastructures in all spectrums of life. Since, variety of applications are being developed using this IoT paradigm, there is an immense necessity for storing data, processing them to get meaningful information and render suitable services to the end-users. The “thing” in this decade is not only a smart sensor or a device; it can be any physical or household object, a smart device or a mobile. With the ever increasing rise in population and smart device usage in every sphere of life, when all of such “thing”s generates data, there is a chance of huge data traffic in the internet. This could be handled only by integrating “Internet of Everything (IoE)” paradigm with a completely diversified technology — Cloud Computing. In order to handle this heavy flow of data traffic and process the same to generate meaningful information, various services in the global environment are utilized. Hence the primary focus revolves in integrating these two diversified paradigm shifts to develop intelligent information processing systems. Energy Efficient Cloud Based Internet of Everything (EECloudIoE) architecture is proposed in this study, which acts as an initial step in integrating these two wide areas thereby providing valuable services to the end users. The utilization of energy is optimized by clustering the various IoT network using Wind Driven Optimization Algorithm. Next, an optimized Cluster Head (CH) is chosen for each cluster, using Firefly Algorithm resulting in reduced data traffic in comparison to other non-clustering schemes. The proposed clustering of IoE is further compared with the widely used state of the art techniques like Artificial Bee Colony (ABC) algorithm, Genetic Algorithm (GA) and Adaptive Gravitational Search algorithm (AGSA). The results justify the superiority of the proposed methodology outperforming the existing approaches with an increased life-time and reduction in traffic.}
}
@article{LIANG2020329,
title = {Memory-aware resource management algorithm for low-energy cloud data centers},
journal = {Future Generation Computer Systems},
volume = {113},
pages = {329-342},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20305835},
author = {Bin Liang and Xiaoshe Dong and Yufei Wang and Xingjun Zhang},
keywords = {Cloud data center, Memory-aware, Cloud task mapping, Virtual machine deployment},
abstract = {The continuous advancement of cloud computing technology has driven the vigorous development of cloud data centers. This manifests itself not only in increasing numbers, but also in rapid expansion scale. At the same time, the problems of high energy consumption, high cost and high carbon emissions began to stand out. These factors have become a bottleneck restricting the further development of cloud computing technology. As a result of the application of virtualization technology, mature cloud service providers use virtual machines instead of physical machines to provide users with computing, storage and other services. Therefore, the scheduling algorithm of cloud tasks and virtual machines has been widely studied by academia as a core problem. However, most of the current cloud data center resource management algorithms take CPU utilization as the main consideration, and increase the CPU utilization through virtual machine integration to reduce the energy consumption of cloud data centers. However, these algorithms will cause waste and low utilization of other resources in the cloud data center due to excessive consideration of CPU utilization. This paper systematically analyzes the mapping relationship between cloud tasks, virtual machines and physical machines. At the same time, the performance of cloud tasks, virtual machines, and physical machines is modeled in the cloud data center. By comprehensively considering the CPU and memory characteristics of cloud tasks, the memory priority cloud task mapping rule is established. Based on the rule, the memory-aware resource management algorithm for low-energy cloud data centers (MALE) is proposed. The algorithm maps cloud tasks and deploys virtual machines according to the memory requirements of the cloud tasks, thereby achieving the goal of simultaneously reducing the total fee of cloud users and the energy consumption of cloud data centers. Finally, the algorithm is compared with the other three algorithms. The experimental simulation results show that the effect of the proposed algorithm in this paper is significantly better than the comparison algorithm for the total fee of cloud users and the energy consumption of the cloud data center.}
}
@article{PENG2022102329,
title = {HEA-PAS: A hybrid energy allocation strategy for parallel applications scheduling on heterogeneous computing systems},
journal = {Journal of Systems Architecture},
volume = {122},
pages = {102329},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102329},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121002265},
author = {Jiwu Peng and Kenli Li and Jianguo Chen and Keqin Li},
keywords = {Dynamical pre-allocate energy, Energy allocation, Heterogeneous computing systems, Precedence-constrained tasks, Static pre-allocate energy},
abstract = {Heterogeneous Computing Systems (HCS) have received widespread attention due to their powerful computing power, low cost, and high scalability. In HCS ranging from small-embedded devices to large data centers, energy consumption is one of the crucial design constraints. Meanwhile, the schedule length (response time) of parallel applications directly affects their Quality of Service (QoS) experience. In this study, we address the problem of minimizing the schedule length of energy-constrained parallel applications (MSLEC) on heterogeneous computing systems. Firstly, we define the concept of task energy demand rate and energy allocation factor to reasonably allocate the allocatable energy. Secondly, We propose a two-stage hybrid energy allocation (HEA) strategy and divide the allocatable energy into two parts according to the energy allocation factor, namely static pre-allocate energy (SAE) and dynamic pre-allocate energy (DAE). In the first stage, we pre-allocate SAE for each task based on the minimum energy demand and energy demand rate before task scheduling. In the second stage, we dynamically allocate DAE to each task during the operation of the scheduling algorithm. Thirdly, We conduct a rigorous mathematical proof of the feasibility of the proposed strategy. Finally, according to the proposed strategy, we design a novel HEA-based parallel application scheduling (HEA-PAS) algorithm, which aims to solve the MSLEC problem. Experiments on real-world and randomly generated parallel applications show that the proposed HEA-PAS algorithm outperforms the state-of-the-art methods in terms of effectiveness.}
}
@article{SI2022124131,
title = {Configuration optimization and energy management of hybrid energy system for marine using quantum computing},
journal = {Energy},
volume = {253},
pages = {124131},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.124131},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222010349},
author = {Yupeng Si and Rongjie Wang and Shiqi Zhang and Wenting Zhou and Anhui Lin and Guangmiao Zeng},
keywords = {Marine hybrid energy system, Quantum computing, Configuration optimization, Energy management},
abstract = {Improving the technology of hybrid energy systems is an important development direction for the greening of ships, the configuration optimization and energy management of ship hybrid energy system is vital to enhance the marine electric power system reliability, economic efficiency, and sustainability. Focusing on this problem, this paper has carried out a study on the multi-objective configuration optimization method (COM) and energy management strategy (EMS) for ship hybrid energy system based on quantum computing. First, the mathematical model of distributed power modules of the hybrid energy system is established, and a configuration optimization objective function that aims at low-cost, long equipment life, and high reliability of power supply is constructed. Then, the combination with fuzzy rules and quantum multi-objective artificial bee colony algorithm to solve the objective function, the configuration scheme satisfying multiple constraints is obtained. On this basis, an energy management optimization objective function that meets both low-cost operation and maximum clean energy utilization for ship electric power system is established, the objective function is optimized in multi-objective quantum particle swarm optimization (QPSO) algorithm, and the real-time optimal scheduling for hybrid energy systems of the ship is realized. Experiments on simulative navigation data verify the feasibility of the multi-objective configuration optimization method using the quantum artificial bee colony (QABC) algorithm. Furthermore, energy management experiments with different strategies, experimental results show that the energy management strategy proposed in this paper outperforms other methods- and effectively reduces the operating costs, fuel costs, and pollutant emissions of marine power system, meeting the environmental requirements of the Energy Efficiency Operating Index (EEOI) for ships.}
}
@article{SHAMACHURN202136,
title = {Optimization of an off-grid domestic Hybrid Energy System in suburban Paris using iHOGA software},
journal = {Renewable Energy Focus},
volume = {37},
pages = {36-49},
year = {2021},
issn = {1755-0084},
doi = {https://doi.org/10.1016/j.ref.2021.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1755008421000168},
author = {Heman Shamachurn},
abstract = {At the COP21, France committed to reduce its greenhouse gas emissions by 50% by 2050. To attain the set goals, it will increase the share of renewable energy in its final energy consumption. Moreover, a 50% reduction in the energy consumption is envisaged. The National Low-Carbon Strategy aims to cut down the emissions of the construction sector by 54% through several actions, including the acceleration of energy renovation work. Understanding how a typical house in France could contribute to the set objectives has not been studied so far. Therefore, an optimized Hybrid Energy System (HES) is proposed for a house in Paris. Wind energy, solar-photovoltaic energy, battery and diesel-generator were considered for a house which had an average measured electricity consumption of 25.81 kWh/day. A cost of energy of 0.28 €/kWh was achieved. The multi-objective optimizations provided, at only 1.4–6% higher Net Present Cost (NPC), 57–73.8% reduced CO2 emission and 56–68% reduced unmet load as compared to the single-objective optimization of minimizing only the NPC. The diesel consumption-related CO2 emission of the optimization scenarios was 2.66–10.14 g per kWh of electricity generated by the proposed HES, which is significantly less than the average of 73 g per kWh of electricity produced in France for the year 2016. Though currently not economically attractive compared to the grid, such an HES, if optimized for several dwellings, will lead to decarbonising development, enabling France to contribute significantly towards its COP21 commitments.}
}
@article{HOU2020113900,
title = {A hierarchical energy management strategy for hybrid energy storage via vehicle-to-cloud connectivity},
journal = {Applied Energy},
volume = {257},
pages = {113900},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.113900},
url = {https://www.sciencedirect.com/science/article/pii/S0306261919315879},
author = {Jun Hou and Ziyou Song},
keywords = {Vehicle-to-cloud connectivity, Energy management, Model predictive control, Real-time optimization, Hybrid energy storage},
abstract = {In order to enhance energy efficiency and improve system performance, the road mobility system requires more preview information and advanced methods. This paper proposes a novel hierarchical optimal energy management strategy for electric buses with a battery/ultracapacitor hybrid energy storage system, to optimal split the power and reduce the battery life degradation. This method is based on vehicle-to-cloud connectivity. In the cloud platform, an optimal energy management strategy is developed using dynamic programming, where the battery degradation cost and the electric cost are taken into consideration. In the vehicle level, a model predictive control is developed to deal with the uncertainties, reduce the energy losses, and handle the system constraints. The cost function of the model predictive control includes the ultracapacitor state of charge planning and energy losses. In order to evaluate the effectiveness of the proposed method, a rule-based energy management strategy is developed as the baseline approach. The China bus driving cycle and other six real bus driving cycles recorded in China are used to validate the robustness of the proposed method. To be more realistic, the random uncertainties up to 20% are included in all driving cycles. Furthermore, the time delay and packet losses in communication are also considered. Simulation results show that the proposed method significantly outperforms the rule-based method, and the average improvement could be over 40% in the studied driving cycles.}
}
@article{TIAN2021106850,
title = {Privacy preservation method for MIQP-based energy management problem: A cloud-edge framework},
journal = {Electric Power Systems Research},
volume = {190},
pages = {106850},
year = {2021},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2020.106850},
url = {https://www.sciencedirect.com/science/article/pii/S0378779620306490},
author = {Nianfeng Tian and Qinglai Guo and Hongbin Sun},
keywords = {Mixed-integer quadratic programming, Cloud-edge framework, Privacy preservation, Information masking},
abstract = {The increasing development of information and communication technologies in power grid makes it possible to optimize the energy management of numerous physical users with cloud-based service, while users’ concerns about privacy security have attracted increasingly more attention. To attack this challenge, this paper proposes the information masking (IM) method for the energy management problem in the form of mixed-integer quadratic programming (MIQP). Additionally, the feasibility and optimality of the recovered solution in the IM of MIQP is proven and given in the form of two lemmas. Next, the implementation mechanism of IM is designed based on a cloud-edge framework. Furthermore, the general requirements of the IM of MIQP are elaborated with respect to privacy security and computational cost to better accommodate practical applications. Finally, the optimal schedule of microgrid with on-site generators and flexible demand resources is modelled and simulated to demonstrate the feasibility and effectiveness of the proposed methodology.}
}
@article{DENIZIAK2021114184,
title = {Synthesis of self-adaptable energy aware software for heterogeneous multicore embedded systems},
journal = {Microelectronics Reliability},
volume = {123},
pages = {114184},
year = {2021},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2021.114184},
url = {https://www.sciencedirect.com/science/article/pii/S0026271421001505},
author = {Stanisław Deniziak and Leszek Ciopiński},
keywords = {Self-adaptivity, Embedded system, Developmental genetic programing, Multicore system},
abstract = {Contemporary embedded systems work in changing environments, some features (e.g., execution time, power consumption) of the system are often not completely predictable. Therefore, for systems with strong constraints, a worst-case design is applied. We observed that by enabling the self-adaptivity we may obtain highly optimized systems still guaranteeing the high quality of service. This paper presents a method of synthesis of real-time software for self-adaptive multicore systems. The method assumes that the system specification is given as a task graph. Then, the tasks are scheduled on a multicore architecture consisting of low-power and high-performance cores. We apply the developmental genetic programming to generate the self-adaptive scheduler and the initial schedule. The initial schedule is optimized, taking into consideration the power consumption, the real-time constraints as well as the self-adaptivity. The scheduler modifies the schedule during the system execution, whenever execution time of the recently finished task occurs other than assumed during the initial scheduling. We propose two models of self-adaptivity: self-optimization of power consumption and self-adaptivity of real-time scheduling. We present some experimental results for standard benchmarks, showing the advantages of our method in comparison with the worst case design used in existing approaches.}
}
@article{CHIKANO2019181,
title = {irbasis: Open-source database and software for intermediate-representation basis functions of imaginary-time Green’s function},
journal = {Computer Physics Communications},
volume = {240},
pages = {181-188},
year = {2019},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2019.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S001046551930058X},
author = {Naoya Chikano and Kazuyoshi Yoshimi and Junya Otsuki and Hiroshi Shinaoka},
keywords = {Matsubara/imaginary-time Green’s function, Many-body quantum theories},
abstract = {The open-source library, irbasis, provides easy-to-use tools for two sets of orthogonal functions named intermediate representation (IR). The IR basis enables a compact representation of the Matsubara Green’s function and efficient calculations of quantum models. The IR basis functions are defined as the solution of an integral equation whose analytical solution is not available for this moment. The library consists of a database of pre-computed high-precision numerical solutions and computational code for evaluating the functions from the database. This paper describes technical details and demonstrates how to use the library.
Program summary
Program Title: irbasis Program Files doi: http://dx.doi.org/10.17632/2fh88ynxm6.1 Licensing provisions: MIT license Programming language: C++, Python. External routines/libraries: numpy, scipy and h5py for Python library, HDF5 C library for C++ library. Nature of problem: Numerical orthogonal systems for Green’s function Solution method: Galerkin method, piece-wise polynomial representation}
}
@article{RAM2022425,
title = {Critical assessment on application of software for designing hybrid energy systems},
journal = {Materials Today: Proceedings},
volume = {49},
pages = {425-432},
year = {2022},
note = {International Conference on Advancement in Materials, Manufacturing and Energy Engineering (ICAMME-2021)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.02.452},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321015728},
author = {Khemshika Ram and Prasanna Kumar Swain and Ruchita Vallabhaneni and Anil Kumar},
keywords = {Hybrid, Renewable energy, Technical analysis, PV, Optimization},
abstract = {HRES (hybrid renewable energy systems) are an amalgamation of traditional and renewable sources that strive to overcome the intermittent nature of renewable energy production sources. The paper aims to provide a critical analysis of the capabilities and limitations of various software to assess HRES. The analysis cycle from design to optimization is an extremely complex task due to the many parameters being considered and has to be done thoroughly. Thus, the use of correct software to obtain comprehensive analysis is critical as it leads to minimal limitations in terms of cost, implementation, and maintenance of the system. When compared to traditional systems, hybridization can reduce costs and emissions by an estimated 20%. The software reviewed are the most widely used and comprehensive for HRES. Yet, they all come with their limitations. For example, HOMER is incapable of conducting a thermal analysis while only HOMER and IHOGA can conduct hydro energy sources analysis. As such, the paper aims to provide an in-depth review of the capabilities and limitations of various software regarding HRES, so that users can decide which software (both individually and in combination with each other) would be most appropriate for their HRES.}
}
@article{CARREGA2020107210,
title = {Coupling energy efficiency and quality for consolidation of cloud workloads},
journal = {Computer Networks},
volume = {174},
pages = {107210},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107210},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619316226},
author = {Alessandro Carrega and Matteo Repetto},
keywords = {Energy efficiency, Quality of service, Workload consolidation, Infrastructure-as-a-service cloud model, Elastic cloud applications},
abstract = {Efficient usage of IT equipment in Data Centers requires modulating their power-consumption according to the actual workload. The most effective strategy to this aim consists in consolidating as many applications as possible on the smallest number of servers, so that idle devices can be shut down or put in low-power states. Usually, this process is driven by computing and networking resources requested by each application (e.g., CPU and RAM) and applies a certain degree of overcommitment, assuming that such resources are not fully used continuously. However, this approach is critical with real workload patterns, which usually change over time; as a matter of fact, consolidation in real scenarios often leads to either low efficiency or violations of Quality of Service (QoS) constraints, depending on the level of overcommitment. In this paper, we investigate a novel consolidation strategy based on an enhanced system model for the Infrastructure-as-a-Service cloud paradigm, which targets a better trade-off between Energy Efficiency and Quality of Service. We explicitly target modular cloud applications, which design is split into multiple components deployed in Virtual Machine (VM)s or containers. Our consolidation strategy allows to “freeze” parts of the application which are not currently used, making them available when requested with minimal latency. This improves energy saving with respect to other approaches, especially when idle VMs are present for backup or redundancy purposes, without degrading the service level. We compare multiple heuristics available in Optaplanner to solve our consolidation problem, and investigate improvements with respect to a more traditional approach. Our evaluation includes both simulations and experimentation in a real test-bed. The comparison shows that the Late Acceptance algorithm on average finds better solutions than other alternatives and energy efficiency improves up to 40% with respect to more conventional strategies, with deterioration of QoS indexes below 1%.}
}
@article{SHARMA2020100373,
title = {An artificial neural network based approach for energy efficient task scheduling in cloud data centers},
journal = {Sustainable Computing: Informatics and Systems},
volume = {26},
pages = {100373},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100373},
url = {https://www.sciencedirect.com/science/article/pii/S2210537918302798},
author = {Mohan Sharma and Ritu Garg},
keywords = {Cloud computing, Artificial neural network, Task scheduling, Energy and rack awareness, Genetic algorithm},
abstract = {Energy efficiency is considered as a crucial objective in cloud data centers as it reduces cost and meets the standard set in green computing. Task scheduling an important problem becomes more complex and critical under energy efficiency consideration. Key issues in recent research on energy efficient task scheduling are execution overhead and scalability. Machine learning has been widely employed for energy efficient task scheduling problem but mostly used to predict resource consumption only instead of deciding the schedule itself. However, we used the neural network to decide which resource should be assigned to given task independently. In this paper, we proposed an energy efficient independent task scheduler using supervised neural networks with the aim to reduce makespan, energy consumption, execution overhead and number of active racks. Proposed artificial neural network-based scheduler takes incoming task and current cloud environment state as input and predict the best computing resource for given task as output which compiles our aim. We used genetic algorithm to generate a huge dataset (∼18 million training instances) and trained our neural network on this dataset using back propagation algorithm with 99.9% accuracy. We simulated experiments on heavily loaded and lightly loaded cloud environment and compared with well-known approaches: Genetic algorithm, MinMIN-MINMin heuristic and Linear regression based energy efficient task schedulers. Results clearly indicate that proposed work outperforms considered algorithms. In heavily (lightly) loaded environment, it improves makespan by 59% (64%), energy consumption by 45% (71%), execution overhead by 88% (43%) respectively and number of active racks by 70%.}
}
@article{SCHWEIZER2020612,
title = {High-speed digital in-line holography for in-situ dust cloud characterization in a minimum ignition energy device},
journal = {Powder Technology},
volume = {376},
pages = {612-621},
year = {2020},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2020.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S0032591020307907},
author = {Christian Schweizer and Shrey Prasad and Ankit Saini and Chad V. Mashuga and Waruna D. Kulatilaka},
keywords = {Digital in-line holography, Minimum ignition energy, Dust explosion, Dust cloud characterization, Particle characterization},
abstract = {Precise measurement of the minimum ignition energy (MIE) of combustible dust is crucial to safety in the process industries. The development of comprehensive methods for MIE dust cloud characterization is therefore highly desirable. The objective of this study is to investigate the application of high-speed digital in-line holography (DIH) for volumetric and in-situ characterization of dust clouds near the ignition zone of a Kühner MIKE3 MIE device. The dispersion behavior of borosilicate glass and soda-lime glass dust clouds are studied at 20 kHz. Size measurements, with successful detection down to 13 μm, are compared with Beckman Coulter particle size analyzer results. The 5-ms hologram videos also yield new transient particle volume, concentration, and velocity measurements. Sample sizes of 15–150 mg of dust powder are sufficient for each high-speed run. This work showcases new diagnostic capabilities that are accessible to dust explosion research and identifies areas for future research.}
}
@article{YAN2020166,
title = {Distributed energy storage node controller and control strategy based on energy storage cloud platform architecture},
journal = {Global Energy Interconnection},
volume = {3},
number = {2},
pages = {166-174},
year = {2020},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S2096511720300505},
author = {Tao Yan and Jialiang Liu and Qianqian Niu and Jizhong Chen and Shaohua Xu and Meng Niu and Jerry Y.S. Lin},
keywords = {Distributed energy storage, Optimum control, Plug and play, Energy storage cloud platform},
abstract = {Based on the energy storage cloud platform architecture, this study considers the extensive configuration of energy storage devices and the future large-scale application of electric vehicles at the customer side to build a new mode of smart power consumption with a flexible interaction, smooth the peak/valley difference of the load side power, and improve energy efficiency. A plug and play device for customer-side energy storage and an internet-based energy storage cloud platform are developed herein to build a new intelligent power consumption mode with a flexible interaction suitable for ordinary customers. Based on the load perception of the power grid, this study aims to investigate the operating state and service life of distributed energy storage devices. By selecting an integrated optimal control scheme, this study designs a kind of energy optimization and deployment strategy for stratified partition to reduce the operating cost of the energy storage device on the client side. The effectiveness of the system and the control strategy is verified through the Suzhou client-side distributed energy storage demonstration project.}
}
@article{GHOSE2020100416,
title = {Urgent point aware energy-efficient scheduling of tasks with hard deadline on virtualized cloud system},
journal = {Sustainable Computing: Informatics and Systems},
volume = {28},
pages = {100416},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100416},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920301438},
author = {Manojit Ghose and Aryabartta Sahu and Sushanta Karmakar},
keywords = {Online scheduling, Energy-efficient, Virtualized cloud, Urgent point, Tasks with hard deadline},
abstract = {Cloud computing platform has emerged to be a promising computing paradigm of recent time. Various applications from different domains having rigid deadline constraints are deployed in the cloud system for their respective benefits. Energy-efficient execution of these applications, meeting their deadline constraints is a challenge. Most of the existing research on the energy-efficient scheduling of these applications in the cloud domain consider a linear relationship between the energy consumption and the resource utilization of the system, and they focus on maximizing the utilization of resources to reduce the active number of computing nodes to minimize energy consumption. In this paper, we first devise a power consumption model for the cloud system which considers both the static and dynamic components of it and assumes a nonlinear relationship with utilization. Then we introduce the concept of urgent points in case of tasks having deadline in the context of a heterogeneous cloud computing environment. Then we propose two energy-efficient scheduling approaches, named UPS and UPS-ES designed based on the urgent points of the tasks and two threshold values of the host utilization. Extensive simulation experiments are conducted both for synthetic tasksets and Google cloud tracelogs. The results are compared with a state of the art scheduling policy and found that our policies perform significantly better than them, with an energy reduction of up to 42% while the deadline constraints of all the tasks are met.}
}
@article{DENG202014123,
title = {Predictive Hybrid Powertrain Energy Management with Asynchronous Cloud Update},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {14123-14128},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1013},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320313781},
author = {Junpeng Deng and Luigi {del Re} and Stephen Jones},
keywords = {powertrain control, predictive control, HEV, cloud computing, energy management},
abstract = {The optimal energy management of a hybrid powertrain has the task to provide the required traction power combining both power sources in the best way. This can be achieved well if the future drive cycle is known/precomputed. However, both speed and traction power requirement may deviate from the expected ones due to many factors, like traffic, weather etc. Against this background, it might be sensible to recompute them whenever needed to keep using the latest future information. Unfortunately, this computation is typically too slow for real time use. In this paper we propose a control structure in which the real time task is solved by a predictive controller which tracks the optimal reference from the cloud, and requests an update of the reference regularly. The update can integrate new information from V2X. This asynchronous operation allows recovering most of the performance of the perfect prediction, while removing tight constraints on the offline computation and copes better with interruptions in communications to the cloud.}
}
@article{MANCEBO2021100558,
title = {FEETINGS: Framework for Energy Efficiency Testing to Improve Environmental Goal of the Software},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100558},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100558},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000494},
author = {Javier Mancebo and Coral Calero and Felix Garcia and Mª Angeles Moraga and Ignacio {Garcia-Rodriguez de Guzman}},
keywords = {Software sustainability, Green software, Energy efficiency, Energy consumption},
abstract = {Software is a fundamental part of today's society. However, both users and software professionals need to be aware that its use impacts on the environment, due to the high energy consumption it entails. One of the main gaps to be faced is the difficulty of analyzing software energy consumption in the endeavor to know whether a particular software product is as much energetically efficient as possible, or at least more efficient than another, and to improve the environmental objectives of the software. For this reason, a Framework for Energy Efficiency Testing to Improve eNviromental Goals of the Software (FEETINGS) is presented in this paper. FEETINGS is composed of three main components: an ontology to provide precise definitions and harmonize the terminology related to software energy measurement; a process to guide researchers in carrying out the energy consumption measurements of the software, and a technological environment, which allows the capture, analysis and interpretation of software energy consumption data. This paper also presents an example of the application of the FEETINGS, which aims to raise awareness of the energy consumed by the software in activities that we perform daily, such as writing a tweet or a Facebook post. As a result, we have been able to verify that FEETINGS allows us to carry out an analysis and measurement of software energy consumption to provide users with good practices, as using an emoji or a picture rather than a GIF. © 2001 Elsevier Science. All rights reserved.}
}
@article{NASERI2021100574,
title = {Reduction of energy consumption and delay of control packets in Software-Defined Networking},
journal = {Sustainable Computing: Informatics and Systems},
volume = {31},
pages = {100574},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100574},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000652},
author = {Abdullah Naseri and Mahmood Ahmadi and Latif PourKarimi},
keywords = {Energy consumption, Linear programming, Packet delay, SDN, SDN controller},
abstract = {The energy consumption of network devices is a significant part of the network's energy costs. Recently, Software-Defined Networking (SDN) has been suggested as a suitable and stable solution to decrease energy consumption. The main concern is how to manage resources for optimum energy consumption in SDN. Hence, this paper proposes a binary linear programming model. This model manages (on or off devices) switches, controllers, and SDN links based on the number of flows at the network edge, the processing capability of the controllers, the distance between switches and controllers, and the traffic. Therefore, with this model, the lowest energy consumption, and the lowest delay of control packets is achieved. The results show that this model, on average, has saved 40% of energy consumption, and it has also had up to 80% decrement when there is less network traffic. In addition, this model has decreased 68% of the delay of controlling packets with 70% decrement in less traffic.}
}
@article{KUMARI2020148,
title = {Blockchain and AI amalgamation for energy cloud management: Challenges, solutions, and future directions},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {148-166},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S074373152030277X},
author = {Aparna Kumari and Rajesh Gupta and Sudeep Tanwar and Neeraj Kumar},
keywords = {Blockchain, AI, Energy cloud management, Smart grid},
abstract = {In the recent years, the Smart Grid (SG) system faces various challenges like the ever-increasing energy demand, the enormous growth of renewable energy sources (RES) with distributed energy generation (EG), the extensive Internet of Things (IoT) devices adaptation, the emerging security threats, and the foremost goal of sustaining the SG stability, efficiency and reliability. To cope up these issues there exists, the energy cloud management (ECM) system, which combines the infrastructure for energy, with intelligent energy usage and value-added services as per consumers demand. To achieve these, efficient demand-side forecasting and secure data transmission are the key factors. The energy management issues pose extreme gravity in finding sustainable solutions by using the blockchain (BC) and Artificial Intelligence (AI). AI-based techniques support various services such as energy load prediction, classification of the consumer, load management, and analysis where the BC provides data immutability and trust mechanism for secure energy management. Therefore, this paper reviews several existing AI-based approaches along with the advantages and challenges of integrating the BC technology and AI in the ECM system. We presented a decentralized AI-based ECM framework for energy management using BC and validate it using a case study. It is shown that how BC and AI can be used to mitigate ECM with security and privacy issues. Finally, we highlighted the open research issues and challenges of the BC-AI-based ECM system.}
}
@article{SAIMLER2020102016,
title = {Uplink/downlink decoupled energy efficient user association in heterogeneous cloud radio access networks},
journal = {Ad Hoc Networks},
volume = {97},
pages = {102016},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.102016},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519304524},
author = {Merve Saimler and Sinem Coleri Ergen},
keywords = {Energy efficiency, User association, Uplink/downlink decoupling},
abstract = {Heterogeneous Cloud Radio Access Networks (H−CRAN) is a network architecture that combines Macro Base Stations (MBS)s and Small Base Stations (SBS)s with cloud infrastructures. The dense deployment of SBSs in H-CRAN is needed to provide high data rates to User Equipments (UEs) but causes high energy consumption. Unrealistic power models lead to inefficient UE association schemes in terms of energy. In this paper, we study the joint optimization of Uplink (UL) and Downlink (DL) decoupled UE association and switching on/off the SBSs in H-CRAN by incorporating a realistic power model with the objective of minimizing the power consumption in H-CRAN. The power model encompasses static and the dynamic power consumption of MBS, the static power consumption of SBS, the power consumption of transmission links to cloud infrastructure and the power consumption of UEs. The problem is transformed into Single Source Capacitated Facility Location Problem (SSCFLP) which is NP-Hard. We then propose a heuristic algorithm based on the use of LP relaxation and solving many-to-one assignment problem with generalized assignment problem heuristics. Extensive simulations demonstrate that the proposed heuristic algorithm performs very close to optimal and achieves significant improvements in minimizing total power consumption compared to coupled UE association algorithm and algorithms utilizing the power consumption models that do not encompass MBS dynamic power consumption and the power consumption of transmission links to cloud infrastructure for various scenarios.}
}
@article{BI201657,
title = {TRS: Temporal Request Scheduling with bounded delay assurance in a green cloud data center},
journal = {Information Sciences},
volume = {360},
pages = {57-72},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516302675},
author = {Jing Bi and Haitao Yuan and Wei Tan and Bo Hu Li},
keywords = {Scheduling, Cloud data center, Particle swarm optimization, Simulated annealing, Energy management},
abstract = {The growing deployment of Internet services in cloud data centers significantly increases the grid energy cost of cloud providers. Considering the environmental effect, many of current cloud providers migrate to green cloud data centers (GCDCs), and seek to reduce the usage of brown energy by partially (or entirely) adopting renewable energy sources. However, the temporal diversity in the grid price, wind speed and solar irradiance makes it a big challenge to minimize the grid energy cost of a GCDC while meeting the performance of each delay bounded request. This work proposes a Temporal Request Scheduling algorithm (TRS) that jointly considers the temporal diversity. TRS considers the long tail in real-life requests’ delay, and can provide strict delay assurance to all arriving requests by scheduling them to execute within their delay bound. Besides, this work explicitly provides mathematical modeling of the relation between the service rate in a GCDC and the refusal of delay bounded requests. Specifically, TRS solves a constrained nonlinear optimization problem by a hybrid meta-heuristic in each of its iterations. Compared with some existing scheduling methods, TRS can achieve higher throughput and lower grid energy cost for a GCDC while meeting each request’s delay requirement.}
}
@article{TOUBAN2022592,
title = {Computed Tomography Measured Psoas Cross Sectional Area Is Associated With Bone Mineral Density Measured by Dual Energy X-Ray Absorptiometry},
journal = {Journal of Clinical Densitometry},
volume = {25},
number = {4},
pages = {592-598},
year = {2022},
issn = {1094-6950},
doi = {https://doi.org/10.1016/j.jocd.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1094695022000439},
author = {Basel M. Touban and Michael J. Sayegh and Jesse Galina and Sonja Pavlesen and Tariq Radwan and Mark Anders},
keywords = {Bone mineral density, CT scan, DEXA, fragility fracture, osteoporosis, sarcopenia},
abstract = {Dual-energy X-ray absorptiometry (DEXA) is the gold standard for osteoporosis screening and diagnosis. However, abdominal conventional computed tomography (CT) scan is widely available and multiple studies validated its use as a screening tool for osteoporosis compared to DEXA. The aim of this study was to determine the reliability of measuring core muscle size at the L3–L4 intervertebral disk space and estimate the relationship between core muscle size and bone mineral density (BMD) measured by DEXA. Retrospective chart review was performed on patients who underwent a DEXA scan for osteoporosis and a conventional abdominal CT scan within one-year apart. Total cross-sectional area (CSA) and Hounsfield Unit (HU) density of core muscles (psoas, paraspinal, and abdominal wall muscles) were measured. The association between psoas, paraspinal, abdominal, and central muscle CSA and Bone Mineral density (BMD) at L3, L4, total Lumbar Spine (LS), and right (R) and left (L) hip was estimated in crude and adjusted for age and sex linear regression models. Sixty patients (37 females, 23 males) met the inclusion criteria. The average interval between DEXA and abdominal CT scans was 3.6 months (range 0.1–10.2). Psoas muscle density was significantly positively associated with R hip BMD in both crude and adjusted models (β = 20.2, p = 0.03; β = 18.5, p = 0.01). We found a significant positive linear association between psoas muscle CSA and HU density with BMD of LS, R, and L hip in both crude and adjusted models. The strongest significant positive linear association was observed between total abdominal CSA and R hip BMD in crude and age and sex adjusted (ß = 85.3, p = 0.01; ß = 63.9, p = 0.02, respectively). CT scans obtained for various clinical indications can provide valuable information regarding BMD. This is the first study investigating association between BMD with central muscle density and CSA, and it demonstrated their significant positive the association.}
}
@article{BAZEL2017471,
title = {A green cloud-point microextraction method for spectrophotometric determination of Ni(II) ions with 1-[(5-benzyl-1,3-thiazol-2-yl)diazenyl]naphthalene-2-ol},
journal = {Journal of Molecular Liquids},
volume = {242},
pages = {471-477},
year = {2017},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2017.07.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167732217308334},
author = {Yaroslav Bazel and Andrii Tupys and Yurii Ostapiuk and Oleksandr Tymoshuk and Vasyl Matiychuk},
keywords = {Nickel(II), Thiazolylazonaphthol dyes, Triton X-100, Spectrophotometry, Cloud-point microextraction},
abstract = {In this paper the efficiency of cloud point microextraction method has been confirmed by the preconcentration and spectrophotometric determination of nickel(II) ions with a new organic reagent 1-[(5-benzyl-1,3-thiazol-2-yl)diazenyl]naphthalene-2-ol. The influence of several factors (the acidity of the media, the time of the reaction and heating, the type of extraction solvent used) on the interactions between Ni(II) and the azo dye was checked. Optimal conditions for carrying out the complexation reaction between the reagent and Ni(II) were established (absorbance maximum wavelength λmax=605nm, time of heating the solution theat.=10min, the acidity pHopt.=5.5). Qualitative characteristics of the formed complex were calculated (ε610=1.56×104Lmol−1cm−1, lgβ=6.18). Two spectrophotometric methods of nickel(II) determination with the application of conventional liquid-liquid extraction and cloud-point microextraction were developed, and their main analytical characteristics were compared. The elaborated extraction technique with the application of cloud-point extraction and liquid-liquid microextraction provides significantly better performance characteristics for nickel determination (LOD=3.9μgL−1, LOQ=11.8μgL−1), and instead of 25mL of toxic toluene, only 200μL of ethylene glycol was used. The new method of Ni(II) determination was tested on model solutions and real samples.}
}
@article{SUN2020105347,
title = {Autonomous cell activation for energy saving in cloud-RANs based on dueling deep Q-network},
journal = {Knowledge-Based Systems},
volume = {192},
pages = {105347},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105347},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119306112},
author = {Guolin Sun and Daniel Ayepah-Mensah and Anton Budkevich and Guisong Liu and Wei Jiang},
keywords = {Cell activation, Deep reinforcement learning, Energy efficiency, Dueling deep Q-network, H-CRAN},
abstract = {Heterogeneous cloud radio access network (H-CRAN) is a promising technology to help overcome the traffic density which in 5G communication networks. One of the main challenges that will arise in H-CRAN is how to minimize energy consumption. In this paper, a deep reinforcement learning method is used to minimize energy consumption. Firstly, we propose an autonomous cell activation framework and customized physical resource allocation schemes to balance energy consumption and QoS satisfaction in C-RANs. We formulate the cell activation problem as a Markov decision process(MDP). To solve the problem, we develop a dueling deep Q-network (DQN) based autonomous cell activation framework to ensure user QoS demand and minimized energy consumption with the minimum number of active RRHs under varying traffic demand. Simulation results illustrate the effectiveness of our proposed solution in minimized energy consumption in a network.}
}
@article{GUCHHAIT2022,
title = {Intelligent reactive power control of renewable integrated hybrid energy system model using static synchronous compensators and soft computing techniques},
journal = {Journal of King Saud University - Engineering Sciences},
year = {2022},
issn = {1018-3639},
doi = {https://doi.org/10.1016/j.jksues.2022.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1018363922000307},
author = {Pabitra Kumar Guchhait and Samrat Chakraborty and Debottam Mukherjee and Ramashis Banerjee},
keywords = {Wind-diesel hybrid power system, Transient stability, Static synchronous compensator, PID with derivative filter controller, Soft computing},
abstract = {Modern power system faces a severe problem of instability largely due to inconsistent reactive power. It causes damage to the power grid within a few milliseconds. Therefore, proper management of reactive power under disturbing situations has a key role in its safe operation. Devices such as flexible alternating current transmission systems (FACTS) accurately manage the system’s reactive power in accordance with the load demand. In this study, a new reactive power control strategy is employed for optimization of the reactive power along with the stability improvement of the system under different small perturbed conditions. Therefore, this study focuses on controlling the reactive power for an isolated wind-diesel hybrid power system model (WDHPSM) with the aid of a static synchronous compensator (STATCOM) together with the use of an integral minus proportional derivative (IPD) controller keeping a derivative-based filter (IPDF) as a secondary controller for better utilization of its purpose. The obtained results are compared when the no control strategy is applied in the model. Another comparison has been done between the multiple applied soft computing techniques (oppositional harmonic search, ant lion optimization, binary-coded genetic algorithm, and symbiosis organisms search) which optimize the parameters of the controller of WDHPSM.}
}
@article{TANG2022108857,
title = {An UAV-assisted mobile edge computing offloading strategy for minimizing energy consumption},
journal = {Computer Networks},
volume = {207},
pages = {108857},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108857},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622000688},
author = {Qiang Tang and Lixin Liu and Caiyan Jin and Jin Wang and Zhuofan Liao and Yuansheng Luo},
keywords = {Mobile edge computing, Calculation task allocation, Unmanned aerial vehicle communications},
abstract = {Owing to the high economic benefits, flexible deployment, and controllable maneuverability, unmanned aerial vehicles (UAVs) have been envisioned as promising and potential technologies for dispensing wireless communication services. This paper investigates a mobile edge computing (MEC) system assisted by multiple access points (APs) and an UAV, in which APs may not be able to straightly establish wireless communications with terrestrial Internet of Thing devices (IoT) due to ground signal blockage. Consequently, an UAV is dispatched as a mobile AP to serve a group of users and render the air-to-ground channel. In this scenario, we contemplate dividing the computing tasks of IoTDs into three parts: either be calculated locally, or offloaded to the UAV for processing, or accomplished on AP through relaying. This work attempts to minimize the weighted sum of communication consumption, calculation consumption, and the UAV’s flight consumption over a finite UAV mission duration by jointly optimizing calculation task allocation ratio, power distribution as well as the UAV’s trajectory. However, the resulting problem we put forward is demonstrated to be highly non-convex and challenging to solve. To tackle this issue, we decompose the original problem into two sub-problems hinging on the block coordinate descent (BCD) method. We settle the two sub-problems iteratively through the Lagrangian duality method and succession convex approximation (SCA) technique. The simulation results further reveal that the proposed approach is superior to other comparison baselines.}
}
@article{ALIRAHMI2022101850,
title = {Soft computing based optimization of a novel solar heliostat integrated energy system using artificial neural networks},
journal = {Sustainable Energy Technologies and Assessments},
volume = {50},
pages = {101850},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101850},
url = {https://www.sciencedirect.com/science/article/pii/S221313882100864X},
author = {S. Mojtaba Alirahmi and A. Khoshnevisan and P. Shirazi and P. Ahmadi and D. Kari},
keywords = {Artificial neural network, MED-TVC, PEM electrolyzer, Exergoeconomic, Gasifier},
abstract = {This study proposes and investigates a novel energy system based on biomass and solar energy. This plant is composed of a biomass unit, a solar unit, and a waste-heat recovery unit. This novel proposed integrated system can provide the needs such as electricity, hydrogen, freshwater, heating, and hot water production. For electricity generation, two gas turbines, one steam Rankine cycle, and one organic Rankine cycle are used. In contrast, for utilization of solar energy, a heliostat field, and for biomass conversion, a gasifier is used. In addition, the desalination unit and PEM electrolyzer are utilized to produce fresh water and hydrogen, respectively. Firstly, the present work aims to investigate the developed system from the exergoeconomic and environmental perspective. Multi-objective optimization is conducted to determine the maximum amount of exergetic efficiency and the minimum value of the cost rate. An artificial neural network (ANN) is employed as a mediator tool to accelerate the optimization process. The relation between objective functions and design parameters is studied utilizing ANN to obtain the plant optimal decision variables. Employing the Pareto Envelope-based selection algorithm II (PESA-II) method, the optimum amount for the total cost rate and exergy efficiency is found 224.1 $/h and 26.7%, respectively. In addition, three evolutionary-based optimization algorithms are applied to determine the optimum results of the suggested plant.}
}
@article{SINGH2022230,
title = {Gene expression programming for computing energy dissipation over type-B piano key weir},
journal = {Renewable Energy Focus},
volume = {41},
pages = {230-235},
year = {2022},
issn = {1755-0084},
doi = {https://doi.org/10.1016/j.ref.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1755008422000242},
author = {Deepak Singh and Munendra Kumar},
abstract = {Computation of energy dissipation over weir (or spillway) structures is vital to solving numerous engineering problems, notably during floods. Therefore, an accurate estimation of the energy dissipation at the base of the spillway is essential. The spillways must be designed to spill large amounts of water at high efficiency with high structural performances without causing any damage to the structure or its surroundings. Energy dissipation over weir structures is difficult to predict using conventional formulas based on empirical methods. Consequently, new and accurate techniques are still highly demanded. In this study, Gene Expression Programs (GEP) were used to estimate the relative energy dissipation at the base of type-B Piano Key Weirs (PKWs) by taking three non-dimensional parameters into account: headwater ratio, magnification ratio, and the number of cycles. The performances of the GEP model were compared with empirical equations based on statistical factors coefficient of determination (R2), and root mean square error (RMSE). The computed values of the relative residual energy using the proposed models are within ±5% of the observed ones. Results indicate that the proposed GEP model predicted the relative residual energy satisfactorily with the coefficient of determination (R2 = 0.9979 for training, 0.9980 for testing) and root mean square error (RMSE) of 0.0099, 0.0092 for training and testing datasets, respectively.}
}
@article{GHASEMI201693,
title = {Application of Micro-cloud point extraction for spectrophotometric determination of Malachite green, Crystal violet and Rhodamine B in aqueous samples},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {164},
pages = {93-97},
year = {2016},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1386142516301627},
author = {Elham Ghasemi and Massoud Kaykhaii},
keywords = {Micro-cloud point extraction, Malachite green, Crystal violet, Rhodamine B, Water analysis},
abstract = {A novel, green, simple and fast method was developed for spectrophotometric determination of Malachite green, Crystal violet, and Rhodamine B in water samples based on Micro-cloud Point extraction (MCPE) at room temperature. This is the first report on the application of MCPE on dyes. In this method, to reach the cloud point at room temperature, the MCPE procedure was carried out in brine using Triton X-114 as a non-ionic surfactant. The factors influencing the extraction efficiency were investigated and optimized. Under the optimized condition, calibration curves were found to be linear in the concentration range of 0.06–0.60mg/L, 0.10–0.80mg/L, and 0.03–0.30mg/L with the enrichment factors of 29.26, 85.47 and 28.36, respectively for Malachite green, Crystal violet, and Rhodamine B. Limit of detections were between 2.2 and 5.1μg/L.}
}
@article{CASTELLANOS2020782,
title = {Effect of particle size polydispersity on dust cloud minimum ignition energy},
journal = {Powder Technology},
volume = {367},
pages = {782-787},
year = {2020},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2020.04.037},
url = {https://www.sciencedirect.com/science/article/pii/S0032591020303235},
author = {Diana Castellanos and Pranav Bagaria and Chad V. Mashuga},
keywords = {Dust explosion, Polydispersity, Minimum ignition energy (MIE), Aluminum dust},
abstract = {Dust explosions in the process industries have prompted the need for better understanding of dust explosion fundamentals to improve risk assessments. Parameters such as Pmax, Kst and MIE are important to the risk assessment process and depend on dust size, shape, chemical composition etc. Studies often cited particle size (median diameter, d50) as a crucial parameter. However, d50 does not provide information regarding size distribution span (polydispersity), which effects the explosion parameters. This study investigates five aluminum powder blends with similar d50 and varying polydispersity to demonstrate the weighted mean diameter D (3,2) is a better statistic to associate with MIE. This study highlights the importance of reporting polydispersity, D (3,2) along with d50/median to properly assess the particle size distribution of the dust for accurate explosion risk assessment.}
}
@article{SHCHERBINA202274,
title = {Molecular complexes of non-chelating polydentate Lewis bases with group 13 Lewis acids: crystal structure and computed energy of stepwise donor–acceptor bond formation},
journal = {Mendeleev Communications},
volume = {32},
number = {1},
pages = {74-77},
year = {2022},
issn = {0959-9436},
doi = {https://doi.org/10.1016/j.mencom.2022.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S0959943622000244},
author = {Nadezhda A. Shcherbina and Igor V. Kazakov and Anna S. Lisovenko and Mariya A. Kryukova and Irina S. Krasnova and Michael Bodensteiner and Alexey Y. Timoshkin},
keywords = {donor–acceptor interaction, group 13 Lewis acids, pyrazine, hexamethylenetetramine, structural study, DFT computations},
abstract = {The crystal structures of five donor–acceptor (DA) complexes of 1:1 and 2:1 composition between E(C6F5)3 (E = B, Al, Ga and In) and pyrazine (pyz) as a non-chelating bidentate nitrogen-containing donor, as well as the GaI3 pyz GaI3 complex have been established for the first time. A joint analysis of the experimental structural data and the results of computations at the M06-2X/def2-TZVP level of theory reveals that with an increase in the number of acceptor molecules in the DA complex, the DA bond distances increase, while the DA bond energies and Wiberg bond indexes decrease, indicating a weaker bonding. The previously reported ‘inverse’ relationship between the Lewis acidity and the capacity of a polydentate donor to complex with multiple Lewis acids is not confirmed.}
}
@article{CUTAJAR2021102750,
title = {A Software Tool for the Design and Operational Analysis of Pressure Vessels used in Offshore Hydro-pneumatic Energy Storage},
journal = {Journal of Energy Storage},
volume = {40},
pages = {102750},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102750},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21004801},
author = {Charise Cutajar and Tonio Sant and Robert N. Farrugia and Daniel Buhagiar},
keywords = {Offshore, Hydro-pneumatic, Energy storage, Pressure vessels, Optimisation},
abstract = {Despite the breakthroughs and rapid growth in the global installed offshore wind capacity over the last two decades, a prevailing major challenge is the intermittent nature of the wind resource itself. Consequently, co-locating energy storage systems with wind turbines deployed out at sea is perceived as a key component that will help to overcome this crucial challenge and address the mismatch between the supply of green energy and actual energy demand in real time. Of particular interest are hydro-pneumatic energy storage solutions that can deliver a safer and more reliable operation for a longer life span when compared to electro-chemical storage solutions. This paper presents a smart software tool named SmartPVB, which has been specifically developed for the optimisation of the design of pressure vessel bundles used in offshore hydro-pneumatic energy storage systems. The optimised design parameters obtained through the software SmartPVB help drive the material requirements to a minimum. A sensitivity study to analyse the influence of various parameters on a hydro-pneumatic energy storage system mounted on the seabed at a water depth of 200 m is also outlined. Results suggest that the optimal operating pressure ratio to minimise steel requirments for the pressure vessel bundle lies between 2.4 and 2.6. Furthermore, it is shown that the overall mass of the pressure vessel bundle per unit of storage capacity increases with sea depth, while it decreases with increasing peak operating pressures up to 200 bar.}
}
@article{YAN202085,
title = {Network security protection technology for a cloud energy storage network controller},
journal = {Global Energy Interconnection},
volume = {3},
number = {1},
pages = {85-97},
year = {2020},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2020.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S2096511720300293},
author = {Tao Yan and Jialiang Liu and Qianqian Niu and Jizhong Chen and Shaohua Xu and Meng Niu},
keywords = {Cloud energy storage system, Node controller, Network security, Smart grid, Distributed energy storage},
abstract = {As part of the ongoing information revolution, smart power grid technology has become a key focus area for research into power systems. Intelligent electrical appliances are now an important component of power systems, providing a smart power grid with increased control, stability, and safety. Based on the secure communication requirements of cloud energy storage systems, this paper presents the design and development of a node controller for a cloud energy storage network. The function division and system deployment processes were carried out to ensure the security of the communication network used for the cloud energy storage system. Safety protection measures were proposed according to the demands of the communication network, allowing the system to run safely and stably. Finally, the effectiveness of the system was verified through a client-side distributed energy storage demonstration project in Suzhou, China. The system was observed to operate safely and stably, demonstrating good peak-clipping and valley filling effects, and improving the system load characteristics.}
}
@article{GUEROUT2014225,
title = {Quality of service modeling for green scheduling in Clouds},
journal = {Sustainable Computing: Informatics and Systems},
volume = {4},
number = {4},
pages = {225-240},
year = {2014},
note = {Special Issue on Energy Aware Resource Management and Scheduling (EARMS)},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2014.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S221053791400047X},
author = {Tom Guérout and Samir Medjiah and Georges {Da Costa} and Thierry Monteil},
keywords = {Cloud computing, Quality of service, Energy-efficiency, Dynamic Voltage and Frequency Scaling, Multi-objective optimization, Simulation},
abstract = {Most Cloud providers support services under constraints of Service Level Agreement (SLA) definitions. The SLAs are composed of different quality of service (QoS) rules promised by the provider. Thus, the QoS in Clouds becomes more and more important. Precise definitions and metrics have to be explained. This article proposes an overview of Cloud QoS parameters as well as their classification, but also it defines usable metrics to evaluate QoS parameters. Moreover, the defined QoS metrics are measurable and reusable in any scheduling approach for Clouds. The use of these QoS models is done through the performance analysis of three scheduling approaches considering four QoS parameters. In addition to the energy consumption and the Response Time, two other QoS parameters are taken into account in different virtual machines scheduling approaches. These parameters are dynamism and robustness, which are usually not easily measurable. The evaluation is done through simulations, using two common scheduling algorithms and a Genetic Algorithm (GA) for virtual machines (VMs) reallocation, allowing us to analyze the QoS parameters evolution in time. Simulation results have shown that including various and antagonist QoS parameters allows a deeper analysis of the intrinsic behavior and insight of these three algorithms. Also, it is shown that the multi-objective optimization allows the service provider to seek the best trade-off between service performances and end user's experience.}
}
@article{ETXEGARAI20221,
title = {An analysis of different deep learning neural networks for intra-hour solar irradiation forecasting to compute solar photovoltaic generators' energy production},
journal = {Energy for Sustainable Development},
volume = {68},
pages = {1-17},
year = {2022},
issn = {0973-0826},
doi = {https://doi.org/10.1016/j.esd.2022.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0973082622000230},
author = {Garazi Etxegarai and Asier López and Naiara Aginako and Fermín Rodríguez},
keywords = {Solar irradiation forecasting, Artificial Neural Network, Very short-term forecasting, Long Short Term memory, Convolutional Neural Network},
abstract = {Renewable energies are the alternative that leads to a cleaner generation and a reduction in CO2 emissions. However, their dependency on weather makes them unreliable. Traditional energy operators need a highly accurate estimation of energy to ensure the appropriate control of the network, since energy generation and demand must be balanced. This paper proposes a forecaster to predict solar irradiation, for very short-term, specifically, in the 10 min ahead. This study develops two tools based on artificial neural networks, namely Long-Short Term Memory neural networks and Convolutional Neural Network. The results demonstrate that the Convolutional Neural Network has a higher accuracy. The tool is tested examining the root mean square error, which was of 52.58 W/m2 for the testing step. Compared against the benchmark, it has obtained an improvement of 8.16%. Additionally, for the 82% of the tested days it has given a less than 4% error between the predicted and the actual energy generation. Results indicate that the forecaster is accurate enough to be implemented on a photovoltaic generation plan, improving their integration into the electrical grid, not only for providing power but also ancillary services.}
}
@article{LIU2016104,
title = {Green data center with IoT sensing and cloud-assisted smart temperature control system},
journal = {Computer Networks},
volume = {101},
pages = {104-112},
year = {2016},
note = {Industrial Technologies and Applications for the Internet of Things},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2015.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S1389128615004739},
author = {Qiang Liu and Yujun Ma and Musaed Alhussein and Yin Zhang and Limei Peng},
keywords = {Internet of Things, Air Conditioning, Cloud Computing, Green Data Center},
abstract = {With the growing shortage of energy around the world, energy efficiency is one of the most important considerations for a data center. In this paper, we propose a green data center air conditioning system assisted by cloud techniques, which consists of two subsystems: a data center air conditioning system and a cloud management platform. The data center air conditioning system includes environment monitoring, air conditioning, ventilation and temperature control, whereas the cloud platform provides data storage and analysis to support upper-layer applications. Moreover, the detailed design and implementation are presented, including the dispatch algorithm for the temperature control, topological structure of the sensor network, and framework for the environment monitoring node. A feasibility evaluation is used to verify that the proposed system can significantly reduce the data center energy consumption without degradation in the cooling performance.}
}
@article{KUNWAR2022106930,
title = {Multi-phase field simulation of Al3Ni2 intermetallic growth at liquid Al/solid Ni interface using MD computed interfacial energies},
journal = {International Journal of Mechanical Sciences},
volume = {215},
pages = {106930},
year = {2022},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2021.106930},
url = {https://www.sciencedirect.com/science/article/pii/S0020740321006408},
author = {Anil Kunwar and Ensieh Yousefi and Xiaojing Zuo and Youqing Sun and David Seveno and Muxing Guo and Nele Moelans},
keywords = {Interfacial energy, Molecular dynamics, Phase field method, AlNi IMC, Multiscale simulation},
abstract = {Considering its application in developing Raney-type Ni catalysts and in metal surface coatings, the study on the growth behavior of Al3Ni2 intermetallic compound (IMC) at the Al/Ni material interface is of utmost importance. The present work integrates nanoscale molecular dynamics (MD) calculation with mesoscale phase field model for studying the interfacial phenomena associated with Al3Ni2 growth in Al/Ni interface at 1173.15 K. The interfacial energies computed from MD are in the range 0.9–1.2 J/m2 with FCC/IMC featuring as the interface with the largest value and IMC/LIQUID as the one with the lowest value. Phase field model parameters characterizing a varying interface energy formulation are established to simulate the 2D growth of interfacial IMC grains. With the help of an atomistically informed phase field model, it has been revealed that the phase areas and morphology are obviously sensitive to the interfacial properties. The methodologies and results of these multiscale simulations for IMC interfaced between Al and Ni microstructures offer the complementary and accelerated design route of in-silico studies for materials systems experimented at high temperature.}
}
@article{CANIZARES2020110522,
title = {MT-EA4Cloud: A Methodology For testing and optimising energy-aware cloud systems},
journal = {Journal of Systems and Software},
volume = {163},
pages = {110522},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110522},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300054},
author = {Pablo C. Cañizares and Alberto Núñez and Juan {de Lara} and Luis Llana},
keywords = {Cloud modelling, Metamorphic testing, Simulation, Evolutionary algorithms, Energy-aware systems},
abstract = {Currently, using conventional techniques for checking and optimising the energy consumption in cloud systems is unpractical, due to the massive computational resources required. An appropriate test suite focusing on the parts of the cloud to be tested must be efficiently synthesised and executed, while the correctness of the test results must be checked. Additionally, alternative cloud configurations that optimise the energetic consumption of the cloud must be generated and analysed accordingly, which is challenging. To solve these issues we present MT-EA4Cloud, a formal approach to check the correctness – from an energy-aware point of view – of cloud systems and optimise their energy consumption. To make the checking of energy consumption practical, MT-EA4Cloud combines metamorphic testing, evolutionary algorithms and simulation. Metamorphic testing allows to formally model the underlying cloud infrastructure in the form of metamorphic relations. We use metamorphic testing to alleviate both the reliable test set problem, generating appropriate test suites focused on the features reflected in the metamorphic relations, and the oracle problem, using the metamorphic relations to check the generated results automatically. MT-EA4Cloud uses evolutionary algorithms to efficiently guide the search for optimising the energetic consumption of cloud systems, which can be calculated using different cloud simulators.}
}
@article{KHAN2020102497,
title = {An energy, performance efficient resource consolidation scheme for heterogeneous cloud datacenters},
journal = {Journal of Network and Computer Applications},
volume = {150},
pages = {102497},
year = {2020},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.102497},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519303571},
author = {Ayaz Ali Khan and Muhammad Zakarya and Rahim Khan and Izaz Ur Rahman and Mukhtaj Khan and Atta ur Rehman Khan},
keywords = {Containers, Performance, Migrations, Energy efficiency, Clouds},
abstract = {Datacenters are the principal electricity consumers for cloud computing that provide an IT backbone for today's business and economy. Numerous studies suggest that most of the servers, in the US datacenters, are idle or less-utilised, making it possible to save energy by using resource consolidation techniques. However, consolidation involves migrations of virtual machines, containers and/or applications, depending on the underlying virtualisation method; that can be expensive in terms of energy consumption and performance loss. In this paper, we: (a) propose a consolidation algorithm which favours the most effective migration among VMs, containers and applications; and (b) investigate how migration decisions should be made to save energy without any negative impact on the service performance. We demonstrate through a number of experiments, using the real workload traces for 800 hosts, approximately 1516 VMs, and more than million containers, how different approaches to migration, will impact on datacenter's energy consumption and performance. We suggest, using reasonable assumptions for datacenter set-up, that there is a trade-off involved between migrating containers and virtual machines. It is more performance efficient to migrate virtual machines; however, migrating containers could be more energy efficient than virtual machines. Moreover, migrating containerised applications, that run inside virtual machines, could lead to energy and performance efficient consolidation technique in large-scale datacenters. Our evaluation suggests that migrating applications could be ~5.5% more energy efficient and ~11.9% more performance efficient than VMs migration. Further, energy and performance efficient consolidation is ~14.6% energy and ~7.9% performance efficient than application migration. Finally, we generalise our results using several repeatable experiments over various workloads, resources and datacenter set-ups.}
}
@article{KAHL2022128267,
title = {Data-Constrained Modelling with multi-energy X-ray computed microtomography to evaluate the porosity of plasma sprayed ceramic coatings},
journal = {Surface and Coatings Technology},
volume = {436},
pages = {128267},
year = {2022},
issn = {0257-8972},
doi = {https://doi.org/10.1016/j.surfcoat.2022.128267},
url = {https://www.sciencedirect.com/science/article/pii/S0257897222001888},
author = {B.A. Kahl and Y.S. Yang and C.C. Berndt and A.S.M. Ang},
keywords = {Data-Constrained Modelling, X-ray micro Computed Tomography, Synchrotron radiation, Ceramic coatings, Porosity, Plasma spray},
abstract = {Atmospheric plasma spray (APS) deposited ceramic coatings of zirconium boride (ZrB2) and hydroxyapatite (HAp) were inspected using Data-Constrained Modelling (DCM) of X-ray Micro-Computed Tomography (X-ray μ-CT) datasets to evaluate coating porosity. The DCM approach was used to quantify the porosity and void distributions in the coatings. The results from the 3D analysis were compared to 2D porosity and void distributions determined from the image analysis method. The 3D porosity determined from the ZrB2-1 model, 24.7%, agreed well with the 2D porosity estimation of 22.1 ± 2.6%. Additionally, the 3D porosity determined from the HAp-1 model, 22.8%, was marginally greater than the estimated 2D porosity, 19.8 ± 2.1%. However, a comparison of the 2D and 3D void distributions revealed that a 2D assessment poorly predicted the 3D microstructure of coatings and cannot be used to infer properties that depend strongly on the 3D void network. The DCM method has the ability to include the effect of fine-structure details over the 3D scan volume and hence incorporated micropores and cracks that are typically overlooked during traditional porosity inspections. Furthermore, the 3D analysis demonstrated the deficiencies in typical CT segmentation methods applied to data with a moderate CT resolution size, which was 5.4 μm. The superior DCM approach enabled the quantification of pores below the CT resolution limit that would not have been accurately modelled using typical CT segmentation methods.}
}
@article{ZHANG2022123,
title = {Momentum computed tomography of low-energy charged particles produced in collisional reactions},
journal = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
volume = {511},
pages = {123-142},
year = {2022},
issn = {0168-583X},
doi = {https://doi.org/10.1016/j.nimb.2021.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0168583X2100375X},
author = {Yuezhao Zhang and Deyang Yu},
keywords = {Atomic collision, Sputtering, High energy density physics, Particle momentum distribution, Imaging spectrometer, Momentum computed tomography, Iterative reconstruction, Maximum , Compressed sensing, Alternating direction method of multipliers},
abstract = {The momentum distributions of secondary particles in collisional reactions carry rich information about the interaction dynamics. Traditional spectrometer measurements are always limited to certain aiming directions or a small solid angle. The velocity map imaging device is developed only for certain symmetric momentum distributions, while the reaction microscope is restricted to low count-rate experiments. They all fail in imaging arbitrary momentum distributions of high-intensity particle showers. We propose the momentum computed tomography (MCT) to overcome this situation, which pursues a projection-reconstruction solution to the measurement problem. We lay down the projection theory both in the continuum and discretized forms in the context of a general parameterized MCT model. The reconstruction problem is formulated under the maximum a posteriori framework and the compressed sensing scenario. It is solved with an indefinite preconditioned alternating direction method of multipliers. Numerical experiments are carried out to illustrate the MCT working principles.}
}
@article{LYDIA2022473,
title = {Soft computing models for forecasting day-ahead energy consumption},
journal = {Materials Today: Proceedings},
volume = {58},
pages = {473-477},
year = {2022},
note = {International Conference on Artificial Intelligence & Energy Systems},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322012949},
author = {M. Lydia and G. {Edwin Prem Kumar}},
keywords = {Forecasting, Energy consumption, Time-series, Linear Regression, SMO, Bagging},
abstract = {The massive increase in automation, innovative technologies and standard of living has witnessed an equivalent rise in energy consumption. The depletion of conventional energy sources and the challenges faced in harnessing the non-conventional sources have reinforced the necessity for optimized energy consumption. Forecasting energy consumption will serve as a great tool for meticulous planning, judicious usage and outage prevention. In this paper, time series forecasting models have been used to forecast day-ahead energy consumption. Soft computing approaches have been employed for building single step and multiple step forecasting using five different datasets. The performance of the models has been evaluated using appropriate performance criteria.}
}
@article{UETANI2022111,
title = {Visualization of pulmonary artery intimal sarcoma by color-coded iodine map using dual-energy computed tomography},
journal = {Journal of Cardiology Cases},
volume = {26},
number = {2},
pages = {111-113},
year = {2022},
issn = {1878-5409},
doi = {https://doi.org/10.1016/j.jccase.2022.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S1878540922000603},
author = {Teruyoshi Uetani and Shinji Inaba and Haruhiko Higashi and Jun Irita and Jun Aono and Hikaru Nishiyama and Yuki Tanabe and Riko Kitazawa and Teruhito Kido and Shuntaro Ikeda and Osamu Yamaguchi},
keywords = {Pulmonary artery intimal sarcoma, Cardiac tumor, Computed tomography, Positron emission tomography},
abstract = {Pulmonary artery intimal sarcomas (PAIS) are often misdiagnosed as pulmonary embolisms (PE) as their clinical findings and imaging findings are similar. However, given the clinical outcome of both diseases is different in its prognosis, accurate and rapid diagnosis is mandatory. This is a case report of a histologically-proven PAIS which was initially treated as a PE. The color-coded iodine map using dual-energy computed tomography (dual-energy CT iodine map) well reflected the distribution of the tumor consistent with 18fluoro-2-deoxyglucose-uptake region using positron emission tomography/CT. This case demonstrates the potential of using dual-energy CT iodine map to differentiate PAIS from PE.
Learning objective
Use of a dual-energy computed tomography iodine map to visualize a pulmonary artery intimal sarcoma may provide useful diagnostic information.}
}
@article{SIGWELE2020107302,
title = {Energy-efficient 5G cloud RAN with virtual BBU server consolidation and base station sleeping},
journal = {Computer Networks},
volume = {177},
pages = {107302},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107302},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620301742},
author = {Tshiamo Sigwele and Yim Fun Hu and Misfa Susanto},
keywords = {Cloud computing, C-RAN, 5G, Virtual machine placement, Simulated annealing, Genetic algorithm, Energy-efficiency},
abstract = {Heterogeneous network (HetNet) deployment where macro cells are overlaid by small cells is considered a de-facto solution for meeting the ever increasing mobile traffic demand in fifth generation (5G) networks. However, deployment of a large number of small cell base stations (BSs) result in considerable increase in HetNet energy consumption. Cloud radio access networks (C-RAN) has been proposed as an energy-efficient architecture that leverages cloud computing technology where baseband processing is performed in virtual baseband units (vBBU) in the BS cloud. In this paper, we address the energy efficiency (EE) optimization problem in the downlink for two-tier heterogeneous C-RAN (H-CRAN) comprising of macro and pico-cells. At the radio side of H-CRAN, a dynamic pico BS switching OFF algorithm based on a utility function is proposed while maintaining coverage and quality of service (QoS). In the cloud side, heuristic approximation algorithms are proposed including simulated annealing (H-CRAN SA) and genetic algorithm (H-CRAN GA) to minimize energy consumption by reducing the number of BBU servers used through vBBU placement. The proposed scheme is compared with distributed long term evolution advanced (LTE-A) Hetnet system and simulation results show that the proposed H-CRAN SA and H-CRAN GA schemes save 48% and 45% of energy on a daily average, respectively while maintaining the required QoS.}
}
@article{CATTOLUCCHINO2021107906,
title = {Modelling double skin façades (DSFs) in whole-building energy simulation tools: Validation and inter-software comparison of a mechanically ventilated single-story DSF},
journal = {Building and Environment},
volume = {199},
pages = {107906},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107906},
url = {https://www.sciencedirect.com/science/article/pii/S0360132321003127},
author = {Elena {Catto Lucchino} and Adrienn Gelesz and Kristian Skeie and Giovanni Gennaro and András Reith and Valentina Serra and Francesco Goia},
keywords = {Inter-software comparison, Model validation, EnergyPlus, TRNSYS, IDA ICE, IES VE},
abstract = {Double skin façades (DSFs) have been proposed as responsive building systems to improve the building envelope’s performance. Reliable simulation of DSF performance is a prerequisite to support the design and implementation of these systems in real buildings. Building energy simulation (BES) tools are commonly used by practitioners to predict the whole building energy performance, but the simulation of the thermophysical behaviour of DSFs may be challenging when carried out through BES tools. Using an exhaust-air façade case study, we analyse and assess the reliability of four popular BES tools when these are used to simulate a DSF, either through available in-built models or through custom-built representations based on zonal models. We carry out this study by comparing numerical simulations and experimental data for a series of significant thermophysical quantities, and we reflect on the performance and limitations of the different tools. The results show that no tool is outstandingly better performing over the others, but some tools offer better predictions when the focus is placed on certain thermophysical quantities, while others should be chosen if the focus is on different ones. After comparing the different models’ limitations and challenges, we conclude that BES tools can simulate the performance of DSF systems over long periods. However, their use alone is not recommended when the simulation’s scope is to replicate and study short-term phenomena and dynamic aspects, such as sizing the building’s HVAC system.}
}
@article{MASHHADIMOGHADDAM2020221,
title = {Embedding individualized machine learning prediction models for energy efficient VM consolidation within Cloud data centers},
journal = {Future Generation Computer Systems},
volume = {106},
pages = {221-233},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19308969},
author = {Seyedhamid {Mashhadi Moghaddam} and Michael O’Sullivan and Cameron Walker and Sareh {Fotuhi Piraghaj} and Charles Peter Unsworth},
keywords = {Cloud computing, Load balancing, Prediction models, Machine learning},
abstract = {The fast growth in demand for utility-based IT services has lead to the formation of large scale Cloud data centers. The electrical energy consumption of these data centers results in high operational costs and carbon dioxide emissions. Cloud data centers benefit from the use of virtualization technology to reduce their energy consumption. This technology enables a Cloud data center to allocate its physical resources (CPU, memory, hard disk, network bandwidth) on demand and balance loads between their physical hosts by live migration of Virtual Machines (VMs). However, the migration of VMs can result in Service Level Agreement Violations (SLAVs) and consequently low Quality of Service (QoS). Hence, in this paper, we propose an energy aware VM consolidation algorithm that minimizes SLAVs. Dynamic VM consolidation has three stages: a) Detecting over- and under-utilized hosts; b) Selecting one or more VMs for migration from those hosts; c) Finding destination hosts for the selected VMs. Therefore, the proposed VM consolidation algorithm contains different models for each stage. For the first stage, we developed different fine-tuned Machine Learning (ML) prediction models for individual VMs to predict the best time to trigger migrations from hosts. For the second stage, we lexicographically consider migration time and host CPU usage when selecting VMs to migrate. Finally, a new method based on the Best Fit Decreasing (BFD) algorithm was developed to select a destination host for the VMs being migrated. Our algorithm was compared to a baseline VM consolidation algorithm that used Local Regression for detecting over-utilized hosts, minimum migration time for the VM selection stage and power-aware best fit for the host selection stage. The comparison demonstrated that our VM consolidation algorithm improved energy consumption and SLAVs by 26% and 50%, respectively.}
}
@article{LEONG2017370,
title = {Cloud-point extraction of green-polymers from Cupriavidus necator lysate using thermoseparating-based aqueous two-phase extraction},
journal = {Journal of Bioscience and Bioengineering},
volume = {123},
number = {3},
pages = {370-375},
year = {2017},
issn = {1389-1723},
doi = {https://doi.org/10.1016/j.jbiosc.2016.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389172316302894},
author = {Yoong Kit Leong and John Chi-Wei Lan and Hwei-San Loh and Tau Chuan Ling and Chien Wei Ooi and Pau Loke Show},
keywords = {Aqueous two-phase system, Bioseparation, Polyhydroxyalkanoate, Purification, Thermoseparating polymer},
abstract = {Polyhydroxyalkanoates (PHAs), a class of renewable and biodegradable green polymers, have gained attraction as a potential substitute for the conventional plastics due to the increasing concern towards environmental pollution as well as the rapidly depleting petroleum reserve. Nevertheless, the high cost of downstream processing of PHA has been a bottleneck for the wide adoption of PHAs. Among the options of PHAs recovery techniques, aqueous two-phase extraction (ATPE) outshines the others by having the advantages of providing a mild environment for bioseparation, being green and non-toxic, the capability to handle a large operating volume and easily scaled-up. Utilizing unique properties of thermo-responsive polymer which has decreasing solubility in its aqueous solution as the temperature rises, cloud point extraction (CPE) is an ATPE technique that allows its phase-forming component to be recycled and reused. A thorough literature review has shown that this is the first time isolation and recovery of PHAs from Cupriavidus necator H16 via CPE was reported. The optimum condition for PHAs extraction (recovery yield of 94.8% and purification factor of 1.42 fold) was achieved under the conditions of 20 wt/wt % ethylene oxide-propylene oxide (EOPO) with molecular weight of 3900 g/mol and 10 mM of sodium chloride addition at thermoseparating temperature of 60°C with crude feedstock limit of 37.5 wt/wt %. Recycling and reutilization of EOPO 3900 can be done at least twice with satisfying yield and PF. CPE has been demonstrated as an effective technique for the extraction of PHAs from microbial crude culture.}
}
@article{ABOHAMAMA2020113306,
title = {A hybrid energy–Aware virtual machine placement algorithm for cloud environments},
journal = {Expert Systems with Applications},
volume = {150},
pages = {113306},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113306},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420301317},
author = {A.S. Abohamama and Eslam Hamouda},
keywords = {Cloud computing, Server consolidation, Virtual machine placement, Permutation-based optimization},
abstract = {The high energy consumption of cloud data centers presents a significant challenge from both economic and environmental perspectives. Server consolidation using virtualization technology is widely used to reduce the energy consumption rates of data centers. Efficient Virtual Machine Placement (VMP) plays an important role in server consolidation technology. VMP is an NP-hard problem for which optimal solutions are not possible, even for small-scale data centers. In this paper, a hybrid VMP algorithm is proposed based on another proposed improved permutation-based genetic algorithm and multidimensional resource-aware best fit allocation strategy. The proposed VMP algorithm aims to improve the energy consumption rate of cloud data centers through minimizing the number of active servers that host Virtual Machines (VMs). Additionally, the proposed VMP algorithm attempts to achieve balanced usage of the multidimensional resources (CPU, RAM, and Bandwidth) of active servers, which in turn, reduces resource wastage. The performance of both proposed algorithms are validated through intensive experiments. The obtained results show that the proposed improved permutation-based genetic algorithm outperforms several other permutation-based algorithms on two classical problems (the Traveling Salesman Problem and the Flow Shop Scheduling Problem) using various standard datasets. Additionally, this study shows that the proposed hybrid VMP algorithm has promising energy saving and resource wastage performance compared to other heuristics and metaheuristics. Moreover, this study reveals that the proposed VMP algorithm achieves a balanced usage of the multidimensional resources of active servers while others cannot.}
}
@article{BEYAHMEDKHERNACHE2021102004,
title = {HEVC hardware vs software decoding: An objective energy consumption analysis and comparison},
journal = {Journal of Systems Architecture},
volume = {115},
pages = {102004},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102004},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000199},
author = {Mohammed {Bey Ahmed Khernache} and Yahia Benmoussa and Jalil Boukhobza and Daniel Menard},
keywords = {Power consumption, Mobile platform, HEVC, Software video decoding, Hardware video decoding, Heterogeneous architecture},
abstract = {Web data are experiencing a proliferation of video content for mobile platforms. This is accompanied by new advances in heterogeneous general purpose processor (GPP) cores embedded in mobile devices which offer a great opportunity to enhance both performance and energy efficiency of software (SW) video decoding. On the other hand, hardware (HW) video accelerators are more energy-efficient but are not flexible and their time-to-market is significant. In this context, this paper proposes a characterization methodology to investigate the performance and power consumption of two video decoding approaches on mobile platforms. The first one uses a HW decoder intellectual property (HDIP) in addition to a GPP (for the control). The second one is SW-based and uses only a heterogeneous multi-core GPP. The objective is to study the behavior of both video decoding approaches by comparing them and to understand why and in which case it is worth relying on the GPP rather than the HDIP. We also derive the optimal GPP configuration (number of cores and their frequency) that minimizes the energy consumption for a given video bit-stream on a given platform. The proposed methodology was applied on the HEVC video codec standard. In some state-of-the-art work figures, the SW video decoding consumes up to 1000× more energy than HDIPs. Our results show that, for video resolutions of 1080p and lower and at the operating system perspective point of view, the HEVC SW decoding consumes on average less than 4× more energy (mJ/Frame) than the HW one. Then, the more we scale up the resolution, the more we get the advantage of using the HW video decoding. Furthermore, the HEVC HW and SW decoders consume effectively less than 30% and 50% of the global power consumption of the tested platforms, respectively.}
}
@article{SINGH2022320,
title = {Role of Dual Energy Computed Tomography in Inflammatory Bowel Disease},
journal = {Seminars in Ultrasound, CT and MRI},
volume = {43},
number = {4},
pages = {320-332},
year = {2022},
note = {Dual source CT: Applications, Technology},
issn = {0887-2171},
doi = {https://doi.org/10.1053/j.sult.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0887217122000361},
author = {Ramandeep Singh and Rubal Rai and Nayla Mroueh and Avinash Kambadakone},
abstract = {Dual-energy computed tomography (DECT), which allows material-based differential X-ray absorption behavior from near simultaneously acquired low- and high-kilovolt datasets is finding increasing applications in the evaluation of bowel diseases. In patients with inflammatory bowel disease, DECT techniques permit both qualitative and quantitative assessment. Particularly in patients with Crohn's disease, monoenergetic and iodine specific images have been explored. This article focuses on the principles and applications of DECT in inflammatory bowel disease along with review of its limitations and challenges.}
}
@article{XING2016191,
title = {Development of a cloud-based platform for footprint assessment in green supply chain management},
journal = {Journal of Cleaner Production},
volume = {139},
pages = {191-203},
year = {2016},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616311672},
author = {Ke Xing and Wei Qian and Atiq Uz Zaman},
keywords = {Green supply chain management, Life-cycle assessment, Cloud computing, Organic cotton},
abstract = {Managing life-cycle information presents a critical challenge for footprint assessment and performance measurement in supply chains. Extant literature and supply chain collaboration models fall short in providing an interactive platform to enable cross-organisational life-cycle information gathering, sharing and management. This paper proposes a cloud-based life-cycle assessment (LCA) platform that enables dynamic life-cycle data collection and exchange, and supports supply chain collaboration for environmental footprint assessment. Using green supply chain management of cotton T-shirts as an example, the paper further illustrates the potential of the proposed cloud-based model in helping supply chain stakeholders to address the implications of managing life-cycle information and to improve the timeliness of their carbon footprint assessment.}
}
@article{CHANDIO2019100352,
title = {Energy efficient VM scheduling strategies for HPC workloads in cloud data centers},
journal = {Sustainable Computing: Informatics and Systems},
volume = {24},
pages = {100352},
year = {2019},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2019.100352},
url = {https://www.sciencedirect.com/science/article/pii/S2210537916301718},
author = {Aftab Ahmed Chandio and Nikos Tziritas and Muhammad Saleem Chandio and Cheng-Zhong Xu},
keywords = {Energy efficiency, HPC cloud, VM scheduling, Power management schemes, DVFS, Hypervisor, Cloud computing},
abstract = {In virtualized environments, virtual machine (VM) scheduling strategies incorporated with energy efficient techniques are needed to reduce the operational cost of the system while delivering high Quality of Service (QoS). It is widely accepted that the cost of the energy consumption in the environment is a dominant part of the owner’s budget. However, when considering energy efficiency, VM scheduling decisions become more constrained, leading in the violation of job deadlines and hence compromising QoS. This paper studies energy efficient VM scheduling strategies in virtualized environments to minimize the queue time and makespan under the fulfillment of SLA requirements (i.e., deadline). Specifically, six energy efficient VM scheduling strategies are investigated incorporated with the dynamic voltage and frequency scaling (DVFS) power management technique. They consist of user-oriented and system-oriented policies. The strategies are extensively simulated and compared with three power management governing methods provided at hypervisor level (i.e., userspec, ondemand, and performance). To conduct simulation experiments, we employ real-world high performance computing (HPC) workloads collected from a production data center. For comparison and evaluation, we analyze the: (a) energy consumption, (b) runtime, (c) queue time, (d) makespan, and (e) slowdown ratio. Lastly, we highlight the strengths and weaknesses of the VM scheduling strategies that can help to choose the most appropriate VM scheduling strategy for a given scenario.}
}
@article{NDUWAYEZU2022108834,
title = {Latency and energy aware rate maximization in MC-NOMA-based multi-access edge computing: A two-stage deep reinforcement learning approach},
journal = {Computer Networks},
volume = {207},
pages = {108834},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.108834},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622000500},
author = {Maurice Nduwayezu and Ji-Hoon Yun},
keywords = {Mobile edge computing, Multicarrier non-orthogonal multiple access, Resource block assignment, Deep reinforcement learning},
abstract = {Future network services are emerging with an inevitable need for high wireless capacity along with strong computational capabilities, stringent latency and reduced energy consumption Two technologies are promising, showing potential to support these requirements: multi-access (or mobile) edge computing (MEC) and non-orthogonal multiple access (NOMA). While MEC allows users to access the abundant computing resources at the edge of the network, NOMA technology enables an increase in the density of a cellular network. However, integrating NOMA technology into MEC systems faces challenges in terms of joint offloading decisions (remote or local computation) and inter-user interference management. In this paper, with the objective of maximizing the system-wide sum computation rate under latency and energy consumption constraints, we propose a two-stage deep reinforcement learning algorithm to solve the joint problem in a multicarrier NOMA-based MEC system, in which the first-stage agent handles offloading decisions while the second-stage agent considers the offloading decisions to determine the resource block assignments for users. Simulation results show that compared with other benchmark algorithms, the proposed algorithm improves the sum computation rate while meeting the latency and energy consumption requirements, and it outperforms the approach in which a single agent handles both offloading decisions and resource block assignments due to faster convergence performance.}
}
@article{JIANG201790,
title = {Fault-tolerant system design on cloud logistics by greener standbys deployment with Petri net model},
journal = {Neurocomputing},
volume = {256},
pages = {90-100},
year = {2017},
note = {Fuzzy Neuro Theory and Technologies for Cloud Computing},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.08.134},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217304149},
author = {Fuu-Cheng Jiang and Ching-Hsien Hsu},
keywords = {Cloud computing, Fault-tolerant system, Petri nets, Cost optimization},
abstract = {The cost-aware exploration on enhancing fault-tolerant becomes an important issue of service quality from cloud platform. To approach this goal with greener design, a novel server backup strategy is adopted with two types of standby server with warm standby and cold standby configurations. On such two-level standby scheme, cost elaboration has been explored in terms of deployment ratio between warm standbys and cold standbys. The cold standbys provide a greener power solution than those of conventional warm standbys. The optimal cost policy has been proposed to maintain regulated quality of service for the cloud customers. On qualitative study, a Petri net is developed and designed to visualize the whole system operational flow. On quantitative research for decision support, the theory of finite source queue is elaborated and relevant comprehensive mathematical analysis on cost pattern has been made in detail. Relevant simulations have been conducted to validate the proposed cost optimization model as well. On green contribution, the saving of power consumption has been estimated on the basis of switching warm standbys into cold standbys, which amounts for the reduction of CO2 emission. Hence the proposed approach indeed provides a feasibly standby architecture to meet cloud logistic economy with greener deployment.}
}
@article{AZIZI2022103333,
title = {Deadline-aware and energy-efficient IoT task scheduling in fog computing systems: A semi-greedy approach},
journal = {Journal of Network and Computer Applications},
volume = {201},
pages = {103333},
year = {2022},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2022.103333},
url = {https://www.sciencedirect.com/science/article/pii/S1084804522000029},
author = {Sadoon Azizi and Mohammad Shojafar and Jemal Abawajy and Rajkumar Buyya},
keywords = {Internet of Things, Fog computing, Cloud computing, Task scheduling, Semi-greedy algorithm, Deadline-aware, Energy consumption},
abstract = {With the rapid advancement of Internet of Things (IoT) devices, a variety of IoT applications that require a real-time response and low latency have emerged. Fog computing has become a viable platform for processing emerging IoT applications. However, fog computing devices tend to be highly distributed, dynamic, and resource-constrained, so deploying fog computing resources effectively for executing heterogeneous and delay-sensitive IoT tasks is a fundamental challenge. In this paper, we mathematically formulate the task scheduling problem to minimize the total energy consumption of fog nodes (FNs) while meeting the quality of service (QoS) requirements of IoT tasks. We also consider the minimization of the deadline violation time in our model. Next, we propose two semi-greedy based algorithms, namely priority-aware semi-greedy (PSG) and PSG with multistart procedure (PSG-M), to efficiently map IoT tasks to FNs. We evaluate the performance of the proposed task scheduling approaches with respect to the percentage of IoT tasks that meet their deadline requirement, total energy consumption, total deadline violation time, and the system’s makespan. Compared with existing algorithms, the experiment results confirm that the proposed algorithms improve the percentage of tasks meeting their deadline requirement up to 1.35x and decrease the total deadline violation time up to 97.6% compared to the second-best results, respectively, while the energy consumption of fog resources and makespan of the system are optimized.}
}
@article{DOROTA2020102450,
title = {An analysis method for data taken by Imaging Air Cherenkov Telescopes at very high energies under the presence of clouds},
journal = {Astroparticle Physics},
volume = {120},
pages = {102450},
year = {2020},
issn = {0927-6505},
doi = {https://doi.org/10.1016/j.astropartphys.2020.102450},
url = {https://www.sciencedirect.com/science/article/pii/S0927650520300232},
author = {Sobczyńska Dorota and Adamczyk Katarzyna and Sitarek Julian and Szanecki Michał},
keywords = {-Rays, General – Methods, Observational – Instrumentation, Detectors – Telescopes},
abstract = {The effective observation time of Imaging Air Cherenkov Telescopes (IACTs) plays an important role in the detection of γ-ray sources, especially when the expected flux is low. This time is strongly limited by the atmospheric conditions. Significant extinction of Cherenkov light caused by the presence of clouds reduces the photon detection rate and also complicates or even makes impossible proper data analysis. However, for clouds with relatively high atmospheric transmission, high energy showers can still produce enough Cherenkov photons to allow their detection by IACTs. In this paper, we study the degradation of the detection capability of an array of small-sized telescopes for different cloud transmissions. We show the expected changes of the energy bias, energy and angular resolution and the effective collection area caused by absorption layers located at 2.5 and 4.5 km above the observation level. We demonstrate simple correction methods for reconstructed energy and effective collection area. As a result, the source flux that is observed during the presence of clouds is determined with a systematic error of  ≲ 20%. Finally, we show that the proposed correction method can be used for clouds at altitudes higher than 5 km a.s.l. As a result, the analysis of data taken under certain cloudy conditions will not require additional time-consuming Monte Carlo simulations.}
}
@article{BORYLO20201,
title = {Latency and energy-aware provisioning of network slices in cloud networks},
journal = {Computer Communications},
volume = {157},
pages = {1-19},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.03.050},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419313179},
author = {Piotr Borylo and Massimo Tornatore and Piotr Jaglarz and Nashid Shahriar and Piotr Chołda and Raouf Boutaba},
keywords = {5G network slicing, Distributed computing, Multi-objective optimization, VNF placement},
abstract = {Modern network services are constantly increasing their requirements in terms of bandwidth, latency and cost efficiency. To satisfy these requirements, the concept of network slicing has been introduced in the context of next-generation 5G networks. However, to successfully provision resources to slices, a complex optimization problem must be addressed to allocate resources over a cloud network, i.e., a distributed computing infrastructure interconnected through high-capacity network links. In this study, we propose two new latency and energy-aware optimization models for provisioning 5G slices in cloud networks comprising both distributed computing and network resources. The proposed approaches differ from other existing solutions since we conduct our studies with respect to the end-to-end latency. Relevant models of latency and energy consumption are proposed based on a comprehensive review of the state-of-the-art. To effectively solve those optimization problems, a configurable heuristic is also proposed and investigated over different network topologies. Performance of the proposed heuristic is compared against near-optimal solutions. Moreover, we assess the importance of matching between resource provisioning algorithms and architectural assumptions related to 5G network slices and a proper problem modeling.}
}
@article{YAN2020102194,
title = {A simplified prediction model for energy use of air conditioner in residential buildings based on monitoring data from the cloud platform},
journal = {Sustainable Cities and Society},
volume = {60},
pages = {102194},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102194},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720301815},
author = {Lu Yan and Meng Liu},
keywords = {Residential building, Air conditioner, Energy use prediction, Ensemble learning, Feature selection, Energy management},
abstract = {The energy use prediction of residential buildings has an increasingly important role in urban energy management. This study proposed a prediction model for the cooling energy use of air conditioners in residential buildings. Large-scale monitoring data of the operation of 1325 air conditioners in Chongqing were collected from the networking cloud platform of an air conditioner manufacturer, including setting parameters by occupants, indoor environmental parameters, time parameters and energy use parameters. The historical monitoring data of previous week before the forecast day, the meteorological data of the forecast day and the apparatus parameters of AC were employed as the original data set in this study. Feature selection engineering, including correlation analysis, importance analysis and collinearity analysis, were performed in sequence to select the most correlated and important input features for energy use prediction. Afterwards, prediction models that use four ensemble learning methods and two single learning methods were developed and compared by evaluation metrics. The best model for prediction was proposed. The results show that eleven input features have a great relationship to the daily cooling energy use and were considered the inputs to the prediction model. The XGBoost model was chosen as the best model in this study. The proposed prediction model can help researchers understand which historical features are important for the future daily cooling energy use prediction of AC. This prediction model can provide some references for different groups to implement energy management for residential buildings.}
}
@article{HAN2020101784,
title = {Adaptability assessment method of energy storage working conditions based on cloud decision fusion under scenarios of peak shaving and frequency regulation},
journal = {Journal of Energy Storage},
volume = {32},
pages = {101784},
year = {2020},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2020.101784},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X20316212},
author = {Xiaojuan Han and Zixuan Wei and Zhenpeng Hong and Dengxiang Liang},
keywords = {Energy storage working conditions, Peak shaving and frequency regulation, Decision fusion, Cloud model, Analytic hierarchy process},
abstract = {Energy storage participating in grid auxiliary services can effectively enhance the regulation capacity of the grid and promote the consumption of renewable energy, and the selection type of energy storage systems is the basis to ensure its safe and economic operation. Starting from the economics and safety of energy storage systems, an adaptive evaluation method of energy storage working conditions based on the cloud decision fusion is proposed. Aiming at strong subjective characteristics of the analytic hierarchy, an adaptability assessment model of energy storage working conditions based on the entropy weight-analysis hierarchy process method is established to obtain the scores of different types of energy storage systems. Aiming at the characteristics of ambiguity and randomness in decision-making indicators, an adaptability assessment model of energy storage working conditions based on the entropy weight-cloud model is established to obtain the scores of different types of energy storage systems. The results of the two scores are fused using Dempster-Shafer evidence theory to get the evaluation result of the best energy storage condition adaptability. In the application scenarios of the peak shaving and frequency regulation, the effectiveness of the proposed method is verified by simulation analysis of performance indicators of the peak shaving and frequency regulation. The simulation results show that the iron phosphate battery has the highest adaptability to work conditions of the peak shaving and frequency regulation, and the Dempster–Shafer evidence theory can eliminate the randomness and qualitative-quantitative doping of decision indicators on the selection type of energy storage systems, which can provide a theoretical basis for the planning of energy storage stations.}
}
@article{MACK1999S37,
title = {The use of head computed tomography in elderly patients sustaining minor head trauma},
journal = {Annals of Emergency Medicine},
volume = {34},
number = {4, Part 2},
pages = {S37},
year = {1999},
note = {Acep Research Forum},
issn = {0196-0644},
doi = {https://doi.org/10.1016/S0196-0644(99)80224-8},
url = {https://www.sciencedirect.com/science/article/pii/S0196064499802248},
author = {LR Mack and TM Hogan and JC Silva}
}
@article{ZHI201726,
title = {Delay-aware downlink beamforming with discrete rate adaptation for green cloud radio access network},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {24},
number = {1},
pages = {26-34},
year = {2017},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(17)60184-5},
url = {https://www.sciencedirect.com/science/article/pii/S1005888517601845},
author = {Yu Zhi and Wang Ke and Ji Hong},
keywords = {C-RAN, delay-aware downlink beamforming, discrete rate adaptation, green communication, mixed integer second-order cone programming},
abstract = {With the popularity of variety delay-sensitive services, how to guarantee the delay requirements for mobile users (MUs) is a great challenge for downlink beamformer design in green cloud radio access networks (C-RANs). In this paper, we consider the problem of the delay-aware downlink beamforming with discrete rate adaptation to minimize the power consumption of C-RANs. We address the problem via a mixed integer nonlinear program (MINLP), and then reformulate the MINLP problem as a mixed integer second-order cone program (MI-SOCP), which is a convex program when the integer variables are relaxed as continuous ones. Based on this formulation, a deflation algorithm, whose computational complexity is polynomial, is proposed to derive the suboptimal solution. The simulation results are presented to validate the effectiveness of our proposed algorithm.}
}
@article{WEI2020112005,
title = {Cloud detection for Landsat imagery by combining the random forest and superpixels extracted via energy-driven sampling segmentation approaches},
journal = {Remote Sensing of Environment},
volume = {248},
pages = {112005},
year = {2020},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2020.112005},
url = {https://www.sciencedirect.com/science/article/pii/S0034425720303758},
author = {Jing Wei and Wei Huang and Zhanqing Li and Lin Sun and Xiaolin Zhu and Qiangqiang Yuan and Lei Liu and Maureen Cribb},
keywords = {Landsat, Cloud detection, RFmask, Random forest, SEEDS, Superpixel segmentation},
abstract = {A primary challenge in cloud detection is associated with highly mixed scenes that are filled with broken and thin clouds over inhomogeneous land. To tackle this challenge, we developed a new algorithm called the Random-Forest-based cloud mask (RFmask), which can improve the accuracy of cloud identification from Landsat Thematic Mapper (TM), Enhanced Thematic Mapper Plus (ETM+), and Operational Land Imager and Thermal Infrared Sensor (OLI/TIRS) images. For the development and validation of the algorithm, we first chose the stratified sampling method to pre-select cloudy and clear-sky pixels to form a prior-pixel database according to the land use cover around the world. Next, we select typical spectral channels and calculate spectral indices based on the spectral reflection characteristics of different land cover types using the top-of-atmosphere reflectance and brightness temperature. These are then used as inputs to the RF model for training and establishing a preliminary cloud detection model. Finally, the Super-pixels Extracted via Energy-Driven Sampling (SEEDS) segmentation approach is applied to re-process the preliminary classification results in order to obtain the final cloud detection results. The RFmask detection results are evaluated against the globally distributed United States Geological Survey (USGS) cloud-cover assessment validation products. The average overall accuracy for RFmask cloud detection reaches 93.8% (Kappa coefficient = 0.77) with an omission error of 12.0% and a commission error of 7.4%. The RFmask algorithm is able to identify broken and thin clouds over both dark and bright surfaces. The new model generally outperforms other methods that are compared here, especially over these challenging scenes. The RFmask algorithm is not only accurate but also computationally efficient. It is potentially useful for a variety of applications in using Landsat data, especially for monitoring land cover and land-use changes.}
}
@article{LI202215,
title = {Energy-latency tradeoffs for edge caching and dynamic service migration based on DQN in mobile edge computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {166},
pages = {15-31},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522000569},
author = {Chunlin Li and Yong Zhang and Xiang Gao and Youlong Luo},
keywords = {Edge caching, Energy-latency tradeoffs, Dynamic service migration, Deep Q-Network},
abstract = {Mobile edge computing sinks computing and storage capabilities to the edge of the network to provide reliable and low-latency services. However, the mobility of users and the limited coverage of edge servers can cause service interruptions and reduce service quality. A cooperative edge caching strategy based on energy-latency balance is proposed to solve high power consumption and latency caused by processing computationally intensive applications. In the cache selection phase, the request prediction method based on a deep neural network improves the cache hit rate. In the cache placement stage, the objective function is established by comprehensively considering power consumption and latency, and We use the branch-and-bound algorithm to get the optimal value. We propose an improved service migration method to solve the problem of service interruption caused by user movement. The service migration problem is modeled using a Markov decision process (MDP). The optimization goal is to reduce service latency and improve user experience under the premise of specified cost and computing resources. Finally, the optimal solution of the model is solved by the deep Q-Network (DQN) algorithm. Experiments show that our edge caching algorithm has lower latency and energy consumption than other algorithms in the same conditions. The service migration algorithm proposed in this paper is superior to different service migration algorithms in migration cost and success rate.}
}
@article{ANZALONE2019116,
title = {Measurements of High Energy Cosmic Rays and Cloud presence: A method to estimate Cloud Coverage in Space and Ground-Based Infrared Images},
journal = {Nuclear and Particle Physics Proceedings},
volume = {306-308},
pages = {116-123},
year = {2019},
note = {CRIS 2018 "Entering the Era of Multi-Messenger Astronomy"},
issn = {2405-6014},
doi = {https://doi.org/10.1016/j.nuclphysbps.2019.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S2405601419300999},
author = {A. Anzalone and A. Bruno and F. Isgrò},
keywords = {Cloud Detection, Atmosphere Monitoring, Infrared Imaging},
abstract = {Several projects and already-operative observatories aimed at detecting High Energy Cosmic Rays (HECR) are/will be equipped with instruments to monitor the atmosphere. Since cloud presence can affect the night-time indirect measurements of the HECRs and Cherenkov radiation, it is crucial to know the meteorological conditions during the observation period of the HECR detectors. Several meteorological satellites already provide useful information, however to obtain accurate reconstructions of the detected events it is more suitable using devices that operate synchronously with the main detector. To this purpose, infrared cameras that acquire images of the whole field of view are thought to support the atmosphere monitoring during observations from both space and ground. Meaningful parameters, like cloud coverage and cloud top/bottom height, can be retrieved from the analysis of those data. Multi-spectral information are typically analyzed and combined to obtain cloud masks for each image, where a cloudy/cloud-free probability flag is associated with each pixel. These algorithms normally use several spectral bands that are not always available in non-meteorological sensors. A different approach is presented in this paper. It only relies on the gray level values of the image pixel, and it can be applied on thermal infrared as well as visible images acquired from both space and ground. To test the method on real cloudy scenes, images from polar satellite and all-sky data archives are considered, and the results are compared to the corresponding cloudiness masks provided by the same data repositories.}
}
@article{HSIEH202099,
title = {Utilization-prediction-aware virtual machine consolidation approach for energy-efficient cloud data centers},
journal = {Journal of Parallel and Distributed Computing},
volume = {139},
pages = {99-109},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S074373151930190X},
author = {Sun-Yuan Hsieh and Cheng-Sheng Liu and Rajkumar Buyya and Albert Y. Zomaya},
keywords = {Cloud computing, Cloud data centers, Utilization prediction model, Dynamic virtual machine (VM) consolidation},
abstract = {In the age of the information explosion, the energy demand for cloud data centers has increased markedly; hence, reducing the energy consumption of cloud data centers is essential. Dynamic virtual machine VM consolidation, as one of the effective methods for reducing energy energy consumption is extensively employed in large cloud data centers. It achieves the energy reductions by concentrating the workload of active hosts and switching idle hosts into low-power state; moreover, it improves the resource utilization of cloud data centers. However, the quality of service (QoS) guarantee is fundamental for maintaining dependable services between cloud providers and their customers in the cloud environment. Therefore, reducing the power costs while preserving the QoS guarantee are considered as the two main goals of this study. To efficiently address this problem, the proposed VM consolidation approach considers the current and future utilization of resources through the host overload detection (UP-POD) and host underload detection (UP-PUD). The future utilization of resources is accurately predicted using a Gray-Markov-based model. In the experiment, the proposed approach is applied for real-world workload traces in CloudSim and were compared with the existing benchmark algorithms. Simulation results show that the proposed approaches significantly reduce the number of VM migrations and energy consumption while maintaining the QoS guarantee.}
}
@article{MAHENGE20221048,
title = {Energy-efficient task offloading strategy in mobile edge computing for resource-intensive mobile applications},
journal = {Digital Communications and Networks},
volume = {8},
number = {6},
pages = {1048-1058},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822000505},
author = {Michael Pendo John Mahenge and Chunlin Li and Camilius A. Sanga},
keywords = {Mobile edge computing, Quality of experience, Task offloading, Communication networks, Particle swarm optimization},
abstract = {Mobile Edge Computing (MEC) has been considered a promising solution that can address capacity and performance challenges in legacy systems such as Mobile Cloud Computing (MCC). In particular, such challenges include intolerable delay, congestion in the core network, insufficient Quality of Experience (QoE), high cost of resource utility, such as energy and bandwidth. The aforementioned challenges originate from limited resources in mobile devices, the multi-hop connection between end-users and the cloud, high pressure from computation-intensive and delay-critical applications. Considering the limited resource setting at the MEC, improving the efficiency of task offloading in terms of both energy and delay in MEC applications is an important and urgent problem to be solved. In this paper, the key objective is to propose a task offloading scheme that minimizes the overall energy consumption along with satisfying capacity and delay requirements. Thus, we propose a MEC-assisted energy-efficient task offloading scheme that leverages the cooperative MEC framework. To achieve energy efficiency, we propose a novel hybrid approach established based on Particle Swarm Optimization (PSO) and Grey Wolf Optimizer (GWO) to solve the optimization problem. The proposed approach considers efficient resource allocation such as sub-carriers, power, and bandwidth for offloading to guarantee minimum energy consumption. The simulation results demonstrate that the proposed strategy is computational-efficient compared to benchmark methods. Moreover, it improves energy utilization, energy gain, response delay, and offloading utility.}
}
@article{AVDEEVA2021108600,
title = {Results of measurements of the cardiac micropotential energies in the amplitude-time intervals recorded by the nanosensor-based hardware and software complex},
journal = {Measurement},
volume = {173},
pages = {108600},
year = {2021},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2020.108600},
url = {https://www.sciencedirect.com/science/article/pii/S0263224120311192},
author = {Diana K. Avdeeva and Ivan V. Maksimov and Maxim L. Ivanov and Mikhail M. Yuzhakov and Nikita V. Turushev and Sergey A. Rybalka and Roman E. Batalov and Wenjia Guo and Elena B. Filippova},
keywords = {Atrial fibrillation, Nanosensor, Micropotential, Analysis, Interval, Energy, Dynamics, Sinus rhythm, Non-invasive, Broadband, High resolution},
abstract = {The method for automatic analysis of micropotentials over the entire ECG recording has been developed and the results of measuring the energies of micropotentials in various amplitude-time intervals have been analyzed to assess the diagnostic value of the developed method. The paper presents the results of the study of micropotentials in a patient with atrial fibrillation of 4-year duration recorded by the nanosensor-based hardware and software complex in frequency range from 0 to 10 kHz, with a level of higher than 1 μV and a sampling rate of 64 kHz, whose sinus rhythm restored. The most significant decrease in the energy of micropotentials by an order of magnitude or more was observed in the range of amplitudes (5.1–20.0) μV. The method can be employed for individual use and in hospitals for early detection of asymptomatic ischemia that often leads to sudden cardiac death.}
}
@article{LU2019995,
title = {Green supplier selection in straw biomass industry based on cloud model and possibility degree},
journal = {Journal of Cleaner Production},
volume = {209},
pages = {995-1005},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.10.130},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618331469},
author = {Zhiming Lu and Xiaokun Sun and Yaxian Wang and Chuanbo Xu},
keywords = {Green supplier selection, Straw biomass industry, Multi-criteria decision making (MCDM), Cloud model, Possibility degree},
abstract = {Abstract:
Selecting the supplier is a very critical part to the comprehensive performance of modern enterprises. With the large-scale development of straw biomass industry, green supplier selection has become a key decision-making task which needs to collect and process mass of information, it is necessary to make the supplier to go green. Nevertheless, it is safe to say, in fact, that so far there are a few researches on supplier selection in straw biomass industry. At the same time, some crucial problems urgently needed to be solved exist in present research, as shown below:①decision-making methods are frequently used, even though they have some serious problems, e.g. information loss.②some key index factors are ignored, green supplier selection hasn't got attracted sufficient attention.③little work has been performed on the fuzziness or the uncertainty of index weight. To solve the problems mentioned above, this paper tries to find out some ways to make improvements:①Cloud model is proposed to handle the fuzziness and randomness of evaluation information.②Index system is more comprehensive. Important issues, such as on-time delivery, pollutant emissions per unit fuel(t) and green certification, are paid more attention to.③Fuzzy AHP is applied to determine the index weight, the uncertainty of criteria and sub-criteria are considered, the vagueness of human thought is dealt with.④A decision framework based on Cloud model and possibility degree is put forward to guide the optimal selection of green supplier selection.⑤A Chinese case is carried out and related sensitivity analysis is performed. The results show that the proposed novel model not only can find the more suitable green supplier, but also reveal the big gap between alternatives clearly. The strength of the proposed new decision framework is proved.}
}
@article{IBRAHIM202077,
title = {An energy efficient service composition mechanism using a hybrid meta-heuristic algorithm in a mobile cloud environment},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {77-87},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520302744},
author = {Godar J. Ibrahim and Tarik A. Rashid and Mobayode O. Akinsolu},
keywords = {Mobile cloud computing, Service composition, Energy consumption, Meta-heuristic algorithm},
abstract = {By increasing mobile devices in technology and human life, using a runtime and mobile services has gotten more complex along with the composition of a large number of atomic services. Different services are provided by mobile cloud components to represent the non-functional properties as Quality of Service (QoS), which is applied by a set of standards. On the other hand, the growth of the energy-source heterogeneity in mobile clouds is an emerging challenge according to the energy saving problem in mobile nodes. In order to mobile cloud service composition as an NP-Hard problem, an efficient selection method should be taken by problem using optimal energy-aware methods that can extend the deployment and interoperability of mobile cloud components. Also, an energy-aware service composition mechanism is required to preserve high energy saving scenarios for mobile cloud components. In this paper, an energy-aware mechanism is applied to optimize mobile cloud service composition using a hybrid Shuffled Frog Leaping Algorithm and Genetic Algorithm (SFGA). Experimental results capture that the proposed mechanism improves the feasibility of the service composition with minimum energy consumption, response time, and cost for mobile cloud components against some current algorithms.}
}
@article{LIU2022196,
title = {Use of dual-energy computed tomography to detect mycoplasma-related pulmonary embolism and splenic infarction},
journal = {Pediatrics & Neonatology},
volume = {63},
number = {2},
pages = {196-197},
year = {2022},
issn = {1875-9572},
doi = {https://doi.org/10.1016/j.pedneo.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1875957221002035},
author = {Yi-Ching Liu and Zen-Kong Dai and Jong-Hau Hsu and Jui-Sheng Hsu and I-Chen Chen}
}
@article{TRAN2022280,
title = {Clinical applications of dual-energy computed tomography in neuroradiology},
journal = {Seminars in Ultrasound, CT and MRI},
volume = {43},
number = {4},
pages = {280-292},
year = {2022},
note = {Dual source CT: Applications, Technology},
issn = {0887-2171},
doi = {https://doi.org/10.1053/j.sult.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0887217122000300},
author = {Ngoc-Anh Tran and Aaron D. Sodickson and Rajiv Gupta and Christopher A. Potter},
abstract = {Dual-energy computed tomography (DECT) has developed into a robust set of techniques with increasingly validated clinical applications in neuroradiology. We review some of the most common applications in neuroimaging along with demonstrative case examples that showcase the use of this technology in intracranial hemorrhage, stroke imaging, trauma imaging, artifact reduction, and tumor characterization.}
}
@article{PARK2022103231,
title = {X-ray computed tomography, electron microscopy, and energy-dispersive X-ray spectroscopy of severed Zelkova serrata roots (Japanese elm tree)},
journal = {Micron},
volume = {156},
pages = {103231},
year = {2022},
issn = {0968-4328},
doi = {https://doi.org/10.1016/j.micron.2022.103231},
url = {https://www.sciencedirect.com/science/article/pii/S0968432822000270},
author = {Junhyung Park and Dahye Seo and Ki Woo Kim},
keywords = {Calcium, Hyphae, Occlusion, Silicon, Xylem},
abstract = {X-ray computed tomography (XCT), field emission scanning electron microscopy (FESEM), and energy-dispersive X-ray spectroscopy (EDS) were evaluated for imaging and element identification of woody plant roots. Lateral roots of Japanese zelkova (Zelkova serrata) were severed in spring and maintained in soil for six months. The lateral roots were observed using XCT without maceration and sectioning. The general wood characteristics were discernible to reveal the bark and xylem structures in contrast-inverted tomograms. Virtual sections showed a newly formed ring of woundwood encircling the severed lateral roots. FESEM exhibited secondary xylem structures in which tyloses, fungal hyphae, and aggregates were present. While silicon was dispersed in and around the fungal hyphae, calcium was localized as distinct aggregates using EDS. These results suggest that the combined use of XCT, FESEM, and EDS has merit into the morphological assessment of tree health care, providing virtual sections, high-resolution images, and element composition from an entire woodblock.}
}
@article{STEFANOVABAHCHEVANSKA2017135,
title = {A green cloud-point extraction-chromogenic system for vanadium determination},
journal = {Journal of Molecular Liquids},
volume = {248},
pages = {135-142},
year = {2017},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2017.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167732217335894},
author = {Teodora Stefanova-Bahchevanska and Nikolina Milcheva and Serhii Zaruba and Vasil Andruch and Vassil Delchev and Kiril Simitchiev and Kiril Gavazov},
keywords = {Cloud-point extraction, Vanadium, 1-(2-thiazolylazo)-2-naphthol (TAN), Hydrogen peroxide, TDDFT calculations},
abstract = {A cloud-point extraction-chromogenic system containing vanadium(V), 1-(2-thiazolylazo)-2-naphthol (TAN), H2O2 and Triton X-100 was investigated. The optimum conditions for vanadium extraction and spectrophotometric determination were found to be: concentrations of TAN and H2O2 (5×10−5 and 3.0×10−4molL−1, respectively), mass fraction of Triton X-100 (3%), acidity (5×10−3molL−1 H2SO4), incubation time (60min for hot plate or 10min for microwave oven) and wavelength (607nm). The composition of the extracted species was determined (V: TAN: O22–=1: 1: 2), and the geometries of four possible structures, which differ by the coordination mode of O22−, were optimized at the BLYP/cc-pVDZ level of theory. The UV–Vis spectra were calculated and compared with the experimental spectra. Species with two monodentate peroxide ligands are present in the extract, along with species containing one monodentate and one bidentate peroxide. The effect of foreign ions was studied, and the analytical characteristics of the developed procedure were determined. Spectral interference of copper(II) was observed; however, it can be corrected mathematically by measuring the absorbance at 580 and 610nm. Iron(III) can be efficiently masked with HPO42−. The method was applied for analysis of various samples. Beer's law was obeyed up to 0.76μgmL−1 of vanadium. The limits of detection (LOD) and determination (LOQ) and the molar absorptivity (ε607) were 1.4ngmL−1, 4.8ngmL−1 and 8.84×104Lmol−1cm−1, respectively.}
}
@article{THAMAN2017205,
title = {Green cloud environment by using robust planning algorithm},
journal = {Egyptian Informatics Journal},
volume = {18},
number = {3},
pages = {205-214},
year = {2017},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2017.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866517300361},
author = {Jyoti Thaman and Manpreet Singh},
keywords = {Planning algorithms, Scheduling algorithms, Ready wueue, Robust, Cloud computing},
abstract = {Cloud computing provided a framework for seamless access to resources through network. Access to resources is quantified through SLA between service providers and users. Service provider tries to best exploit their resources and reduce idle times of the resources. Growing energy concerns further makes the life of service providers miserable. User’s requests are served by allocating users tasks to resources in Clouds and Grid environment through scheduling algorithms and planning algorithms. With only few Planning algorithms in existence rarely planning and scheduling algorithms are differentiated. This paper proposes a robust hybrid planning algorithm, Robust Heterogeneous-Earliest-Finish-Time (RHEFT)1Robust Heterogeneous-Earliest-Finish-Time (RHEFT).1 for binding tasks to VMs. The allocation of tasks to VMs is based on a novel task matching algorithm called Interior Scheduling. The consistent performance of proposed RHEFT algorithm is compared with Heterogeneous-Earliest-Finish-Time (HEFT)2Heterogeneous-Earliest-Finish-Time (HEFT).2 and Distributed HEFT (DHEFT)3Distributed HEFT (DHEFT).3 for various parameters like utilization ratio, makespan, Speed-up and Energy Consumption. RHEFT’s consistent performance against HEFT and DHEFT has established the robustness of the hybrid planning algorithm through rigorous simulations.}
}
@article{TAVAKOLAN2022103485,
title = {A parallel computing simulation-based multi-objective optimization framework for economic analysis of building energy retrofit: A case study in Iran},
journal = {Journal of Building Engineering},
volume = {45},
pages = {103485},
year = {2022},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.103485},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221013437},
author = {Mehdi Tavakolan and Farzad Mostafazadeh and Saeed {Jalilzadeh Eirdmousa} and Amir Safari and Kaveh Mirzaei},
keywords = {Building energy retrofit, Multi-objective optimization, Parallel computing, Energy efficiency, Genetic algorithm, Energy pricing policy},
abstract = {The building sector represents a large share of rising global energy demand. Improving energy efficiency in existing building stock is a crucial strategy. Adopting the best energy retrofitting strategy in a specific building is a challenging task due to a plethora of possible combinations of retrofit measures and mutually contrasting objective functions. In addition, peculiar conditions of Iran, such as extremely subsidized energy prices, and step utility tariffs, escalate the challenges of building energy retrofit. Accordingly, the current study presents a simulation-based multi-objective optimization framework characterized by parallel processing structure and results-saving archive. The framework is implemented by integrating MATLAB® as an optimization engine with EnergyPlus as a dynamic energy simulator to minimize primary energy consumption and discounted payback period while maximizing the net present value. The algorithm explores a vast domain of possible solutions including, building envelope, cooling and heating systems, and renewable energy sources. The framework is applied to a single-family residence located in Iran. Three different scenarios are examined with reference to prospective energy pricing policies to evaluate their effect on the attractiveness of energy retrofit projects. For each scenario, final solutions are selected from respective Pareto fronts according to cost-optimality and energy-efficiency criteria and considering budget constraints. The results indicate that even though significant reductions in primary energy consumption can be achieved, implementing energy retrofit under the current energy pricing policy in Iran would not yield economic benefits. However, the elimination of subsidies along with offering incentives for building energy retrofits presents promising outcomes.}
}
@article{DEBAKKER2022105091,
title = {Independent changes in bone mineralized and marrow soft tissues following acute knee injury require dual-energy or high-resolution computed tomography for accurate assessment of bone mineral density and stiffness},
journal = {Journal of the Mechanical Behavior of Biomedical Materials},
volume = {127},
pages = {105091},
year = {2022},
issn = {1751-6161},
doi = {https://doi.org/10.1016/j.jmbbm.2022.105091},
url = {https://www.sciencedirect.com/science/article/pii/S1751616122000182},
author = {Chantal M.J. {de Bakker} and Nikolas K. Knowles and Richard E.A. Walker and Sarah L. Manske and Steven K. Boyd},
keywords = {Dual-energy computed tomography, Bone density measurement, Finite element analysis, Bone marrow edema, Acute knee injury, Material decomposition},
abstract = {Musculoskeletal injuries often induce local accumulation of blood and/or fluid within the bone marrow, which is detected on medical imaging as edema-like marrow signal intensities (EMSI). In addition to its biological effects on post-injury recovery, the displacement of low-attenuating, largely adipocytic marrow by EMSI may introduce errors into quantitative computed tomography (QCT) measurements of bone mineral density (vBMD) and resulting bone stiffness estimates from image-based finite element (FE) analysis. We aimed to investigate the impact of post-injury changes in marrow soft tissue composition on CT-based bone measurements by applying CT imaging at multiple spatial resolutions. To do so, dual energy QCT (DECT) material decomposition was used to detect EMSI in the tibiae of nineteen participants with a recent anterior cruciate ligament tear. We then measured bone density and FE-based apparent modulus within the EMSI region and in a matched volume in the uninjured contralateral knee. Three measurement methods were applied: 1.) standard, QCT density calibration and density-based FEM; 2.) a DECT density calibration that provides density measurements adjusted for marrow soft tissues; and 3.) high-resolution peripheral QCT (HR-pQCT) density and microFE analyses. When measured using standard, single-energy QCT, vBMD and apparent modulus were elevated in the EMSI compared to the contralateral. After adjusting for marrow soft tissue composition using DECT, these measurements were no longer different between the two regions. By allowing for high-resolution, localized density analysis, HR-pQCT indicated that trabecular tissue mineral density was 9 mgHA/cm3 lower, while density of marrow soft tissues was 18 mgHA/cm3 higher, in the EMSI than the contralateral region, suggesting that EMSI have opposite effects on the measured density of trabecular bone and the underlying soft marrow. Thus, after an acute injury, altered composition of marrow soft tissues may artificially inflate overall measurements of bone density and apparent modulus obtained using standard QCT. This can be corrected by accounting for marrow soft tissue attenuation, either by using DECT-based density calibration or HR-pQCT microFE and measurements of local density of trabeculae.}
}
@article{MANCEBO2021106560,
title = {A process for analysing the energy efficiency of software},
journal = {Information and Software Technology},
volume = {134},
pages = {106560},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106560},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000446},
author = {Javier Mancebo and Félix García and Coral Calero},
keywords = {Software sustainability, Green software, Software consumption measurement, Energy consumption measurement process, Energy efficiency},
abstract = {Context
It is essential to be aware of the energy efficiency of software when it is running, so that it can be improved; to that end, energy consumption measurements need to be carried out. To ensure that these measurements are as reliable as possible, it is recommended that a well-defined process be followed.
Objective
To identify how the process for analysing the energy efficiency of software should be carried out (including the definition of the software to be evaluated, the selection of measuring instruments, the analysis and the presentation of results, etc.), in an endeavour to improve the reliability and consistency of the information obtained regarding energy efficiency.
Method
An analysis of related work was carried out, to extract some good practices in measuring energy consumption; based on our experience, a process to analyse the energy efficiency of the software has been defined.
Results
We have defined a process to analyse the energy efficiency of the software. We describe this process through a set of phases that covers all the steps needed to carry out a correct analysis of the energy consumption of the software executed. Moreover, this process was validated with two different studies using different measurement instruments (one with a hardware-based approach and one with a software-based approach) to ensure its applicability to all types of studies with software energy consumption measurement.
Conclusion
The steps to be followed to analyse the energy efficiency of the software need to be established. A new process has hence been defined to improve the reliability and consistency of the measurements. Furthermore, this process facilitates the replicability and comparison of the studies carried out.}
}
@article{HASSAN2020431,
title = {A smart energy and reliability aware scheduling algorithm for workflow execution in DVFS-enabled cloud environment},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {431-448},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19322307},
author = {Hadeer A. Hassan and Sameh A. Salem and Elsayed M. Saad},
keywords = {Energy efficient, DVFS-enabled, Green cloud computing, Task scheduling algorithm, Reliability, DAG workflow, Heterogeneous system},
abstract = {The energy consumption is one of the major concerns addressed by recent researches in the green cloud environment. As a result, to decrease the enormous increase in energy consumption, one of the most promising scheduling techniques used nowadays is the Dynamic Voltage Frequency Scaling (DVFS) technique. DVFS reduces energy consumption by lowering the processors’ frequency for virtual machines (VMs); this results in an increase in the occurrence of errors during the execution of the workflow, which decreases the reliability of the system. As a consequence, this paper addresses the DVFS problem by proposing a new Smart Energy and Reliability Aware Scheduling algorithm (SERAS) for workflow execution in the cloud environment. The SERAS approach split the target deadline of workflow across tasks. Afterward, the proposed algorithm decreases the frequency of processors for VMs using the DVFS technique without missing the tasks’ deadline. As a consequence, the SERAS algorithm allocates the tasks to the most appropriate VMs with suitable frequencies levels while guaranteeing both the reliability and the completion time requirement of green cloud systems. To vindicate the effectiveness of the SERAS algorithm in real-world applications, we carried out a series of experiments on four real workflows generated using a scientific toolkit. Also, we performed comprehensive experiments with recent researches. The results showed that the SERAS algorithm outperforms its competitors while keeping both the reliability and completion time requirements. Furthermore, the estimated time complexity and average execution time show the applicability of the SERAS algorithm compared with their competitors.}
}
@article{SHEN2021121251,
title = {Superconducting fault current limiter (SFCL): Experiment and the simulation from finite-element method (FEM) to power/energy system software},
journal = {Energy},
volume = {234},
pages = {121251},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121251},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221014997},
author = {Boyang Shen and Yu Chen and Chuanyue Li and Sheng Wang and Xiaoyuan Chen},
keywords = {Superconducting fault current limiter (SFCL), Resistive-type SFCL, High-temperature superconductor (HTS), Finite element method (FEM), Power system computer-aided design (PSCAD), Energy system},
abstract = {The superconducting fault current limiter (SFCL) has been regarded as one of most popular superconducting applications. This article reviews the modern energy system with two major issues (the power stability and fault-current), and introduces corresponding approaches to mitigate these issues, including the importance of using SFCL. Then the article presents the experiment of a resistive-type SFCL used for a power electronic circuit. The experiment well matched the advanced finite-element method (FEM) SFCL model, from which the reliability of FEM SFCL model was confirmed. Afterwards, the FEM model and the power system software PSCAD were used to model a large-scale resistive-type SFCL. Under the same simulation conditions the FEM model well matched the PSCAD model. The FEM method has the advantages of offering specific electromagnetic modeling on superconducting part. The PSCAD SFCL model has much faster simulation speed and can directly cope with all ranges of power networks. This article presents a new vision and an all-in-one study to link the experiment, the numerical model, and the power/energy system software model, and their agreement can be extremely helpful for researchers and engineers to find useful evidences and reliable methods to confidently carry out successful SFCL designs for the electrical energy system.}
}
@article{FARAG2022333,
title = {Role of Dual-energy Computed Tomography in Diagnosis of Acute Pulmonary Emboli, a Review},
journal = {Seminars in Ultrasound, CT and MRI},
volume = {43},
number = {4},
pages = {333-343},
year = {2022},
note = {Dual source CT: Applications, Technology},
issn = {0887-2171},
doi = {https://doi.org/10.1053/j.sult.2022.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0887217122000397},
author = {Ahmed Farag and Jordan Fielding and Tara Catanzano},
abstract = {Prompt diagnosis of pulmonary embolism is essential to avert morbidity and mortality. There are a number of diagnostic options for identification of a pulmonary embolism, including laboratory and imaging investigations. While computed tomography pulmonary angiography (CTPA) has largely supplanted nuclear medicine ventilation/perfusion studies, there remain significant limitations in the optimal performance of CTPA. Dual-energy computed tomography has the ability to overcome many of the limitations of standard of care CTPA, including rescue of poor contrast boluses and the ability to evaluate pulmonary perfusion defects.}
}
@article{LOPEZLOPEZ2022S212,
title = {Assessment of Hepatic Function, Perfusion and Parenchyma Attenuation with Indocyanine Green, Ultrasound and Computed Tomography in a Rat Model: Preliminary Standardization of Baseline Parameters in a Healthy Liver},
journal = {HPB},
volume = {24},
pages = {S212-S213},
year = {2022},
note = {Abstracts of the 15th World Congress of the International Hepato-Pancreato-Biliary Association, 30 March - 2 April, 2022, New York, USA},
issn = {1365-182X},
doi = {https://doi.org/10.1016/j.hpb.2022.05.437},
url = {https://www.sciencedirect.com/science/article/pii/S1365182X2200572X},
author = {V. Lopez-Lopez and N. Garcia-Carrillo and L. Oltra and D. {De Gea} and C.A. {Gonzalez Bermudez} and G. Carbonel and R. Brusadin and A. {Lopez Conesa} and R. Robles-Campos}
}
@article{HOMER2022118006,
title = {Examination of computed aluminum grain boundary structures and energies that span the 5D space of crystallographic character},
journal = {Acta Materialia},
volume = {234},
pages = {118006},
year = {2022},
issn = {1359-6454},
doi = {https://doi.org/10.1016/j.actamat.2022.118006},
url = {https://www.sciencedirect.com/science/article/pii/S1359645422003871},
author = {Eric R. Homer and Gus L.W. Hart and C. {Braxton Owens} and Derek M. Hensley and Jay C. Spendlove and Lydia Harris Serafin},
keywords = {Grain boundaries, Atomistic simulations, Aluminum},
abstract = {The space of possible grain boundary structures is vast, with 5 macroscopic, crystallographic degrees of freedom that define the character of a grain boundary. While numerous datasets of grain boundaries have examined this space in part or in full, we present a computed dataset of 7304 unique aluminum grain boundaries in the 5D crystallographic space. Our sampling also includes a range of possible microscopic, atomic configurations for each unique 5D crystallographic structure, which total over 43 million structures. We present the methods used to generate this dataset, an initial examination of the energy trends that follow the Read-Shockley relationship, hints at trends throughout the 5D space, variations in GB energy when non-minimum energy structures are examined, and insights gained in machine learning of grain boundary energy structure-property relationships. This dataset, which is available for download, has great potential for insight into GB structure-property relationships.}
}
@article{JANGITI2020100414,
title = {EMC2: Energy-efficient and multi-resource- fairness virtual machine consolidation in cloud data centres},
journal = {Sustainable Computing: Informatics and Systems},
volume = {27},
pages = {100414},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100414},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920301414},
author = {Saikishor Jangiti and Shankar Sriram VS},
keywords = {Virtual machine consolidation, Cloud computing, First-ﬁt decreasing, Dominant resource fairness, Residual resource ratio},
abstract = {The rapid rise in the cloud service adoption reflects the growth of Cloud Data Centers' (CDCs) number, size, energy consumption and eco-unfriendly carbon footprints. In CDCs, Virtual Machine Consolidation (VMC) plays a significant role in reducing their energy consumption and thereby reducing the carbon footprints. The state-of-the-art VMC heuristics based on First-Fit Decreasing (FFD) and Dominant Residual Resource (DRR) called DRR-FFD and DRR-BinFill are grouping the VMs based on single VM resource. We attempt to further reduce the energy consumption of CDCs through the proposed EMC2, an energy-efficient VMC framework that employs our multi-resource-fairness based VM selection heuristics, namely VMNeAR-H (Hierarchical), VM NeAR- D (Directed Hierarchical) and VM NeAR-E (Euclidean Distance). A dataset extracted from ENERGY STAR® containing the heterogeneous physical machine resource capacities and their estimated energy consumptions is utilised in the simulation experiments. The proposed EMC2-VMNeAR-D heuristic dominates the existing DRR heuristics in terms of total energy consumed by all the physical machines in the CDC (3.318 % energy savings on average of 40 schedules = 185107 kWh).}
}
@article{YANG201426,
title = {A method for managing green power of a virtual machine cluster in cloud},
journal = {Future Generation Computer Systems},
volume = {37},
pages = {26-36},
year = {2014},
note = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2014.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X14000466},
author = {Chao-Tung Yang and Jung-Chun Liu and Kuan-Lung Huang and Fuu-Cheng Jiang},
keywords = {Green power management, Resource allocation, Live migration, Virtual machine cluster},
abstract = {A green power management scheme is proposed to determine how many physical machines should be run or turned off based on the gross occupied resource weight ratio of the virtual machine cluster. The gross occupied resource weight ratio is defined as the ratio of the sum of resource weights of all virtual machines over the sum of available resource weights of all running physical machines. When the gross occupied resource weight ratio is greater than the maximum tolerant occupied resource weight ratio, preset to ensure quality of service, a standby physical machine in the non-running physical machines is selected and wakened up to join as one of the running physical machines. On the other hand, when the gross occupied resource weight ratio is less than the minimum critical occupied resource weight ratio, preset to trigger energy saving algorithms, one of the running physical machines, selected as a migration physical machine with the virtual machines therein removed after live migration, is moved from other running physical machines, and then turned off. As a result, a resource allocation process is realized to distribute loads of the running physical machines such that the total number of the running physical machines can be flexibly dispatched to achieve the objective of green power management.}
}
@article{REN2022110240,
title = {Diagnostic accuracy of dual-energy computed tomography angiography in the differentiation of benign and malignant pelvic masses},
journal = {European Journal of Radiology},
volume = {150},
pages = {110240},
year = {2022},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2022.110240},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X22000900},
author = {Zhen Ren and Bo Jiang and Xiong Wu and Zhibang Zhang and Hongliang Chen and Haiyi Cai and Chun Fu},
keywords = {Computed Tomography Angiography, Dual-energy CT, Pelvic Neoplasms, Diagnosis, Differential},
abstract = {Purpose
The dual-energy computed tomography(CT) angiography can accurately display subtle details of blood vessels and their surroundings. We aimed to apply dual-energy CT angiography and virtual monochromatic spectral(VMS) images to pelvic mass imaging and evaluate its value of distinguishing between benign and malignant pelvic masses.
Methods
The prospective study was approved by the Institutional Review Board and all participants signed informed consent forms. From August 2018 to July 2020, consecutive female patients with pelvic mass undergone dual-source third-generation CT angiography. The 40 keV VMS images were reconstructed to display mass morphology and corresponding feeding arteries. All images were evaluated by two radiologists blinded to diagnosis(with 9 years and 10 years of experience in CT reading).Disagreements were solved by consensus. The final diagnosis was using histopathology results as the gold standard. Interobserver variability was calculated using Cohen’s kappa and Quadratic Weighted Kappa. The differences between benign and malignant masses were compared using the chi-square test. Accuracy rate, sensitivity, specificity, and the area under the curve (AUC) were calculated as the primary indices for diagnostic accuracy. McNemar test was used to evaluate the difference in diagnostic accuracy between dual-energy CT angiography images and original CT images. A two-tailed P < 0.05 was considered statistically significant.
Results
A total of 64 patients with 28 malignant masses and 47 benign masses were included. The characteristics of malignant masses showed the branch number of the main feeding artery was ≥ 3(71.4%, 20/28), the course of feeding artery(100%, 28/28) and the mass shape(85.7%, 24/28) were both irregulars. Those characteristics all had statistical differences between benign and malignant masses(all P = 0.000). The Models using the course of feeding artery as a predictor, the accuracy, sensitivity and positive likelihood ratio were 89.3% (95 % CI: 0.801, 0.947), 100% (95 % CI: 0.850, 1) and 5.875(95 % CI: 3.125, 11.044), respectively. The diagnostic accuracy of every model by dual-energy CT angiography was significantly higher than that by original CT imaging(all P = 0.000).
Conclusions
The dual-energy CT angiography can distinguish malignant pelvic masses from benign masses by providing characteristic images of feeding arteries and mass shape.}
}
@article{YU201838,
title = {“Inverse” cloud point extraction coupled with large volume injection ion-pair chromatography: A green route integrating extraction, challenging sample cleanup and on-column concentration into fast simple operation},
journal = {Talanta},
volume = {190},
pages = {38-46},
year = {2018},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2018.07.074},
url = {https://www.sciencedirect.com/science/article/pii/S0039914018307811},
author = {Jiechun Yu and Jianhao Huang and Fei Long and Ande Ma and Jialiang Pan},
keywords = {Inverse cloud point extraction, Large-volume injection, Ion-pair chromatography, Melamine, Dairy samples},
abstract = {The efficient isolation of trace hydrophilic compounds from complicated aqueous-rich samples remains a daunting challenge. Herein, to address the analytical bottleneck, a novel inverse cloud-point extraction (ICPE) strategy was proposed based on the extraction principle opposite to that used in traditional CPE. Then, an original large-volume injection ion-pair chromatography (LVI-IPC) method was developed and systematically investigated based on the retention mechanism of the dynamic ion-exchange model. The combined ICPE-LVI-IPC method integrated extraction, challenging sample cleanup, and (on-column) concentration in a fast, simple phase-separation and injection operation. This method was successfully applied to the determination of melamine in dairy samples, including milk, yogurt and milk powder. The organic-solvent-free and sorbent-free sample preparation process could be accomplished within 11 min, requiring only 1.2 mL of water solution per sample. As an efficient on-column concentration strategy, LVI significantly improved the sensitivity. The obtained limit of detection of 0.0028 mg kg−1 was far below the limits established by the Codex Alimentarius Commission. Good recoveries were obtained from samples spiked at four concentration levels (92.6–95.8%), with satisfactory intraday and interday precisions of 1.9–3.9% and 1.6–5.2%, respectively. Further evaluation of the accuracy, by analyzing a certified milk reference material gave a small relative error of 2.2%. Comparisons with a variety of efficient methods showed the superiority of the proposed method in terms of sensitivity, speed, sample and solvent consumption, practicability, and throughput.}
}
@article{GU2020106,
title = {Energy-aware workflow scheduling and optimization in clouds using bat algorithm},
journal = {Future Generation Computer Systems},
volume = {113},
pages = {106-112},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19317066},
author = {Yi Gu and Chandu Budati},
keywords = {Workflow scheduling, Energy efficiency, Throughput, Latency, Clouds},
abstract = {With the ever-increasing deployment of data centers and computer networks around the world, cloud computing has emerged as one of the most important paradigms for large-scale data-intensive applications. However, these cloud environments face many challenges including energy consumption, execution time, heat and CO2 emission, as well as operational cost. Due to the extremely large scale of these applications and a huge amount of resource consumption, even a small portion of the improvements in any of the above fields can yield huge ecological and financial rewards. Efficient and effective workflow scheduling in cloud environments is one of the most significant ways to confront the above problems and achieve optimal resource utilization. We propose an Energy Aware, Time, and Throughput Optimization heuristic (EATTO) based on the bat algorithm. Our goal is to minimize energy consumption and execution time of computation-intensive workflows while maximizing throughput, without imposing any significant loss on the Quality of Service (QoS) guarantee.}
}
@article{PYATI20202343,
title = {Energy-efficient and Dynamic Consolidation of Virtual Machines in OpenStack-based Private Cloud},
journal = {Procedia Computer Science},
volume = {171},
pages = {2343-2352},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.254},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920312461},
author = {Mahantesh Pyati and Narayan {D G} and Shivaraj Kengond},
keywords = {SVM, Virtual Machine, OpenStack, Live Migration, energy-efficient},
abstract = {Dynamic Virtual Machines (VMs) consolidation is an efficient technique for enhancing resource utilization and reducing energy consumption by the physical servers. Over-utilized host affects the degradation of resource utilization and VM performance. Determining when it is best to migrate VMs from an over-utilized host to an under-utilized host is the aim of dynamic Live VM Migration (LVM). It influences the resource utilization and Quality of Services (QoS) offered by cloud providers. In the OpenStack cloud, LVM is done manually, and it uses first-fit as a default migration technique, which is not efficient for resource utilization. The paper proposes a novel approach for the dynamic consolidation of VMs in the OpenStack cloud. The CPU utilization, RAM utilization, and the number of instances of each host are monitored over a period of time, and overload detection of a host is carried out using the SVM classification model. The Load balancing consolidation is performed based on the SVM classification result. Further, an energy-efficient consolidation achieved to optimize the energy of physical servers. We carry out the performance analysis of the proposed work in the OpenStack-based real-time testbed of five-node. The result reveals the fair distribution of load within the servers and the significant benefits of energy-efficient VM consolidation resulting up to 15% energy savings.}
}
@article{RIASUDHEEN2020102021,
title = {An efficient energy-aware routing scheme for cloud-assisted MANETs in 5G},
journal = {Ad Hoc Networks},
volume = {97},
pages = {102021},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.102021},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519302963},
author = {H. Riasudheen and K. Selvamani and Saswati Mukherjee and I.R. Divyasree},
keywords = {Cloud-assisted, Device-to-Device, EECRM, 5G, MANETs},
abstract = {Energy-Saving techniques for mobile communication systems has recently received a great deal of attention in Fifth Generation (5G) networks. The Device-to-Device (D2D) communication in 5G networks has increased number of users and the data transmission rate among the mobile nodes in Cloud Assisted-Mobile Ad Hoc Networks (CA-MANETs). However, the connection between these mobile nodes and peer nodes has to frequently renew due to mobility, link failure, routing overhead and low battery power. During this period, it consumes high energy in searching and linking the mobile nodes. Hence, this research work attempts to reduce the energy consumption by proposing an Energy-Efficient Cloud-Assisted Routing Mechanism (EECRM) for CA-MANETs. Energy consumption is minimized by performing fast local route recovery among mobile nodes and peer nodes. If the link failure occurs, the backup nodes are identified by neighbour nodes and these backup nodes form an overlay that provides the coverage and this overlay network consumes less energy in routing. The results obtained from this proposed routing algorithm provides better performance in terms of less energy consumption, residual energy and network lifetime when compared with other existing network models and routing protocols.}
}
@article{YU2022,
title = {UAV-assisted cooperative offloading energy efficiency system for mobile edge computing},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S235286482200027X},
author = {Xue-Yong Yu and Wen-Jin Niu and Ye Zhu and Hong-Bo Zhu},
keywords = {Computation offloading, Internet of things(IoT), Mobile edge computing(MEC), Block coordinate descent(BCD)},
abstract = {Reliable communication and intensive computing power cannot be provided effectively by temporary hot spots in disaster areas and complex terrain ground infrastructure. Mitigating this has greatly developed the application and integration of UAV and Mobile Edge Computing (MEC) to the Internet of Things (IoT). However, problems such as multi-user and huge data flow in large areas, which contradict the reality that a single UAV is constrained by limited computing power, still exist. Due to allowing UAV collaboration to accomplish complex tasks, cooperative task offloading between multiple UAVs must meet the interdependence of tasks and realize parallel processing, which reduces the computing power consumption and endurance pressure of terminals. Considering the computing requirements of the user terminal, delay constraint of a computing task, energy constraint, and safe distance of UAV, we constructed a UAV-Assisted cooperative offloading energy efficiency system for mobile edge computing to minimize user terminal energy consumption. However, the resulting optimization problem is originally nonconvex and thus, difficult to solve optimally. To tackle this problem, we developed an energy efficiency optimization algorithm using Block Coordinate Descent (BCD) that decomposes the problem into three convex subproblems. Furthermore, we jointly optimized the number of local computing tasks, number of computing offloaded tasks, trajectories of UAV, and offloading matching relationship between multi-UAVs and multiuser terminals. Simulation results show that the proposed approach is suitable for different channel conditions and significantly saves the user terminal energy consumption compared with other benchmark schemes.}
}
@article{MALAHAYATI2019102003,
title = {The impact of green house gas mitigation policy for land use and the forestry sector in Indonesia: Applying the computable general equilibrium model},
journal = {Forest Policy and Economics},
volume = {109},
pages = {102003},
year = {2019},
issn = {1389-9341},
doi = {https://doi.org/10.1016/j.forpol.2019.102003},
url = {https://www.sciencedirect.com/science/article/pii/S1389934119302643},
author = {Marissa Malahayati and Toshihiko Masui},
keywords = {LUCF, Computable general equilibrium (CGE), Reforestation, Intensification, Mitigation, Forest management},
abstract = {Indonesia's economy relies heavily on the land-based sector, which results in high deforestation and forest degradation that leads to large emissions from Land Use Change and Forestry (LUCF). Under its Nationally Determined Contributions, Indonesia has committed to achieving emission reductions of 29% by 2030 as compared to the business-as-usual (BAU) scenario. However, as the largest share comes from LUCF, mitigation efforts in this sector will reduce its production. This study assesses the potential impact of the implementation of the mitigation efforts in the LUCF sector on the economy and environment through the use of a computable general equilibrium model. Two main approaches were introduced to reduce emissions: 1) increasing the crop yield (intensification), which is expected to motivate farmers to use less land and thus, lead to a decrease in deforestation; and 2) encouraging land conservation. Our simulation indicated that intensification alone results in less GDP loss but does not significantly reduce deforestation and emissions from LUCF as farmers will still cultivate the land as usual, and land expansion for commercial plantations, especially palm oil, will continue. Therefore, introducing conservation practices, though costly, may help to protect forest areas and reduce emissions from LUCF. Thus, the simultaneous implementation of intensification and conservation efforts is recommended. To this end, cooperation between the central and regional governments is necessary along with an incentive scheme that encourages farmers to adopt mitigation efforts on their lands.}
}
@article{HAJISAMI2020107170,
title = {Elastic Resource Provisioning for Increased Energy Efficiency and Resource Utilization in Cloud-RANs},
journal = {Computer Networks},
volume = {172},
pages = {107170},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107170},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619316019},
author = {Abolfazl Hajisami and Tuyen X. Tran and Ayman Younis and Dario Pompili},
keywords = {Cloud radio access network, Virtualization, Energy efficiency},
abstract = {Distributed Radio Access Networks (D-RANs), characterized by static deployment of Base Stations (BSs), are facing critical difficulties in handling the temporal and geographical fluctuations of capacity demands. Recently, Cloud RAN (C-RAN) has been introduced as a centralized cloud computing based paradigm for wireless cellular networks in which the BSs are physically decoupled into Virtual Base Stations (VBSs) and Remote Radio Heads (RRHs). In this article, a novel framework is proposed for C-RAN to adapt to the fluctuation in capacity demand while at the same time maximizing the energy efficiency and resource utilization. The RRHs and their corresponding VBSs are divided into clusters within which the active RRH density, transmission power, and size of Virtual Machines (VMs) are adjusted dynamically. To characterize the computational requirements of a VBS, a programmable C-RAN testbed is implemented using the OpenAirInterface (OAI) software platform and USRP boards. Testbed experiment results are presented to describe the consumption and utilization of computing resources in the cloud; simulation results are also presented to illustrate the performance gains of this elastic solution against the current static deployment.}
}
@article{SCARCELLO2022e08902,
title = {Cascade computing model to optimize energy exchanges in prosumer communities},
journal = {Heliyon},
volume = {8},
number = {2},
pages = {e08902},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e08902},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022001906},
author = {Luigi Scarcello and Andrea Giordano and Carlo Mastroianni and Giandomenico Spezzano},
keywords = {Energy community, Renewable energy, Cascade computing, Computing efficiency},
abstract = {Recently, the increasing availability of renewable energy plants has changed the market of electrical energy. The concept of energy community enables prosumers to exploit and exchange the energy produced locally and reduce the need for external energy sources. This can help to obtain significant cost savings and increase the percentage of green energy. In this paper, we present the Cascade model, which aims to achieve a twofold goal: compute an energy schedule that satisfies the needs of single prosumers, and maximize the energy sharing at the community level, thus minimizing the overall cost. The Cascade model partitions the prosumers in groups: at each step, an optimization problem is solved for all the users of a group. The solution enables defining a super-user that summarizes the energy requirements of the groups considered before. Then, a new group is considered in the next step, and so on, until all the groups have been processed. This approach enables preventing the exponential increase in computing complexity that is inevitable when all the prosumers are considered together, using the model referred to as Unified. Experimental results show that the Cascade model leads to a great reduction of computing time, while the overall cost closely approximates the optimal solution ensured by the Unified model.}
}