@article{RAHMAN201911,
title = {Energy-efficient optimal task offloading in cloud networked multi-robot systems},
journal = {Computer Networks},
volume = {160},
pages = {11-32},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619306371},
author = {Akhlaqur Rahman and Jiong Jin and Ashfaqur Rahman and Antonio Cricenti and Mahbuba Afrin and Yu-ning Dong},
keywords = {Cloud networked robotics, Multi-robot systems, Task offloading, Path planning, Genetic algorithm},
abstract = {Task offloading plays a critical role in cloud networked multi-robot systems for leveraging computation support from cloud infrastructure and benefiting greatly from the well-developed cloud network facilities. However, considering the delay constraint, the extra costs of data transmission and remote computation, it is not trivial to make optimized offloading decisions. In particular, task offloading for robots is more complex due to their on-demand mobility and network connectivity that significantly influence the robot–cloud communication links. Moreover, for multi-robot systems, a suitable balance of workload between local network (robot–robot) and global cloud (robot–cloud) is also required, so as to attain proper utilization of resources. Therefore, it is essential to establish more comprehensive offloading schemes for modeling systems that can handle these higher level of complications. With that view, this paper aims to develop a novel multi-layer decision-making scheme for task offloading which jointly considers the following four aspects: (i) selection of task for offloading, (ii) selection of robot to offload a task, (iii) selection of location to offload/perform task, (iv) selection of access point for offloaded task. An integrated framework for cloud networked multi-robot systems is presented to enable our task offloading scheme where the primary robot can aid from additional local robots to improve the offloading process. In particular, we consider a warehouse scenario with 36 cell workspace where a 40 node taskflow is motivated from a “parcel sorting and distribution” application, to be completed by the primary robot. The offloading decision for each task is formulated as part of a joint optimization problem and it is solved by developing a multi-layer genetic algorithm scheme that takes into account motion, network connectivity and local sharing for its offloading decisions. We evaluate the results of the scheme via comparison with two validated benchmarks. The outcome highlights a significant improvement in overall system performance due to joint involvement of motion (path planning), connectivity (bandwidth estimation) and robot–robot communication (local offloading) that facilitates energy-efficient offloading to cloud, faster completion of tasks and better utilization of available resources.}
}
@article{SUTTONPARKER2020484,
title = {Determining end user computing device Scope 2 GHG emissions with accurate use phase energy consumption measurement},
journal = {Procedia Computer Science},
volume = {175},
pages = {484-491},
year = {2020},
note = {The 17th International Conference on Mobile Systems and Pervasive Computing (MobiSPC),The 15th International Conference on Future Networks and Communications (FNC),The 10th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.07.069},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920317506},
author = {Justin Sutton-Parker},
keywords = {Energy & sustainability, Energy saving products, Carbon footprints, Using IT to reduce carbon emissions, Green business practices, Green Technology},
abstract = {Energy creates 35% of global greenhouse gas (GHG) emissions [1]. Information technology (IT) is a significant end user of energy, consuming over 10% of commercial electricity [2]. 0.37GtCO2e of IT GHG emissions are attributed to end user computing (EUC) devices, such as desktop computers and notebooks [3]. The United Nations (UN) indicates combining existing technology innovative and behavioural changes has the potential to transform societies and reduce GHG emissions [4]. As the commercial and public sector transitions from desktops to notebooks [5], accurately identifying energy efficient mobile devices offers the ability to support this concept by reducing concomitant GHG emissions. However, accurately measuring device use phase energy (UPE) generated GHG emissions is elusive. Organisations currently avoid the direct measurement due to scale, complexity and logistical issues that cause the practice to become costly and therefore impractical [6]. As an alternative, EUC energy consumption is estimated based on typical electricity consumption (TEC) benchmark data, extrapolated by the number of users and then converted to GHG equivalent units. Conducting a field experiment measuring EUC energy consumption in a business environment, this research substantiates that accepted estimation methods introduce an error range between -48% to +107%. Consequently, the error causes EUC GHG scope 2 accounting to be underestimated by 30% and abatement opportunities of up to 55% to be overlooked.}
}
@article{LI2020136,
title = {Assessment of the aggressiveness of rectal cancer using quantitative parameters derived from dual-energy computed tomography},
journal = {Clinical Imaging},
volume = {68},
pages = {136-142},
year = {2020},
issn = {0899-7071},
doi = {https://doi.org/10.1016/j.clinimag.2020.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0899707120302321},
author = {Yi Li and Xubin Li and Xiaoyi Ren and Zhaoxiang Ye},
keywords = {Rectal cancer, Tomography, X-ray computed, Prognosis},
abstract = {Purpose
To evaluate the value of quantitative parameters derived from dual-energy computed tomography (DECT) in assessing the aggressiveness of rectal cancer.
Materials and methods
Seventy-eight patients with rectal cancers confirmed by pathology underwent contrasted DECT scans. The normalized iodine concentration (NIC) and normalized water concentration (NWC) of the tumor against artery and tumor sizes were measured. The quantitative parameters were compared and statistically analyzed between subgroups based on the following prognostic factors: pretreatment carcinoembryonic antigen (CEA) levels, mesorectal fascia (MRF) status, T stage (T1,2 and T3,4), N stage (N0 and N1,2), tumor differentiation grade (poor differentiation, poor-moderate differentiation, moderate differentiation, moderate-well differentiation, well differentiation), and extramural venous invasion.
Results
The differences of NIC values between MRF-free and MRF-invaded groups (P = 0.042), between T2 and T3–4 stage groups (P = 0.044), between N0 and N+ (N1, 2) groups (P = 0.036), between poor differentiation group and other differentiated groups (P < 0.05)were respectively significant. No significant differences of NIC values existed between CEA level or extramural venous invasion subgroups. For NWC values and tumor sizes, there were no significant differences between subgroups based on the prognostic factors above all.
Conclusions
Higher NIC value is associated with a more aggressive tumor character. NIC value may have the potential to become an imaging biomarker of tumor aggressiveness.}
}
@article{KO2020102256,
title = {Smart home energy strategy based on human behaviour patterns for transformative computing},
journal = {Information Processing & Management},
volume = {57},
number = {5},
pages = {102256},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102256},
url = {https://www.sciencedirect.com/science/article/pii/S0306457320300662},
author = {Hoon Ko and Jong Hyuk Kim and Kyungjin An and Libor Mesicek and Goreti Marreiros and Sung Bum Pan and Pankoo Kim},
keywords = {Energy strategy, Ecg (electrocardiography), Human behaviour pattern, Transformative computing},
abstract = {Summary
Transformative computing in the fourth-generation industrialization, receives all signals and all sequences from sensing devices under artificial intelligence in wireless networking. Then the system has to combine them and make a useful information for human. In an industrial building or in a home, many electronic devices would be using and they make various energy signals and sequences. The devices can find out the energy wastage in the absence of a smart energy management system to monitor the energy flow, and it causes a blackout. Once the energy flow is analysed, it is possible to realize the special-time or the rush-time, which will require a large amount of energy. Because the existing systems have no monitor to see the energy flow, a large amount of energy can be wasted. To distribute the energy efficiently, a smart energy management system should have the necessary special functions that can monitor the energy flow. Following the analysis result, the system can create a special strategy to plan energy distribution. In this study, the smart energy management system defines a special strategy based on the analysis result of the consumed energy by arranging more or less usage of energy. Moreover, the system can decrease the energy supply to idle devices and the connected extra devices by analysing how many IoT will be used in a service. This smart control system can detect human behaviour when they move and turn in activation automatically, so finally, the system can use the energy efficiently.}
}
@article{HOSSEINZADEH201943,
title = {A model for the minimum ignition energy of dust clouds},
journal = {Process Safety and Environmental Protection},
volume = {121},
pages = {43-49},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957582018302611},
author = {Sepideh Hosseinzadeh and Jan Berghmans and Jan Degreve and Filip Verplaetsen},
keywords = {Minimum ignition energy, Dust clouds, Minimum ignition temperature, Theoretical model},
abstract = {This study refers to the minimum ignition energy (MIE) of dusts as determined by means of the Hartmann tube. To do so, six different dust with different particle size and properties are employed. A theoretical model is developed to predict the MIE based upon the physical and chemical properties of the dust. The model clearly shows the relationship between the minimum ignition energy and the minimum ignition temperature of a dust cloud. The model results in a method to calculate the MIE of dusts. Also, the time necessary for ignition can be calculated with the model. The results show that despite some limitations, the model predicts the MIE for dusts rather well. It is proposed that 40–60% energy loss would be considered when applying the model.}
}
@article{GONZALEZSANCHEZ2020241,
title = {Segmentation of bones in medical dual-energy computed tomography volumes using the 3D U-Net},
journal = {Physica Medica},
volume = {69},
pages = {241-247},
year = {2020},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2019.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S1120179719305356},
author = {José Carlos {González Sánchez} and Maria Magnusson and Michael Sandborg and Åsa {Carlsson Tedgren} and Alexandr Malusek},
keywords = {Deep learning, Convolutional neural network, Segmentation, Dual-energy computed tomography, 92B20},
abstract = {Deep learning algorithms have improved the speed and quality of segmentation for certain tasks in medical imaging. The aim of this work is to design and evaluate an algorithm capable of segmenting bones in dual-energy CT data sets. A convolutional neural network based on the 3D U-Net architecture was implemented and evaluated using high tube voltage images, mixed images and dual-energy images from 30 patients. The network performed well on all the data sets; the mean Dice coefficient for the test data was larger than 0.963. Of special interest is that it performed better on dual-energy CT volumes compared to mixed images that mimicked images taken at 120 kV. The corresponding increase in the Dice coefficient from 0.965 to 0.966 was small since the enhancements were mainly at the edges of the bones. The method can easily be extended to the segmentation of multi-energy CT data.}
}
@article{SANANMUANG2020311,
title = {Dual Energy Computed Tomography in Head and Neck Imaging: Pushing the Envelope},
journal = {Neuroimaging Clinics of North America},
volume = {30},
number = {3},
pages = {311-323},
year = {2020},
note = {State of the Art Evaluation of the Head and Neck},
issn = {1052-5149},
doi = {https://doi.org/10.1016/j.nic.2020.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1052514920300265},
author = {Thiparom Sananmuang and Mohit Agarwal and Farhad Maleki and Nikesh Muthukrishnan and Juan Camilo Marquez and Jeffrey Chankowsky and Reza Forghani},
keywords = {Dual energy CT, Spectral CT, Head and neck squamous cell carcinoma, Thyroid cartilage invasion, Radiomics, Texture analysis, Machine learning, Deep neural networks}
}
@article{LATIF2020114858,
title = {State-of-the-art of controllers and soft computing techniques for regulated load frequency management of single/multi-area traditional and renewable energy based power systems},
journal = {Applied Energy},
volume = {266},
pages = {114858},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.114858},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920303706},
author = {Abdul Latif and S.M. Suhail Hussain and Dulal Chandra Das and Taha Selim Ustun},
keywords = {Load frequency management (LFM), Automatic generation control (AGC), Interconnected renewable microgrid power system, Single/multi area electric power network, Intelligent controller, Soft computing techniques},
abstract = {Load frequency management (LFM) has become more significant in modern power systems due to variation demand and generation profiles. Further, integration of renewable energy resources (RSs) into the power systems makes LFM job more challenging. To this end, the concept of secondary frequency control, or LFM, objective is introduced to single and multi-area power systems to manage the power mismatch of the particular power system. This helps regulate the system frequency for single/multi area systems and schedule tie-line power exchange for multi area systems. To control and reduce the system frequency deviation, load frequency controllers are introduced. However, to achieve optimal power management, intelligent soft computing techniques that take different controllers into account are utilized. This paper aims to provide a review of different controllers utilized in traditional as well as renewable energy-based power system for LFM such as; classical controllers, fractional order controllers, cascaded controllers, sliding mode controller (SMC), tilt-integral-derivative controllers, H-infinity controller and other recently developed controllers. Some popular and recently adopted soft-computing tools for power management such as; genetic algorithm, particle swarm optimization, firefly, cuckoo search techniques, fuzzy tuning tool, model predictive technique and other newer once have been explored. Finally, the paper concludes by highlighting some future scope in the field of LFM.}
}
@article{WEN201939,
title = {Energy and cost aware scheduling with batch processing for instance-intensive IoT workflows in clouds},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {39-50},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18331066},
author = {Yiping Wen and Zhibin Wang and Yu Zhang and Jianxun Liu and Buqing Cao and Jinjun Chen},
keywords = {Energy, Cost, Instance-intensive workflow, Scheduling, Batch processing},
abstract = {Cloud computing is a suitable platform to execute various applications. At the same time, it should not only provide QoS such as high throughput but also achieve relevant criteria such as efficient power consumption and appropriate execution cost. To address this challenge, we present an energy and cost aware algorithm for scheduling instance-intensive IoT workflows with batch processing in clouds, which is named ECIB and aimed to improve energy efficiency and reduce execution cost while meeting the deadline requirements. Specifically, we propose a prediction based strategy to guide the management of resources by the historical data and CPU usage prediction results of physical machines. Then, we propose two strategies to scale up or scale down the virtual machine resources to optimize the energy consumption for the cloud data centers. In addition, we adopt a batch processing strategy to merge some activity instances of the same type so as to reduce execution cost for the cloud users and improve resource utilization for the cloud data centers. The effectiveness of the ECIB algorithm is evaluated by extensive experiments using CloudSim and four types of instance-intensive IoT workflow applications.}
}
@article{ZHANG2019147,
title = {Energy-aware virtual machine allocation for cloud with resource reservation},
journal = {Journal of Systems and Software},
volume = {147},
pages = {147-161},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.09.084},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302152},
author = {Xinqian Zhang and Tingming Wu and Mingsong Chen and Tongquan Wei and Junlong Zhou and Shiyan Hu and Rajkumar Buyya},
keywords = {Cloud computing, Virtual machine allocation, Evolutionary algorithm, Energy efficiency, VM acceptance ratio},
abstract = {To reduce the price of pay-as-you-go style cloud applications, an increasing number of cloud service providers offer resource reservation-based services that allow tenants to customize their virtual machines (VMs) with specific time windows and physical resources. However, due to the lack of efficient management of reserved services, the energy efficiency of host physical machines cannot be guaranteed. In today’s highly competitive cloud computing market, such low energy efficiency will significantly reduce the profit margin of cloud service providers. Therefore, how to explore energy efficient VM allocation solutions for reserved services to achieve maximum profit is becoming a key issue for the operation and maintenance of cloud computing. To address this problem, this paper proposes a novel and effective evolutionary approach for VM allocation that can maximize the energy efficiency of a cloud data center while incorporating more reserved VMs. Aiming at accurate energy consumption estimation, our approach needs to simulate all the VM allocation updates, which is time-consuming using traditional cloud simulators. To overcome this, we have designed a simplified simulation engine for CloudSim that can accelerate the process of our evolutionary approach. Comprehensive experimental results obtained from both simulation on CloudSim and real cloud environments show that our approach not only can quickly achieve an optimized allocation solution for a batch of reserved VMs, but also can consolidate more VMs with fewer physical machines to achieve better energy efficiency than existing methods. To be specific, the overall profit improvement and energy savings achieved by our approach can be up to 24% and 41% as compared to state-of-the-art methods, respectively. Moreover, our approach could enable the cloud data center to serve more tenant requests.}
}
@article{LIAO2018347,
title = {Energy-efficient virtual content distribution network provisioning in cloud-based data centers},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {347-357},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.057},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1732099X},
author = {Dan Liao and Gang Sun and Guanghua Yang and Victor Chang},
keywords = {Content distribution network, Energy efficiency, Cloud computing, Service level agreement},
abstract = {Cloud-based content distribution networks (CDNs) consist of multiple servers that consume large amounts of energy. However, with the development of a cloud-based software defined network (SDN), a new paradigm of the virtual content distribution network (vCDN) has emerged. In an emerging cloud-based vCDN environment, the development and adjustment of vCDN components has become easier with the aid of SDN technology. This transformation provides the opportunity to use vCDNs to reduce energy consumption by adjusting the scale of the vCDN components. Energy costs can be reduced by deactivating the commercial servers carrying the software components of the vCDN, such as replica servers, the firewall or routers. In addition, the CDN requires a high service level agreement (SLA) to respond to clients’ requests, potentially consuming large amounts of energy. In this research, we focus on the issue of the energy savings of a CDN in a cloud computing environment while maintaining a high quality of service (QoS). We propose an approximate algorithm termed max flow forecast (MFF) to determine the number of software components in the vCDN. Additionally, we use a real traffic trace from a website to assess our algorithm. The experimental results show that MFF can produce a larger energy reduction than the existing algorithms for an identical SLA. We fully justify our research as a good example for the emerging cloud.}
}
@article{SOLTANSHAHI2019e02066,
title = {Energy-aware virtual machines allocation by krill herd algorithm in cloud data centers},
journal = {Heliyon},
volume = {5},
number = {7},
pages = {e02066},
year = {2019},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e02066},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019357263},
author = {Minoo Soltanshahi and Reza Asemi and Nazi Shafiei},
keywords = {Computer science, Green computing, Cloud computing, Virtualization, Data center, Krill herd algorithm},
abstract = {The growing demand for computational power has led to the emergence of large-scale data centers that consume massive amounts of energy, thus resulting in high operating costs and CO2 emission. Furthermore, cloud computing environments are required to provide a high Quality of Service (QoS) to their clients and, therefore, need to handle power shortages. An optimized virtual machine allocation to physical hosts lowers energy consumption and allows for high-quality services. In this study, a novel solution was proposed for the allocation of virtual machines to physical hosts in cloud data centers using the Krill Herd algorithm, which is the fastest collective intelligence algorithm recently introduced. The performance of the proposed method was evaluated using the CloudSim simulator, and the results are suggestive of a 35% reduction in energy consumption.}
}
@article{DEWANGAN2019204,
title = {Self-characteristics based Energy-Efficient Resource Scheduling for Cloud},
journal = {Procedia Computer Science},
volume = {152},
pages = {204-211},
year = {2019},
note = {International Conference on Pervasive Computing Advances and Applications- PerCAA 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919306969},
author = {Bhupesh Kumar Dewangan and Amit Agarwal and M Venkatadri and Ashutosh Pasricha},
keywords = {Self-optimization, Self-healing, Resource Cost, Resource Utilization, SLA Violation Rate, QoS},
abstract = {Energy optimization in cloud computing is more attractive and much consideration in resource management, where, energy consumption is also one of the important metrics that should be included to resource management technique apart from performance and utilization of resource metric. The majority of existing methodologies for scheduling of resources are focusing on cost and execution time. In fact, the resource failures also distract the execution of cloud resource provisioning, which results in more operative cost and time. In this paper, the self-optimized energy efficient resource management strategy has been consider, which gives the optimal solution to maximize the resource utilization and identify the faulty resources to avoid misleading of scheduling. This strategy, allocate the resources to the workloads submitted by the cloud user by the fulfillment of the QoS with less SLA violation rate and maximizes the resource utilization at low cost. The novelty of this paper is to applied Antlion Optimization algorithm to find the optimal resource. The experimental results are evidence of utmost performance of proposed work.}
}
@article{MUKAIYATAGAI2020302,
title = {Myocardial delayed enhancement on dual-energy computed tomography: The prevalence and related factors in patients with suspicion of coronary artery disease},
journal = {Journal of Cardiology},
volume = {75},
number = {3},
pages = {302-308},
year = {2020},
note = {Topics in heart failure},
issn = {0914-5087},
doi = {https://doi.org/10.1016/j.jjcc.2019.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0914508719302631},
author = {Natsuko Mukai-Yatagai and Yasutoshi Ohta and Ryosuke Amisaki and Naoko Sasaki and Toshihiko Akasaka and Tomomi Watanabe and Junichi Kishimoto and Masahiko Kato and Toshihide Ogawa and Kazuhiro Yamamoto},
keywords = {Coronary artery disease, Tomography, X-ray computed, Radiology, Dual-energy scanned projection, Myocardium},
abstract = {Background
We aimed to assess the prevalence of myocardial delayed enhancement (MDE) in patients with suspected obstructive coronary artery disease (CAD), and to investigate factors related to the presence or absence of MDE.
Methods
We retrospectively evaluated 191 consecutive patients who underwent coronary computed tomography angiography (CCTA) with MDE imaging for clinical suspicion of CAD from December 2014 to December 2016. The presence of MDE on iodine-density images using dual-energy CT was assessed by two independent readers. Multivariable logistic regression analyses were used to determine factors associated with the presence of MDE.
Results
MDE was detected in 58 (30%) patients. Male gender, hypertension, prior heart failure (HF) hospitalization, and CCTA-detected CAD were independent factors related to the presence of MDE. When CCTA-detected CAD was excluded to narrow down the analysis to factors obtainable before CCTA, interventricular septum thickness (IVST) ≥12 mm was added as another independent factor. The combination of the following four factors: female gender, no history of hypertension, no history of prior HF hospitalization, and IVST < 12 mm demonstrated high specificity (98.3%) and positive predictive value (96.2%) for predicting the absence of MDE.
Conclusions
Male gender, hypertension, prior HF hospitalization, and CAD were independently associated with the presence of MDE in patients with suspected CAD. The combination of female gender, no history of hypertension, no history of prior HF hospitalization, and IVST < 12 mm is likely to be a helpful predictor in discriminating patients without MDE before CCTA.}
}
@article{FERNANDEZCERERO20193,
title = {GAME-SCORE: Game-based energy-aware cloud scheduler and simulator for computational clouds},
journal = {Simulation Modelling Practice and Theory},
volume = {93},
pages = {3-20},
year = {2019},
note = {Modeling and Simulation of Cloud Computing and Big Data},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X18301229},
author = {Damian Fernández-Cerero and Agnieszka Jakóbik and Alejandro Fernández-Montes and Joanna Kołodziej},
keywords = {Computational cluster, Energy saving, Cloud computing, Energy-aware scheduling, Stackelberg game},
abstract = {Energy-awareness remains one of the main concerns for today's cloud computing (CC) operators. The optimisation of energy consumption in both cloud computational clusters and computing servers is usually related to scheduling problems. The definition of an optimal scheduling policy which does not negatively impact to system performance and task completion time is still challenging. In this work, we present a new simulation tool for cloud computing, GAME-SCORE, which implements a scheduling model based on the Stackelberg game. This game presents two main players: a) the scheduler and b) the energy-efficiency agent. We used the GAME-SCORE simulator to analyse the efficiency of the proposed game-based scheduling model. The obtained results show that the Stackelberg cloud scheduler performs better than static energy-optimisation strategies and can achieve a fair balance between low energy consumption and short makespan in a very short time.}
}
@article{ALDULAIMY2018185,
title = {Type-aware virtual machine management for energy efficient cloud data centers},
journal = {Sustainable Computing: Informatics and Systems},
volume = {19},
pages = {185-203},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917304249},
author = {Auday Al-Dulaimy and Wassim Itani and Rached Zantout and Ahmed Zekri},
keywords = {Cloud Computing, Cloud Data Centers, Energy Efficiency, Multiple Choice Knapsack Problem, VM Consolidation, VM Placement},
abstract = {To meet the growing demands on cloud services and applications, a sizeable number of large scale cloud data centers, hosting thousands of heterogeneous servers, is established by cloud service providers. The ever growth in establishing cloud data centers is accompanied by consuming enormous amounts of energy. Thus, proposing efficient management approaches to reduce the energy consumption in cloud data centers becomes a top priority for ensuring the scalability of the cloud computing architecture. In general, the main source of energy overconsumption in today’s data centers is due to the inefficient use of the physical servers’ resources, which results in poor server utilization patterns. So, the key aspect is to utilize the physical resources optimally while serving the cloud user demands. This paper investigates the design and implementation of virtual machine management strategies for energy efficient cloud data centers. Particularly, it considers the processes of virtual machine placement and virtual machine consolidation in enhancing the energy efficiency in cloud infrastructures. While addressing the virtual machine placement problem is important, virtual machine consolidation is even more important to enable continuous reorganization of the already-placed virtual machines on the least number of physical machines. This results in reducing the number of active physical machines by leveraging live virtual machine migration enabled by the virtualization concept. Moreover, since the virtual machine migration operations consume additional energy, the frequency of VM migrations needs to be limited and controlled as well. The paper presents a distributed approach to an energy-efficient dynamic virtual machine consolidation mechanism. This approach determines, based on novel algorithms, which virtual machines to migrate, and when. Then, the placement of the virtual machines selected for migration is achieved based on a generalization of the Knapsack Problem known as the Multiple Choice Knapsack Problem. The placement process suits both static and dynamic virtual machine placement. The results of the performance evaluation demonstrate that the proposed new algorithms are able to enhance the energy efficiency in cloud data centers.}
}
@article{HOSSEINIOUN202088,
title = {A new energy-aware tasks scheduling approach in fog computing using hybrid meta-heuristic algorithm},
journal = {Journal of Parallel and Distributed Computing},
volume = {143},
pages = {88-96},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S074373152030023X},
author = {Pejman Hosseinioun and Maryam Kheirabadi and Seyed Reza {Kamel Tabbakh} and Reza Ghaemi},
keywords = {Fog computing, Task scheduling, Energy consumption, Meta-heuristic Algorithm, DVFS},
abstract = {In recent years, large computational problems have beensolved by the distributed environment in which applications are executed in parallel. Also, lately, fog computing or edge computing as a new environment is applied to collect data from the devices and preprocessing is done before sending for main processing in cloud computing. Since one of the crucial issues in such systems is task scheduling, this issue is addressed by considering reducing energy consumption. In this study, an energy-aware method is introduced by using the Dynamic Voltage and Frequency Scaling (DVFS) technique to reduce energy consumption. In addition, in order to construct valid task sequences, a hybrid Invasive Weed Optimization and Culture (IWO-CA) evolutionary algorithm is applied. The experimental results revealed that the proposed algorithm improves some current algorithms in terms of energy consumption.}
}
@article{DONG2018112,
title = {An efficient global energy optimization approach for robust 3D plane segmentation of point clouds},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {137},
pages = {112-133},
year = {2018},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2018.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S0924271618300133},
author = {Zhen Dong and Bisheng Yang and Pingbo Hu and Sebastian Scherer},
keywords = {Plane segmentation, Multiscale supervoxel, Hybrid region growing, Energy optimization, Simulated annealing, Guided sampling},
abstract = {Automatic 3D plane segmentation is necessary for many applications including point cloud registration, building information model (BIM) reconstruction, simultaneous localization and mapping (SLAM), and point cloud compression. However, most of the existing 3D plane segmentation methods still suffer from low precision and recall, and inaccurate and incomplete boundaries, especially for low-quality point clouds collected by RGB-D sensors. To overcome these challenges, this paper formulates the plane segmentation problem as a global energy optimization because it is robust to high levels of noise and clutter. First, the proposed method divides the raw point cloud into multiscale supervoxels, and considers planar supervoxels and individual points corresponding to nonplanar supervoxels as basic units. Then, an efficient hybrid region growing algorithm is utilized to generate initial plane set by incrementally merging adjacent basic units with similar features. Next, the initial plane set is further enriched and refined in a mutually reinforcing manner under the framework of global energy optimization. Finally, the performances of the proposed method are evaluated with respect to six metrics (i.e., plane precision, plane recall, under-segmentation rate, over-segmentation rate, boundary precision, and boundary recall) on two benchmark datasets. Comprehensive experiments demonstrate that the proposed method obtained good performances both in high-quality TLS point clouds (i.e., SEMANTIC3D.NET dataset) and low-quality RGB-D point clouds (i.e., S3DIS dataset) with six metrics of (94.2%, 95.1%, 2.9%, 3.8%, 93.6%, 94.1%) and (90.4%, 91.4%, 8.2%, 7.6%, 90.8%, 91.7%) respectively.}
}
@article{ARIFVIANTO201648,
title = {Characterization of the porous structures of the green body and sintered biomedical titanium scaffolds with micro-computed tomography},
journal = {Materials Characterization},
volume = {121},
pages = {48-60},
year = {2016},
issn = {1044-5803},
doi = {https://doi.org/10.1016/j.matchar.2016.09.026},
url = {https://www.sciencedirect.com/science/article/pii/S1044580316303308},
author = {B. Arifvianto and M.A. Leeflang and J. Zhou},
keywords = {Titanium scaffold, Space holder, Porous structure, Green body, Sintering},
abstract = {The present research was aimed at gaining an understanding of the porous structure changes from the green body through water leaching and sintering to titanium scaffolds. Micro-computed tomography (micro-CT) was performed to generate 3D models of titanium scaffold preforms containing carbamide space-holding particles and sintered scaffolds containing macro- and micro-pores. The porosity values and structural parameters were determined by means of image analysis. The result showed that the porosity values, macro-pore sizes, connectivity densities and specific surface areas of the titanium scaffolds sintered at 1200°C for 3h did not significantly deviate from those of the green structures with various volume fractions of the space holder. Titanium scaffolds with a maximum specific surface area could be produced with an addition of 60–65vol% carbamide particles to the matrix powder. The connectivity of pores inside the scaffold increased with rising volume fraction of the space holder. The shrinkage of the scaffolds prepared with >50vol% carbamide space holder, occurring during sintering, was caused by the reductions of macro-pore sizes and micro-pore sizes as well as the thickness of struts. In conclusion, the final porous structural characteristics of titanium scaffolds could be estimated from those of the green body.}
}
@article{THEIN20201127,
title = {Reinforcement learning based methodology for energy-efficient resource allocation in cloud data centers},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {10},
pages = {1127-1139},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818306554},
author = {Thandar Thein and Myint Myat Myo and Sazia Parvin and Amjad Gawanmeh},
keywords = {Reinforcement learning, Resource allocation, Energy efficiency, Green cloud, Cloud data centers},
abstract = {Energy-efficient Cloud Infrastructure Resource Allocation Framework is getting popularity as it is paying effective attention to cloud data management with a view to achieve maximize revenue and minimize cost. This infrastructure can encourage for both cloud providers and users for allocating cloud infrastructure resources for fulfilling not only good energy efficiency measured in Power Usage Effectiveness (PUE) and data center Infrastructure Efficiency (DCiE) but also high CPU utilization. Therefore, in this paper we proposed a framework which can show effective performance for achieving the high data center energy efficiency and preventing Service Level Agreement (SLA) violation respectively with the aim of green cloud resources deployment. The framework accomplishes cloud infrastructure resource allocation on the basic of Reinforcement Learning mechanism and Fuzzy Logic for green solutions. The evaluation for Energy-efficient Resource Allocation is experimented on the traces of the PlanetLab virtualized environment for gaining good PUE and CPU utilization.}
}
@article{RAMAKRISHNAN2019265,
title = {Analysis of energy efficiency in cloud based heterogeneous RAN with large-scale antenna systems},
journal = {Computer Networks},
volume = {149},
pages = {265-276},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618313264},
author = {S. Ramakrishnan and Subrat Kar and Dharmaraja Selvamuthu},
keywords = {Cloud RAN, Large scale antenna systems, Massive MIMO, Edge network, Power consumption, Ethernet based CPRI},
abstract = {To address the capacity requirements resulting from huge growth in mobile data traffic, the mobile network operators (MNOs) are densifying their networks with more base stations, and with more spectrum layers. The addition of base stations and spectrum layers increases the energy consumption of the access network. Hence the network needs to be optimized to lower the power consumption (e.g.) through cloud, or deployment of spectral and power efficient radio units like Large-Scale Antenna Systems (LSAS) or massive Multiple Input Multiple Output (MIMO) radio units. In this paper, we analyse the network evolution towards Cloud based Radio Access Network (C-RAN) for a mix of base stations with Macro and LSAS Remote Radio Units (RRU). We derive the computational complexity of the base station components using a flexible power model. We evaluate the need for base station architecture split, when C-RAN is used with LSAS radio units and large antenna configurations. We use a combination of simulators viz.NS3 (for LTE radio network simulation) and CloudSIM Plus (for Cloud based base station simulation). For each base station type, we compare the computational complexity of different sub-components. Further, based on the selected power model, we study the effect of base station's scheduling algorithm (viz.) Proportionate-Fair or Round-Robin, on the computational complexity of the various base station sub-components. We consider Time and Wavelength Division Multiplexed Passive Optical Network (TWDM-PON) for the high capacity low latency fronthaul network. Using the overall radio cluster throughput and power consumption information from Cloud base station, Radio site, and fronthaul network, we assess the energy efficiency metrics for the Cloud RAN and LSAS (with different antenna configurations) with Macro RRUs as reference. Through the simulations, we observe that, the computation complexity was relatively higher for RR scheduling (compared to PF scheduling). But the variations of energy efficiency metric were similar for both the scheduling schemes for different base station configurations. We observe that compared to Macro2TR, LSAS variants are highly energy efficient. This gain is mainly associated with higher throughput due to MU-MIMO (where multiple UEs are accommodated in the same set of radio resources) and lower power consuming components in LSAS. Between LSAS32TR, LSAS64TR and LSAS128TR, we find that LSAS32TR had lower energy efficiency. Further, we observe that the energy efficiency of LSAS64TR is almost same as LSAS128TR, though LSAS64TR supports just half the number of MU-MIMO layers. This implies that the additional number of layers that LSAS128TR system supports, is compensated by power consumed to support the higher antenna chains and spatially multiplexed layers. Thus, MNOs interested in deploying high spectral efficiency solution along with high energy efficiency, may prefer to deploy LSAS64 systems for tower-top deployments of Radio unit deployment, since it offers higher energy efficiency with lower wind-load effect due to smaller antenna size.}
}
@article{IMAMINE2020S256,
title = {Abstract No. 588 Success rate and complications of indocyanine green and lipiodol mixture with fat emulsion for computed tomography–guided pulmonary marking before video-assisted thoracic surgery: a retrospective comparison with short hook wire},
journal = {Journal of Vascular and Interventional Radiology},
volume = {31},
number = {3, Supplement },
pages = {S256},
year = {2020},
note = {Proc: SIR 2020 Annual Scientific Meeting Program},
issn = {1051-0443},
doi = {https://doi.org/10.1016/j.jvir.2019.12.649},
url = {https://www.sciencedirect.com/science/article/pii/S1051044319317130},
author = {R. Imamine and A. Okumura and H. Ito and Y. Yamamoto and Y. Ishizaka and E. Sakata and N. Okumura and T. Koyama}
}
@article{XU2020109738,
title = {Computing critical energy release rates for fracture in atomistic simulations},
journal = {Computational Materials Science},
volume = {181},
pages = {109738},
year = {2020},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2020.109738},
url = {https://www.sciencedirect.com/science/article/pii/S0927025620302299},
author = {G.Q. Xu and M.J. Demkowicz},
keywords = {Atomistic modeling, Fracture, Nickel},
abstract = {We describe a method for computing critical energy release rates for crack propagation in atomistic models and apply it to simulations of fracture in Ni. Our method relies on independent calculations of crack surface area and energy dissipated during fracture. We show that the critical energy release rate for fracture in Ni increases linearly with model size due to increasing energy dissipated through plastic deformation. We conclude with a discussion of prospects for direct comparisons of critical energy release rates computed from atomistic simulations to ones obtained from experiments.}
}
@article{SACHS20189,
title = {Flight of frigatebirds inside clouds – energy gain, stability and control},
journal = {Journal of Theoretical Biology},
volume = {448},
pages = {9-16},
year = {2018},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2018.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S0022519318301292},
author = {Gottfried Sachs and Henri Weimerskirch},
keywords = {Zero visibility flight, Trade cumulus cloud, Circling soaring, Flight stability, Energy saving},
abstract = {Investigating the unique ability of frigatebirds of flying inside clouds, it is shown that they achieve a large energy gain by ascents to high altitudes in strong updrafts of trade cumulus clouds. Frigatebirds often perform that kind of flight, at daytime as well as in the night. This suggests that they are capable of flying inside clouds in a controlled and stabilized manner. The control requirements for ascents in terms of a circling flight in updrafts of trade cumulus clouds are analyzed, and the necessary aerodynamic control moments are determined. Based on a stability investigation, it is shown that there are restoring effects which act against disturbances causing possible deviations from the circling flight condition. The aerodynamic moments which effectuate that stabilization are identified. Furthermore, the problem of neutral azimuth stability which generally exists in the flight of birds and which is the reason for continually increasing deviations from the course is dealt with. It is shown for the circling flight mode of frigatebirds inside clouds that, here, deviations are small and remain constant, suggesting that a corrective control action is not required. This is particularly important for circling flight in conditions without a visual reference, like inside clouds.}
}
@article{LI2019106890,
title = {Energy efficient computation offloading for nonorthogonal multiple access assisted mobile edge computing with energy harvesting devices},
journal = {Computer Networks},
volume = {164},
pages = {106890},
year = {2019},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.106890},
url = {https://www.sciencedirect.com/science/article/pii/S1389128618308077},
author = {Chunlin Li and Jianhang Tang and Yang Zhang and Xin Yan and Youlong Luo},
keywords = {Mobile edge computing, Wireless energy transfer, NOMA, IoT,},
abstract = {The mobile edge computing (MEC) is combined with wireless energy transfer (WET) to enhance the performance of wireless devices in Internet of Things (IoT) systems, which is called wireless powered mobile edge computing (WP-MEC). The sustainable energy supply and adequate computing capabilities are provided for wireless devices in WP-MEC. In this paper, we propose an online energy consumption minimization (OECM) algorithm for multiple IoT devices with latency constraints in WP-MEC environment. The nonorthogonal multiple access (NOMA) technique is used to address the preamble collision where two or more devices may select the same preamble. Based on Lyapunov optimization framework, optimal results of the time allocation for energy transfer and data transmission, the proportions of offloading tasks, data transmit powers and CPU-cycle frequencies are achieved in each time block by an iterative algorithm which leverages a convex approximation approach. The theoretical performance analysis shows that there is an upper bound of the system energy consumption. Simulation results verify that our proposed OECM algorithm outperforms the scheme without wireless energy transfer and other benchmark algorithms.}
}
@article{LOUREIRO2015381,
title = {An explicit time-stepping technique for elastic waves under concepts of Green’s functions computed locally by the FEM},
journal = {Engineering Analysis with Boundary Elements},
volume = {50},
pages = {381-394},
year = {2015},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2014.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0955799714002392},
author = {F.S. Loureiro and J.E.A. Silva and W.J. Mansur},
keywords = {Local Green’s functions, Principle of causality, Time stepping, FEM, Elastic waves},
abstract = {This work presents a new time-marching scheme able to reduce spurious (unphysical) oscillations by means of algorithm damping ratio for elastic wave propagation problems in the framework of the Explicit Green’s Approach (ExGA) (Mansur WJ, et al. J Comp Phys 2007; 227:851–870). The integral expression concerned with the ExGA is written in terms of the Green’s and the Step response functions. Their computations are carried out independently by means of the semidiscrete FEM and the Central difference method. Due to the principle of causality, the Green’s and Step response functions admit a compact support surround the source points for a small enough time step that is usually employed in common explicit time integration methods applied to wave propagation modeling. In this sense, the Green’s and Step response functions at t=Δt can be efficiently computed locally through small subdomains. Each local subdomain with its respective submesh covers only nodes whose Green’s and Step response function values do not vanish. The accuracy and effectiveness of the proposed methodology are demonstrated by analyzing three numerical examples.}
}
@article{SAFARI2018311,
title = {Energy-aware scheduling algorithm for time-constrained workflow tasks in DVFS-enabled cloud environment},
journal = {Simulation Modelling Practice and Theory},
volume = {87},
pages = {311-326},
year = {2018},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2018.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X18300984},
author = {Monire Safari and Reihaneh Khorsand},
keywords = {Energy consumption reduction, Energy-aware scheduling, Workflow tasks, Dynamic voltage and frequency scaling (DVFS)},
abstract = {Energy consumption in cloud data centers is increasing as the use of such services increases. It is necessary to propose new methods of decreasing energy consumption. Green cloud computing helps to reduce energy consumption and significantly decreases both operating costs and greenhouse gas emissions. Scheduling the enormous number of user-submitted workflow tasks is an important aspect of cloud computing. Resources in cloud data centers should compute these tasks using energy efficient techniques. This paper proposed a new energy-aware scheduling algorithm for time-constrained workflow tasks using the DVFS method in which the host reduces the operating frequency using different voltage levels. The goal of this research is to reduce energy consumption and SLA violations and improve resource utilization. The simulation results show that the proposed method performs more efficiently when evaluating metrics such as energy utilization, average execution time, average resource utilization and average SLA violation.}
}
@article{CAIZA2020e03706,
title = {Fog computing at industrial level, architecture, latency, energy, and security: A review},
journal = {Heliyon},
volume = {6},
number = {4},
pages = {e03706},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e03706},
url = {https://www.sciencedirect.com/science/article/pii/S240584402030551X},
author = {Gustavo Caiza and Morelva Saeteros and William Oñate and Marcelo V. Garcia},
keywords = {Computer science, Industry 4.0, Cloud computing, Fog nodes, Fog computing, Smart factories},
abstract = {The industrial applications in the cloud do not meet the requirements of low latency and reliability since variables must be continuously monitored. For this reason, industrial internet of things (IIoT) is a challenge for the current infrastructure because it generates a large amount of data making cloud computing reach the edge and become fog computing (FC). FC can be considered as a new component of Industry 4.0, which aims to solve the problem of big data, reduce energy consumption in industrial sensor networks, improve the security, processing and storage real-time data. It is a promising growing paradigm that offers new opportunities and challenges, beside the ones inherited from cloud computing, which requires a new heterogeneous architecture to improve the network capacity for delivering edge services, that is, providing computing resources closer to the end user. The purpose of this research is to show a systematic review of the most recent studies about the architecture, security, latency, and energy consumption that FC presents at industrial level and thus provide an overview of the current characteristics and challenges of this new technology.}
}
@article{SOUMELIDIS2018361,
title = {Cloud Aided Implementation of Energy Optimal Look-ahead Speed Control},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {9},
pages = {361-366},
year = {2018},
note = {15th IFAC Symposium on Control in Transportation Systems CTS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.07.059},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318307821},
author = {Alexandros Soumelidis and Péter Gáspár and Ádám Kisari and Ádám Bakos and Balázs Németh and András Mihály and Zoltán Hankovszki},
keywords = {Automotive control, Optimal control, Look-ahead, ADAS, Cloud},
abstract = {The aim of the paper is to present the development, implementation and testing of a novel look-ahead cruise control system. The method strives to decrease fuel consumption while maintaining travel time limitations. In the development phase the control algorithm has been evaluated and fine tuned using state of the art software- and hardware in the loop simulations. The real-world implementation and tests were conducted on Hungarian highways with a Volvo FH13 truck. The test results confirmed the simulations and proved that a reduction in fuel consumption can be achieved this way.}
}
@article{BOCCALI2019100034,
title = {Computing models in high energy physics},
journal = {Reviews in Physics},
volume = {4},
pages = {100034},
year = {2019},
issn = {2405-4283},
doi = {https://doi.org/10.1016/j.revip.2019.100034},
url = {https://www.sciencedirect.com/science/article/pii/S2405428319300449},
author = {Tommaso Boccali},
abstract = {High Energy Physics Experiments (HEP experiments in the following) have been at least in the last 3 decades at the forefront of technology, in aspects like detector design and construction, number of collaborators, and complexity of data analyses. As uncommon in previous particle physics experiments, the computing and data handling aspects have not been marginal in their design and operations; the cost of the IT related components, from software development to storage systems and to distributed complex e-Infrastructures, has raised to a level which needs proper understanding and planning from the first moments in the lifetime of an experiment. In the following sections we will first try to explore the computing and software solutions developed and operated in the most relevant past and present experiments, with a focus on the technologies deployed; a technology tracking section is presented in order to pave the way to possible solutions for next decade experiments, and beyond. While the focus of this review is on offline computing model, the distinction is a shady one, and some experiments have already experienced contaminations between triggers selection and offline workflows; it is anticipated the trend will continue in the future.}
}
@article{LIU2020104692,
title = {Using the binomial model for the valuation of real options in computing optimal subsidies for Chinese renewable energy investments},
journal = {Energy Economics},
volume = {87},
pages = {104692},
year = {2020},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2020.104692},
url = {https://www.sciencedirect.com/science/article/pii/S0140988320300311},
author = {Xiaoran Liu and Ehud I. Ronn},
keywords = {Real options in energy markets, Using the binomial model to value American-style options},
abstract = {For the valuation and implementation of renewable energy investments, the issue of providing private investors with a financial incentive to accelerate their investment is frequently a critical component. We apply this principle to the Chinese context. This paper focuses on using the binomial model to compute the required subsidy that would incentivize investors to optimal immediate exercise of the American-style option embedded at the launching phase of the projects for Chinese renewable energy investments. In addition, this paper also aims at contrasting the binomial model with the more-laborious Monte Carlo simulation previously used to evaluate the proper subsidy. By using the same data but a different method, and reducing the number of uncertain factors to one, it is suggested these two methods have similar outcomes but the binomial method requires substantially less computation and is more self-explanatory. This paper thus provides government with an easy-to-implement alternative way to compute the required subsidy.}
}
@article{ALARIFI2019133,
title = {Optimizing the network energy of cloud assisted internet of things by using the adaptive neural learning approach in wireless sensor networks},
journal = {Computers in Industry},
volume = {106},
pages = {133-141},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518307784},
author = {Abdulaziz Alarifi and Amr Tolba},
keywords = {Cloud-assisted internet of things (CIoT), Wireless sensor networks, Energy optimization, Adaptive neural learning, Principal component analysis, Routing protocol},
abstract = {Cloud-assisted internet of things (CIoT) is backboned by the wireless sensor network (WSN) architecture. A sensor network is an autonomous self-resource constraint collection of internet of things (IoT) sensor nodes. The nodes communicate in an ad-hoc fashion to transfer cloud information over the virtual environment. Clustering in WSNs helps to improve the quality of the network by controlling energy consumption and improving data gathering accuracy. This improves the service rates of CIoT. Optimizing IoT sensor networks through energy and overhead management requires complex clustering algorithms. A simple clustering scheme cannot achieve the desired performance enhancement during transmission in a virtual environment. This research attempts to propose a reinforcement-based learning technique, adaptive Q-learning (AQL) to improve network performance with minimum energy–overhead tradeoff in a sensor network-aided CIoT. AQL operates in two distinct phases for cluster head selection and forwarder selection. The decision-making system is used to qualify nodes based on their past behavior over transmission. AQL improves both inter- and intra-cluster communication optimization through adaptive forwarder and header selection conditions. The simulation results prove the consistency of the proposed AQL by retaining the live node counts in the network and their persistent energy despite the reduced overheads in the sensor network. With the achievement of constructive features in the sensor networks, the performance of CIoT is considerably improved. The experimental results illustrate the effectiveness of the proposed learning technique by improving network lifetime with a high request–response rate and by minimizing delay, overhead, and request failures.}
}
@article{BOZORGCHENANI2020577,
title = {An energy harvesting solution for computation offloading in Fog Computing networks},
journal = {Computer Communications},
volume = {160},
pages = {577-587},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.06.032},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420305107},
author = {Arash Bozorgchenani and Simone Disabato and Daniele Tarchi and Manuel Roveri},
keywords = {Fog Computing, Energy harvesting, Computation offloading, Clustering, Internet of Things},
abstract = {Fog Computing is a promising networking paradigm enabling the nodes at the edge to share computational and storage resources. Being pervasively distributed, Fog Nodes are often battery powered and, for this reason, an efficient energy management should be considered to prolong network lifetime. In this paper, we introduce a smart energy management solution able to exploit information about the predicted harvested and consumed energy by Fog Nodes, equipped with small solar panels. The smart energy management is applied on a cluster based Fog Computing environment where computation offloading operations are performed. In the experimental section the effect of the smart energy management is explored in terms of network lifetime by considering variable battery size and Fog Nodes density in a realistic solar-panel harvesting-model and Fog Nodes setting.}
}
@article{JAVIED2019171,
title = {Cloud based Energy Management System Compatible with the Industry 4.0 Requirements},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {10},
pages = {171-175},
year = {2019},
note = {13th IFAC Workshop on Intelligent Manufacturing Systems IMS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319308730},
author = {T. Javied and S. Huprich and J. Franke},
keywords = {Energy management, Cloud, ISO 50001, Resource Efficiency, Industry 4.0},
abstract = {As a result of climate change and continuously rising energy costs, energy as a resource is becoming increasingly important for companies in German industry. The standards of modern companies are increasingly being enhanced by values such as energy efficiency and energy awareness in order to achieve a competitive advantage through the skillful use of resources. Energy audits and energy management systems should lead to a sustainable use of energy. This development is additionally driven by current legal requirements of energy policy which leads to the development of a new area in the market involving the sale of energy management systems and energy services in general. However, only a few system solutions fulfil all core aspects of meaningful energy management according to DIN EN ISO 50001. This paper describes a platform-independent, cloud-based web application that implements energy management in a standardized and user-friendly manner. As part of an overall solution, the multiuser system supports role-based planning, implementation, logging and archiving of the necessary work steps that emerge from the implementation of the standard. Modern communication technologies and transmission protocols enable the flexible connection of different manufacturing systems, so that energy management can fully be integrated into an industry 4.0 production environment.}
}
@article{BAKER2018242,
title = {Cloud-SEnergy: A bin-packing based multi-cloud service broker for energy efficient composition and execution of data-intensive applications},
journal = {Sustainable Computing: Informatics and Systems},
volume = {19},
pages = {242-252},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2018.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S2210537918300398},
author = {Thar Baker and Bandar Aldawsari and Muhammad Asim and Hissam Tawfik and Zakaria Maamar and Rajkumar Buyya},
keywords = {Multi-cloud, Bin-packing, Service composition, Energy efficiency, Data-intensive application},
abstract = {The over-reliance of today's world on information and communication technologies (ICT) has led to an exponential increase in data production, network traffic, and energy consumption. To mitigate the ecological impact of this increase on the environment, a major challenge that this paper tackles is how to best select the most energy efficient services from cross-continental competing cloud-based datacenters. This selection is addressed by our Cloud-SEnergy, a system that uses a bin-packing technique to generate the most efficient service composition plans. Experiments were conducted to compare Cloud-SEnergy's efficiency with 5 established techniques in multi-cloud environments (All clouds, Base cloud, Smart cloud, COM2, and DC-Cloud). The results gained from the experiments demonstrate a superior performance of Cloud-SEnergy which ranged from an average energy consumption reduction of 4.3% when compared to Based Cloud technique, to an average reduction of 43.3% when compared to All Clouds technique. Furthermore, the percentage reduction in the number of examined services achieved by Cloud-SEnergy ranged from 50% when compared to Smart Cloud and average of 82.4% when compared to Base Cloud. In term of run-time, Cloud-SEnergy resulted in average reduction which ranged from 8.5% when compared to DC-Cloud, to 28.2% run-time reduction when compared to All Clouds.}
}
@article{KAMADA2020100212,
title = {Three cases of pulmonary tumor thrombotic microangiopathy (PTTM): Challenge in antemortem diagnosis using lung perfusion blood volume images by dual-energy computed tomography},
journal = {European Journal of Radiology Open},
volume = {7},
pages = {100212},
year = {2020},
issn = {2352-0477},
doi = {https://doi.org/10.1016/j.ejro.2020.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352047720300010},
author = {Hiroki Kamada and Hideki Ota and Yosuke Terui and Koichiro Sugimura and Shigefumi Fukui and Hiroaki Shimokawa and Kei Takase},
keywords = {Pulmonary tumor thrombotic microangiopathy (PTTM), Lung perfused blood volume (PBV) images, Dual-energy computed tomography (CT), Pulmonary hypertension},
abstract = {Pulmonary tumor thrombotic microangiopathy (PTTM) is a specific type of tumor embolism in the small and medium pulmonary arteries, leading to rapid progressive pulmonary hypertension. Antemortem diagnosis of PTTM is extremely difficult. We encountered three patients who were histopathologically or clinically diagnosed with PTTM. In all cases, lung perfused blood volume (PBV) images on dual-energy computed tomography (CT) demonstrated multiple subpleural wedge-shaped defects with no evidence of pulmonary embolism on CT pulmonary angiography. The lung PBV images demonstrated small pulmonary arterial obstruction reflecting the pathology of PTTM. Therefore, lung PBV imaging would be useful for antemortem diagnosis of PTTM.}
}
@article{SHAHID2020534,
title = {Energy and delay efficient fog computing using caching mechanism},
journal = {Computer Communications},
volume = {154},
pages = {534-541},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S014036641931446X},
author = {Muzammil Hussain Shahid and Ahmad Raza Hameed and Saif {ul Islam} and Hasan Ali Khattak and Ikram Ud Din and Joel J.P.C. Rodrigues},
keywords = {Energy efficiency, Fog computing, Caching, Load balancing},
abstract = {Fog computing has emerged as an extension to the existing cloud infrastructure for providing latency-aware and highly scalable services to geographically distributed end devices. The addition of the fog layer in the cloud computing paradigm helps to improve the quality of service (QoS) in time-critical and delay-sensitive applications. Due to the continuous increase in the deployment of fog networks at large scale, energy efficiency is a significant issue in the fog computing paradigm to reduce the service cost and to protect the environment. A plethora of research has been conducted to reduce energy consumption in fog computing, majorly, focusing on the scheduling of incoming jobs to improve energy efficiency. However, node-level mechanisms have largely been neglected. Cache placement is a critical issue in fog networks for efficient content distribution to clients, which requires simultaneous consideration of many factors including quality of network connection, the demand for contents, and users’ activities. In this paper, a popularity-based caching mechanism in content delivery fog networks is proposed. In this context, two energy-aware mechanisms, i.e., content filtration and load balancing, have been applied. In the proposed approach, popular contents are found using random distribution and these contents are categorized into three classes. After finding the file popularity, an active fog node is selected based on the number of neighbors, energy level, and operational power. Further, the popular content is cached on the active node using a filtration mechanism. Moreover, a load-balancing algorithm is proposed to increase the overall system efficiency in the cached fog network. The evaluation of the proposed approach exhibits promising results in terms of energy consumption and latency. The proposed scheme consumes 92.6% and 82.7% less energy in comparison to without caching and simple caching mechanisms, respectively. Similarly, an improvement of 85.29% and 67.4% in delay has also been noticed while using advance caching against the without caching and simple caching techniques, respectively.}
}
@article{RAMIREZCUESTA2020104770,
title = {METRIC-GIS: An advanced energy balance model for computing crop evapotranspiration in a GIS environment},
journal = {Environmental Modelling & Software},
volume = {131},
pages = {104770},
year = {2020},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2020.104770},
url = {https://www.sciencedirect.com/science/article/pii/S1364815220304539},
author = {J.M. Ramírez-Cuesta and R.G. Allen and D.S. Intrigliolo and A. Kilic and C.W. Robison and R. Trezza and C. Santos and I.J. Lorite},
keywords = {Crop coefficient, Evapotranspiration, Modelling, Remote sensing, Satellite, Water requirements},
abstract = {A novel ArcGIS toolbox that applies the Mapping Evapotranspiration with Internalized Calibration model was developed and tested in a semi-arid environment. The tool, named METRIC-GIS, facilitates the pre-processing operations and the automatic identification of potential calibration and pixels review. The energy balance components obtained from METRIC-GIS were contrasted with those from the original METRIC version (R2 = 1; RMSE = 0 W m−2 or mm day−1 for ETc) Additionally, an irrigated scheme located at southern Spain was considered for assessing Kc variability in the maize fields with METRIC-GIS. The identified spatial variability was mainly due to differences in irrigation regimes, crop management practices, and planting and harvesting dates. This information is critical for developing irrigation advisory strategies that contribute to the area sustainability. The developed tool facilitates data input introduction and reduces computational time by up to 50%, providing a more user-friendly alternative to other existing platforms that use METRIC.}
}
@article{MAZOUZI2019132,
title = {Maximizing mobiles energy saving through tasks optimal offloading placement in two-tier cloud: A theoretical and an experimental study},
journal = {Computer Communications},
volume = {144},
pages = {132-148},
year = {2019},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419301720},
author = {Houssemeddine Mazouzi and Khaled Boussetta and Nadjib Achir},
keywords = {Computation offloading, Mobile cloud computing, Mobile edge computing, Cloudlet, Lagrangian decomposition, Offloading middleware},
abstract = {In this paper, we focus on tasks offloading over two tiered mobile edge computing environment. We consider several users with energy constrained tasks that can be offloaded over edge clouds (cloudlets) or on a remote cloud with differentiated system and network resources capacities. We investigate offloading policy that decides which tasks should be offloaded and determine the offloading location on the cloudlets or on the cloud. The objective is to minimize the total energy consumed by the users. We formulate this problem as a Non-Linear Binary Integer Programming. Since the centralized optimal solution is NP-hard, we propose a distributed linear relaxation heuristic based on Lagrangian decomposition approach. To solve the subproblems, we also propose a greedy heuristic that computes the best cloudlet selection and bandwidth allocation following tasks’ energy consumption. We compared our proposal against existing approaches under different system parameters (CPU resources), variable number of users and for six applications, each having specific traffic pattern, resource demands and time constraints. Numerical results show that our proposal outperforms existing approaches. In addition to the theoretical approach, we evaluate our offloading policy using real experiments. In this case, we setup a real testbed composed of client terminal, offloading server located either at the edge or at a remote Cloud. We also implemented our proposal as an offloading middleware on both the client and the offloading server. Using this testbed, we were able to evaluate our offloading decision policy for multi-users context with three real Android OS applications, with different traffic patterns and resource demands. We also discuss the performance of our proposal for each application and we analyze the multi-users effect.}
}
@article{POURREZA200762,
title = {Spectrophtometric determination of malachite green in fish farming water samples after cloud point extraction using nonionic surfactant Triton X-100},
journal = {Analytica Chimica Acta},
volume = {596},
number = {1},
pages = {62-65},
year = {2007},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2007.05.042},
url = {https://www.sciencedirect.com/science/article/pii/S0003267007009543},
author = {N. Pourreza and Sh. Elhami},
keywords = {Cloud point extraction, Malachite green, Spectrophotometry, Triton X-100},
abstract = {A novel and sensitive cloud point extraction procedure for the determination of trace amounts of malachite green by spectrophotometry was developed. Malachite green was extracted at pH 2.5 mediated by micelles of nonionic surfactant Triton X-100. The extracted surfactant-rich phase was diluted with ethanol and its absorbance was measured at 630nm. The effect of different variables such as pH, Triton X-100 concentration, cloud point temperature and time and diverse ions was investigated and optimum conditions were established. The calibration graph was linear in the range of 4–500ngmL−1 of malachite green in the initial solution with r=0.9996 (n=10). Detection limit based on three times the standard deviation of the blank (3Sb) was 1.2ngmL−1 and the relative standard deviation (R.S.D.) for 20 and 300ngmL−1 of malachite green was 1.48 and 1.13% (n=8), respectively. The method was applied to the determination of malachite green in different fish farming and river water samples.}
}
@article{YAMAMOTO2013933,
title = {Intraoperative detection of sentinel lymph nodes in breast cancer patients using ultrasonography-guided direct indocyanine green dye-marking by real-time virtual sonography constructed with three-dimensional computed tomography-lymphography},
journal = {The Breast},
volume = {22},
number = {5},
pages = {933-937},
year = {2013},
issn = {0960-9776},
doi = {https://doi.org/10.1016/j.breast.2013.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0960977613001045},
author = {Shigeru Yamamoto and Noriko Maeda and Kiyoshi Yoshimura and Masaaki Oka},
keywords = {Breast cancer, Sentinel lymph node, Computed tomography, Lymphography, Real-time virtual sonography},
abstract = {Purpose
This study aims to determine the utility of ultrasonography (US)-guided direct dye-marking of sentinel lymph nodes (SLNs) by real-time virtual sonography (RVS) constructed with three-dimensional (3D) computed tomography (CT)-lymphography (LG).
Patients and methods
We identified SLNs in 258 clinically node-negative breast cancer patients using an RVS system to display in real time a virtual multiplanar reconstruction CT image obtained from CT volume data corresponding to the same cross-sectional image from US. CT volume data were obtained using our original 3D CT-LG, which accurately detects SLNs in breast cancer. We then perform US-guided dye-marking close to SLNs using indocyanine green (ICG). Subsequently, indigo carmine blue dye was injected into the subareolar and peritumoral areas around each primary tumor. All patients underwent SLN biopsy and SLN metastases were examined pathologically.
Results
In all 258 patients, we were able to detect the same SLNs visualized on 3D CT-LG, using the RVS system. We detected ICG close to SLNs in 257 of 258 patients (99.6%) during SLN biopsy. In 25 patients (9%), we failed to follow the blue lymphatic route stained by indigo carmine and SLNs were not stained by indigo carmine, but easily detected SLNs by ICG marking.
Conclusion
US-guided direct ICG dye-marking of SLNs using this RVS system seems useful for the detection of SLNs, allowing easy detection of SLNs even when the stained lymphatic route is not followed.}
}
@article{CORRIAS2019108698,
title = {Dual energy computed tomography analysis in cancer patients: What factors affect iodine concentration in contrast enhanced studies?},
journal = {European Journal of Radiology},
volume = {120},
pages = {108698},
year = {2019},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2019.108698},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X19303481},
author = {Giuseppe Corrias and Peter Sawan and Usman Mahmood and Junting Zheng and Marinela Capanu and Marco Salvatore and Giacomo Spinato and Luca Saba and Lorenzo Mannelli},
keywords = {DECT, Iodine, Oncologic patients, Follow-up},
abstract = {Purpose
The aim of the study is to explore the patient's and scan's parameters that affect the iodine concentration in the abdomen using dual energy computed tomography (DECT) in an oncologic population.
Method
This is a retrospective study with consecutive patients with different cancers who underwent a single-source DECT (ssDECT) examinations at our institution between years 2015 and 2017. On axial IODINE images, the radiologist manually drew a circular ROI along the inner contour of the aorta. Mean iodine concentration and ROI areas were recorded. Body mass index for every patient was recorded. Descriptive statistics were summarized for iodine concentration and patient/scan characteristics. Linear regression was used to examine associations between iodine concentration in aorta and studied characteristics. Statistical significance was set at a p value < 0.05.
Results
The univariate analysis, showed a statistically significant association between iodine concentration within the aorta and the area of ROI (Estimated Coefficient β: −0.013), the rate of injection (Estimated Coefficient β: 2.09), the acquisition time (Estimated Coefficient β: −0.195). In multivariable analysis iodine concentration in the aorta increased with higher rate of injection (4 ml/sec), smaller ROI area and lower BMI.
Conclusion
Our results showed how iodine concentration is highly dependent on some intrinsic and extrinsic parameters of the examination. These parameters should be taken into account since lower concentration of iodine decrease contrast-to-noise ratio, and in longitudinal follow up studies, they would affect iodine quantitive assessments in cancer patients with frequent chemotherapy-induced variations in BMI and cardiac function.}
}
@article{CATANOJIMENEZ2020115100,
title = {Dual-energy estimates of volumetric bone mineral densities in the lumbar spine using quantitative computed tomography better correlate with fracture properties when compared to single-energy BMD outcomes},
journal = {Bone},
volume = {130},
pages = {115100},
year = {2020},
issn = {8756-3282},
doi = {https://doi.org/10.1016/j.bone.2019.115100},
url = {https://www.sciencedirect.com/science/article/pii/S875632821930393X},
author = {Simon {Cataño Jimenez} and Sebastian Saldarriaga and Christopher D. Chaput and Hugo Giambini},
keywords = {QCT, Vertebral fracture, Prediction, Osteoporosis, DXA},
abstract = {It is estimated that over 200 million people worldwide are affected by osteoporosis. Vertebral fracture risk prediction using dual energy x-ray absorptiometry (DXA) is confounded by limitations of the technology, such as 2D measurements of bone mineral density (BMD), inability to measure bone distribution and heterogeneity, and potential overestimations of BMD due to degenerative diseases. To overcome these shortcomings, single energy (SE) quantitative computed tomography (QCT) imaging estimates of Hounsfield units (HU) and volumetric BMD have been implemented as alternative methodologies for assessing fracture risk. However, marrow fat within the vertebrae can highly affect the vBMD and fracture properties estimations. To address this issue, 54 vertebrae were dissected from nine cadaveric spines and scanned using SE-QCT (120kVp) and dual energy (DE)-QCT (80/140 kVp), with the latter accounting for marrow fat within the vertebrae. The vertebrae were then scanned using DXA and subjected to mechanical testing to obtain fracture properties. aBMD outcomes from DXA showed a better correlation with DE-QCT vBMD versus SE outcomes [DE: aBMD vs. vBMD (R2: 0.61); SE: aBMD vs. vBMD (R2: 0.27)]. SE-QCT underestimated vertebral vBMD by -56% (p<0.0001) when compared to DE-QCT. vBMD estimates from SE-QCT could predict 45% and 37% of the vertebral failure loads and stiffness, respectively, compared to 67% and 46% from DE-QCT. DE-QCT vBMD outcomes highly correlated with fracture properties of vertebrae as compared to SE-QCT metrics. As DE scanning has the ability to correct for the effects of bone marrow fat, estimated vBMD from SE-QCT were significantly underestimated compared to DE-QCT. Dual energy CT scanning has the potential to more accurately predict vertebral failure and aid the clinician in the evaluation of appropriate interventions. Future studies should consider implementing DE-QCT in their fracture assessment.}
}
@article{CHEN2020287,
title = {Emerging Imaging Techniques in Spondyloarthritis: Dual-Energy Computed Tomography and New MRI Sequences},
journal = {Rheumatic Disease Clinics of North America},
volume = {46},
number = {2},
pages = {287-296},
year = {2020},
note = {Spondyloarthritis: The Changing Landscape Today},
issn = {0889-857X},
doi = {https://doi.org/10.1016/j.rdc.2020.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0889857X20300107},
author = {Min Chen and Paul Bird and Lennart Jans},
keywords = {Spondyloarthritis, Dual-energy computed tomography, MRI, Sacroiliac joints}
}
@article{LI2019152,
title = {Energy-efficient fault-tolerant replica management policy with deadline and budget constraints in edge-cloud environment},
journal = {Journal of Network and Computer Applications},
volume = {143},
pages = {152-166},
year = {2019},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2019.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S1084804519301420},
author = {Chunlin Li and YaPing Wang and Yi Chen and Youlong Luo},
keywords = {Replica management, Energy-efficient, Fault-tolerant, Edge-cloud environment},
abstract = {With the development of large-scale distributed systems such as grids and clouds, data replication management has become a hot research topic. Although replica management can improve the cluster system performance, it also brings a series of management and overhead issues. Therefore, the energy-efficient fault-tolerant replica management policy with the deadline and budget constraints in the edge-cloud environment is proposed. The experiments show that the proposed dynamic replica placement algorithm can effectively reduce the mean job time, reduce the use of network bandwidth and improve the utilization of storage space. Considering the issue of energy efficiency, the energy-aware cluster scaling strategy is proposed to reduce system energy consumption and achieve energy efficiency by sleeping and waking up the data nodes according to the load state of the system. Besides, in order to avoid access failures and data loss caused by node failures, the node failure recovery method based on availability metrics is used to deal with node failures. The experiments show that the performance of the proposed algorithm is better than the other algorithms in terms of energy efficiency and fault tolerance.}
}
@article{BINGHAM201615,
title = {A note on the relative efficiency of methods for computing the transient free-surface Green function},
journal = {Ocean Engineering},
volume = {120},
pages = {15-20},
year = {2016},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2016.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S0029801816301317},
author = {Harry B. Bingham},
keywords = {Time-domain Green function, Integration of ODEs, Wave–structure interaction, Marine hydrodynamics},
abstract = {A number of papers have appeared recently on computing the time-domain, free-surface Green function. Two papers in particular, Chuang et al. (2007) and Li et al. (2015) considered the method developed by Clement (1998) who showed that this Green function is the solution to a fourth-order Ordinary Differential Equation (ODE). This ODE has been suggested as a means for speeding up the calculation of the Green function coefficients compared to the standard algorithms developed for example by Newman (1992). Clement solved the ODE using the classical fourth-order, four-step Runge–Kutta scheme (RK44) with a fixed time step size. The two papers mentioned above proposed alternative numerical methods which are claimed to be more efficient. In this note we consider the relative efficiency of these four methods on a representative test case, and conclude that the standard method is the most efficient. Of the ODE-based methods, the method of Chuang et al. (2007) is found to be slightly more efficient than the RK44 method, while the method of Li et al. (2015) is at least an order of magnitude less efficient. It is also pointed out that ODE methods have yet to be extended to include finite water depth.}
}
@article{GHOSH2020100166,
title = {Energy-Efficient IoT-Health Monitoring System using Approximate Computing},
journal = {Internet of Things},
volume = {9},
pages = {100166},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100166},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520300093},
author = {Avrajit Ghosh and Arnab Raha and Amitava Mukherjee},
keywords = {Wireless Body Sensor Nodes, IoT-based Health Monitoring, Low Power Hardware Prototype, ECG Signal, Discrete Wavelet Transform, Sparse Encoding, Approximate Computing},
abstract = {Wireless Body Sensor Nodes (WBSN) are frequently used for real time IoT-based health monitoring of patients outside the hospital environment. These WBSNs involve bio-sensors to capture signals from a patient’s body and wireless transmitters to transmit the collected signals to a server located in private/public cloud in real time. These WBSNs include hardware for processing of signals before being transmitted to the cloud. Simultaneous occurrence of all these processes inside energy constrained WBSNs results in considerable amount of power consumption, thus limiting their operational lifetime. Due to the inherent error-resilience in signal processing algorithms, most of these data reaching the servers are redundant in nature and hence of not much clinical importance. Transmission and storage of these excess data result in inefficient usages of transmission bandwidth and storage capabilities. In this paper, we develop a real time encoding scheme that performs iterative thresholding and approximation of wavelet coefficients for sparse encoding of bio-signals (ECG signals), thereby reducing the energy and bandwidth consumption of the WBSN. The encoding scheme compresses bio-signals (ECG signals), while still maintaining the clinically important features. We optimize various process parameters to model a low power hardware prototype for the implementation of our algorithm on a real time microcontroller based IoT platform that operates as an end-to-end WBSN system in real time. Experimental results show a system-level energy improvement of 96% with a negligible impact on signal quality (2%).}
}
@article{WU2020556,
title = {Energy efficient for UAV-enabled mobile edge computing networks: Intelligent task prediction and offloading},
journal = {Computer Communications},
volume = {150},
pages = {556-562},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419305481},
author = {Gaoxiang Wu and Yiming Miao and Yu Zhang and Ahmed Barnawi},
keywords = {5G, LSTM, Mobile edge computing, Task offloading, Unmanned arial vehicle},
abstract = {Mobile edge computing (MEC) network provides near-users computing and communication functions and has become a potential 5G evolutionary architecture. In order to overcome the shortcomings of the existing MEC network in fixed base stations and limited computing resources, unmanned arial vehicle (UAV) is introduced as a relay edge computing node and UAV-enabled MEC networks are proposed. However, UAVs have limited energy. Thus, energy consumption would be an optimal target during the information interaction. Therefore, an energy efficiency optimization algorithm based on a three-layer computation offloading strategy is proposed in this paper by combining the UAV position optimization algorithm and the LSTM-based task prediction algorithm. The experiments show that the computation offloading strategy of the UAV-enabled MEC network can be dynamically programmed with the proposed algorithm and architecture, according to the required delay, UAV height, and data size in order to effectively reduce the energy consumption of the UAV.}
}
@article{MISHRA201848,
title = {Energy-efficient VM-placement in cloud data center},
journal = {Sustainable Computing: Informatics and Systems},
volume = {20},
pages = {48-55},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2018.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917302536},
author = {Sambit Kumar Mishra and Deepak Puthal and Bibhudatta Sahoo and Prem Prakash Jayaraman and Song Jun and Albert Y. Zomaya and Rajiv Ranjan},
keywords = {Cloud computing, Energy consumption, VM consolidation, Task scheduling, Makespan},
abstract = {Employing cloud computing to acquire the benefit of cloud by optimizing various parameters that meet changing demands is a challenging task. The optimal mapping of tasks to virtual machines (VMs) and VMs to physical machines (PMs) (known as VM placement) problem are necessary for advancing energy consumption and resource utilization. High heterogeneity of tasks as well as resources, great dynamism and virtualization make the consolidation issue more complicated in the cloud computing system. In this paper, a complete mapping (i.e., task VM and VM to PM) algorithm is proposed. The tasks are classified according to their resource requirement and then searching for the appropriate VM and again searching for the appropriate PM where the selected VM can be deployed. The proposed algorithm reduces the energy consumption by depreciating the number of active PMs, while also minimizes the makespan and task rejection rate. We have evaluated our proposed approach in CloudSim simulator, and the results demonstrate the effectiveness of the proposed algorithm over some existing standard algorithms.}
}
@article{TUCKER201654,
title = {Thermodiffusion in liquid binary alloys computed from molecular-dynamics simulation and the Green-Kubo formalism},
journal = {Computational Materials Science},
volume = {124},
pages = {54-61},
year = {2016},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2016.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0927025616303317},
author = {William C. Tucker and Patrick K. Schelling},
keywords = {Thermodiffusion, Liquid alloys, Molecular dynamics, Green–Kubo method},
abstract = {In the presence of a temperature gradient, the components of a binary liquid tend to segregate. This phenomenon, generally referred to as thermodiffusion or the Soret effect, is usually quantified by the heat of transport. We report heat of transport values Qc∗ for NiAl and NiCu melts computed using molecular-dynamics simulation and the Green-Kubo formalism. Thermal conductivities are also reported. To develop a clear picture of the phenomena, we determined contributions to Qc∗ due to the convective and virial components of the heat current, which were then compared to the related terms in the partial enthalpy. It is shown that the contribution to Qc∗ from the convective component of the heat current is comparable to the average energy of the diffusing atoms, differing by an amount comparable to the activation energy for diffusion. The contribution to Qc∗ from the virial heat current is closely related to the pressure-volume term pcΩ in the partial enthalpy. It is established that the virial heat current plays a dominant role in determining the sign of the reduced heat of transport Qc∗′=Qc∗-hc. By comparing results obtained with different empirical potentials, a trend emerges. Specifically, it is found that the sign of the reduced heat of transport is correlated with the sign of the partial pressure associated with the low-mass component. It is also shown that two different empirical potentials for the NiAl system give vastly different results for Qc∗′. The results indicate that in developing a potential that might accurately predict Qc∗′, the distribution of the partial energy and partial pressure between the two components is critical. Based on these observations, it would appear that existing empirical potentials may not be able to generate reliable predictions for Qc∗′ without additional validation.}
}
@article{A2019139,
title = {Energy efficient VM scheduling and routing in multi-tenant cloud data center},
journal = {Sustainable Computing: Informatics and Systems},
volume = {22},
pages = {139-151},
year = {2019},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2019.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2210537918303160},
author = {Sudarshan Chakravarthy A and Sudhakar Ch and Ramesh T},
keywords = {Cloud computing, Energy efficiency, VM scheduling, Routing},
abstract = {Cloud data center hosting many composite applications of multiple tenants consumes a massive amount of energy. Developing energy efficient mechanisms for data center management has become a vital issue for the cloud providers. Most of the works that deal with mechanisms for energy-efficient resource provisioning focus either on reducing the energy consumption of servers or reducing the energy consumption of network elements, but not both. We have formulated the problem of jointly optimizing the energy consumption of servers and network elements by optimal VM scheduling and routing as an integer programming problem. To solve it a phase-wise optimization approach with two ant colony based meta-heuristic algorithms is proposed. The topology features of the data center network and the communication patterns of the applications are considered in the construction of the solution. The solution is tested for three standard data center networks, 3-tier, B-Cube and Hyper-tree of different sizes and compared against two standard algorithms, first-fit and round-robin algorithms. The results showed that the proposed solution improves energy savings by 15% and 20% on an average when compared with first-fit and round-robin respectively.}
}
@article{COZZI2022S1200,
title = {PO-1416 Dual Energy Computed Tomography Applications for Prostate Radiotherapy: Advantages in Target and OAR Contouring},
journal = {Radiotherapy and Oncology},
volume = {170},
pages = {S1200-S1201},
year = {2022},
note = {ESTRO 2022, 6-10 May 2022, Copenhagen. Onsite in Copenhagen and Online},
issn = {0167-8140},
doi = {https://doi.org/10.1016/S0167-8140(22)03380-1},
url = {https://www.sciencedirect.com/science/article/pii/S0167814022033801},
author = {S. Cozzi and A. Botti and G. Blandino and L. Bardoscia and G. Timon and M.P. Ruggieri and M. Manicone and G. Sceni and P. Ciammella and C. Iotti}
}
@article{LIN201966,
title = {A cloud-based energy data mining information agent system based on big data analysis technology},
journal = {Microelectronics Reliability},
volume = {97},
pages = {66-78},
year = {2019},
issn = {0026-2714},
doi = {https://doi.org/10.1016/j.microrel.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0026271419301064},
author = {Hsueh-Yuan Lin and Sheng-Yuan Yang},
keywords = {Data mining agent systems, Big Data analysis, Web services, Energy saving agent systems},
abstract = {2019 is the first year of 5G and the information flow is growing even more; therefore, data mining technology is one of the key technologies regarding how to find useful information from the vast information flow. This paper aims to develop the cloud-based energy data mining information agent system OntoDMA, as based on the WIAS cloud environment and Big Data analysis technology, which is embedded in a cloud-based active multi-agent system to proactively provide appropriate, real-time, and fast domain information prediction. On one hand, the related technologies for constructing web service platforms are shared; on the other hand, how to widely and seamlessly integrate and support the cloud interaction paradigm handled by the data mining agent system through these technologies is explored. In order to outline the feasibility of the proposed system architecture, a case study is conducted on the energy saving information system, and the relevant R&D results are presented in detail. Then, both the preliminary system R&D interface and experimental verification are illustrated. Finally, the cache performance of the Solutions Pool is increased by 19.82%, the query workload of the Prediction Rules is reduced by 66.51%, and the overall operating time is decreased by 5.21%, which effectively and efficiently relieves the workload on the back-end servo system.}
}
@article{ORSINI2019121,
title = {Efficient Mobile Clouds: Forecasting the Future Connectivity of Mobile and IoT Devices to Save Energy and Bandwidth},
journal = {Procedia Computer Science},
volume = {155},
pages = {121-128},
year = {2019},
note = {The 16th International Conference on Mobile Systems and Pervasive Computing (MobiSPC 2019),The 14th International Conference on Future Networks and Communications (FNC-2019),The 9th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919309342},
author = {Gabriel Orsini and Wolf Posdorfer and Winfried Lamersdorf},
keywords = {Mobile Clouds, Internet of Things, Context Awareness, Context Forecast},
abstract = {Use cases in the Internet of Things (IoT) and in mobile clouds often require the interaction of one or more mobile devices with their infrastructure to provide users with services. Ideally, this interaction is based on a reliable connection between the communicating devices, which is often not the case. Since most use cases do not adequately address this issue, service quality is often compromised. Aimed to address this issue, this paper proposes a novel approach to forecast the connectivity and bandwidth of mobile devices by applying machine learning to the context data recorded by the various sensors of the mobile device. This concept, designed as a microservice, has been implemented in the mobile middleware CloudAware, a system software infrastructure for mobile cloud computing that integrates easily with mobile operating systems, such as Android. We evaluate our approach with real sensor data and show how to enable mobile devices in the IoT to make assumptions about their future connectivity, allowing for intelligent and distributed decision making on the mobile edge of the network.}
}
@article{DEBNATH2016669,
title = {E-Waste Management – A Potential Route to Green Computing},
journal = {Procedia Environmental Sciences},
volume = {35},
pages = {669-675},
year = {2016},
note = {Waste Management for Resource Utilisation},
issn = {1878-0296},
doi = {https://doi.org/10.1016/j.proenv.2016.07.063},
url = {https://www.sciencedirect.com/science/article/pii/S1878029616301529},
author = {Biswajit Debnath and Reshma Roychoudhuri and Sadhan K. Ghosh},
keywords = {Green Computing, E-waste Management, Sustainability},
abstract = {Green computing is a recent trend and an evolving field aiming towards a sustainable future. Different approaches have been established as possible directions towards green computing. Virtualization, Cloud computing, Energy minimization, reduction in use of hazardous substances in electronic items etc are a few approaches. Though major focus has shifted towards energy minimization, other approaches have churned themselves out as new subjects – cloud computing. Green computing encompasses a wide range of things. One of them is e-waste management. End of life electronic equipments known as e-waste is a threat to the whole world. Globally 41.8 million metric tonnes of e-waste was generated in 2014. These electronic items are the hardware part of the computer. The proper management of e-waste hence leaves a good potential to implement green computing. An overwhelming amount of research articles focusing on green computing are present and major focus is on energy minimization, efficient algorithms and cloud computing. Literature on green computing focusing on e-waste management is scant. The research questions aroused are as follows. Is it actually possible to implement green computing using e-waste management? What are the issues pertaining to it? How can this be achieved? How sustainable it this approach? The study tries to answer these questions. The main objective of this study is to establish e-waste management as a parameter for green computing. The study also intends to check the sustainability potential of this approach. The paper will help the stakeholders, practitioners, researchers and decision makers for choosing a benchmark approach and will pave directions for future research.}
}
@article{PERI2016535,
title = {Vegetation and soil – related parameters for computing solar radiation exchanges within green roofs: Are the available values adequate for an easy modeling of their thermal behavior?},
journal = {Energy and Buildings},
volume = {129},
pages = {535-548},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816306983},
author = {Giorgia Peri and Gianfranco Rizzo and Gianluca Scaccianoce and Maria {La Gennusa} and Philip Jones},
keywords = {Green roof canopy, Heat transfer, Shortwave radiation exchange, Building energy simulation, Leaf area index, Fractional vegetation coverage},
abstract = {Several studies analyze the thermal performance of vegetated roofs, presenting either mathematical models or experimental quantifications of heat transfer process through them, also showing the effect of vegetation and soil parameters on the thermal and energy performance of this type of roofing system. However, presently the level of availability of these parameters, has not been enough considered. This work intends to investigate this underestimated issue of the green roofs’ thermal modeling, through the consideration of the availability of parameters pertinent to the shortwave radiation exchange, which are adopted by models based on leaf area index (LAI) and on the fractional vegetation coverage (σf). From the analysis of these models, the importance of a precise knowledge of such parameters arises, despite the availability of their values is still limited. In addition, the absence of a proper database including several green roof plant species and information on their stage of growth, to which a technician possibly will refer in the case of lack of field data, represents quite a worthwhile matter because it may lead to inaccurate estimations of the thermal loads for climatization of buildings equipped with green roofs. In this respect, the manuscript reports an estimation of the errors potentially occurring when not specific vegetation data are used. This evaluation is based on simulations, performed with the DesignBuilder© software, showing the energy effects produced by different assumptions in the green parameters (that is by adopting experimental data revealed by authors and a potential mix of the literature available parameters). Thus, the mission of the paper is to enlighten a difficulty that a technician possibly will encounter when performing an energy and thermal simulation of a green roof; therefore a strong call for making the current database more tailored on green roofs has also arisen.}
}
@article{MAHMOUD201858,
title = {Towards energy-aware fog-enabled cloud of things for healthcare},
journal = {Computers & Electrical Engineering},
volume = {67},
pages = {58-69},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.02.047},
url = {https://www.sciencedirect.com/science/article/pii/S0045790618300399},
author = {Mukhtar M.E. Mahmoud and Joel J.P.C. Rodrigues and Kashif Saleem and Jalal Al-Muhtadi and Neeraj Kumar and Valery Korotaev},
keywords = {Application allocation, Cloud of things, Energy efficiency, Fog Computing, Internet of Things, Healthcare},
abstract = {The Internet-of-Things (IoT) represents the next groundbreaking change in information and communication technology (ICT) after the Internet. IoT is concerned with making everything connected and accessible through the Internet. However, IoT objects (things) are characterized by constrained computing and storage resources. Therefore, the Cloud of Things (CoT) paradigm that integrates the Cloud with IoT is proposed to meet the IoT requirements. In CoT, the IoT capabilities (e.g., sensing) are provisioned as services. Unfortunately, the two-tier CoT model is not efficient in the use cases sensitive to delays and energy consumption (e.g., in healthcare). Consequently, Fog Computing is proposed to support such IoT services and applications. This paper reviews the most relevant Fog-enabled CoT system models and proposes an energy-aware allocation strategy for placing application modules (tasks) on Fog devices. Finally, the performance of the proposed strategy is evaluated in comparison with the default allocation and Cloud-only policies, using the iFogSim simulator. The proposed solution was observed to be more energy-efficient, saving approximately 2.72% of the energy compared to Cloud-only and approximately 1.6% of the energy compared to the Fog-default.}
}
@article{ECKHOFF2019147,
title = {Measurement of minimum ignition energies (MIEs) of dust clouds – History, present, future},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {61},
pages = {147-159},
year = {2019},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950423019303687},
author = {Rolf K. Eckhoff},
keywords = {Dust clouds, Dust explosions, Ignition sources, Electric and electrostatic sparks, Minimum ignition energy, MIE, Laboratory-scale test methods, Historical development},
abstract = {Nearly 130 years ago Holtzwart and von Meyer (1891) demonstrated by experiments that explosible dust clouds could be ignited by inductive electric sparks. Then more than half a century passed before the publication of the important quantitative research of Boyle and Llewellyn (1950) and Line et al. (1959). They worked with capacitive electric sparks and found that the minimum capacitor energies ½CU2 required for ignition of various dust clouds in air decreased substantially when a large series resistance, in the range 104–107 Ω, was introduced in the discharge circuit. When considering that the net energies of the sparks themselves were only of the order of 10% of the ½CU2 discharged, the minimum net spark energies required for ignition with a large series resistance were only a few per cent of the net energies required without such a resistance. Line et al. observed that the essential effect of increasing the series resistance, and hence increasing the discharge time of the sparks, was to reduce the disturbance of the dust cloud by the blast wave from the spark. This phenomenon was explored further by Eckhoff (1970, 2017), and subsequently by some simple experiments by Eckhoff and Enstad (1976). Franke (1974, 1977) and Laar (1980) confirmed the additional finding of Line et al. (1959) that the minimum ½CU2 for ignition is also substantially reduced by including a series inductance in the discharge circuit, rather than a series resistance. The basic reason is the same as with a large series resistance, viz. increased spark discharge time and hence decreased disturbance of the dust cloud by blast wave from the spark. For this reason inclusion of an appreciable series inductance in the spark discharge circuit is an essential element in current standard MIE test methods. In experiments with spark ignition of transient dust clouds produced by a blast of air in a closed vessel, it is necessary to synchronize the occurrence of the spark with the formation of the dust cloud. The precision required from this type of synchronization is typically of the order of 10 ms, which can be obtained even by mechanical arrangements, such as rapid change of spark gap length, or of the distance between two capacitor plates. The present paper reviews some methods that have been/are being used for achieving adequate synchronization of dust cloud appearance and spark discharge. Some current standard experimental methods for determining MIEs of dust clouds experimentally have also been reviewed. The same applies to some theories of electric-spark ignition of dust clouds. At the end of paper some suggestions for possible future modifications of current standard methods for measuring MIEs of explosible dust clouds are presented. With regard to justifying significant modifications of existing standard methods, the “bottom line” is, as quite often in many connections, that any modifications should be based on realistic cost/benefit evaluations.}
}
@article{PANNEERSELVAM2018239,
title = {An investigation into the impacts of task-level behavioural heterogeneity upon energy efficiency in Cloud datacentres},
journal = {Future Generation Computer Systems},
volume = {83},
pages = {239-249},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.12.064},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1731960X},
author = {John Panneerselvam and Lu Liu and Yao Lu and Nick Antonopoulos},
keywords = {Energy-aware stragglers, Long-tails, Resource idleness, Task heterogeneity},
abstract = {Cloud datacentre resources and the arriving jobs are addressed to be exhibiting increased level of heterogeneity. A single Cloud job may encompass one to several number of tasks, such tasks usually exhibit increased level of behavioural heterogeneity though they belong to the same job. Such behavioural heterogeneity are usually evident among the level of resource consumption, resource intensiveness, task duration etc. These task behavioural heterogeneity within jobs impose various complications in achieving an effective energy efficient management of the Cloud jobs whilst processing them in the server resources. To this end, this paper investigates the impacts of the task level behavioural heterogeneity upon energy efficiency whilst the tasks within given jobs are executed in Cloud datacentres. Real-life Cloud trace logs have been investigated to exhibit the impacts of task heterogeneity from three different perspectives including the task execution trend and task termination pattern, the presence of few proportions of resource intensive and long running tasks within jobs. Furthermore, the energy implications of such straggling tasks within jobs have been empirically exhibited. Analysis conducted in this study demonstrates that Cloud jobs are extremely heterogeneous and tasks behave distinctly under different execution instances, and the presence of energy-aware long tail stragglers within jobs can significantly incur extravagant level of energy expenditures.}
}
@article{MAHBUB2020100266,
title = {IoT-Cognizant cloud-assisted energy efficient embedded system for indoor intelligent lighting, air quality monitoring, and ventilation},
journal = {Internet of Things},
volume = {11},
pages = {100266},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100266},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520301001},
author = {Mobasshir Mahbub and M. Mofazzal Hossain and Md. Shamrat Apu Gazi},
keywords = {IoT, Taxonomy, Architecture, Embedded System, Intelligent Autonomous Lighting, Air Quality Monitoring, Ventilation, Energy Consumption, Economic Analysis},
abstract = {Pervasive monitoring, delivered through the innovations of wireless sensor systems, disperses across numerous areas of modern living. This enables environmental parameters to be detected, interpreted, evaluated and deduced from natural resources. The increase in the number of Internet-connected devices has prompted the escalation of the technological revolution in the Internet of Things (IoT). The seamless integration of sensors and actuators enables IoT technologies to be one of the paramount of modern life. The continuous development of IoT technologies allows it to become more intelligent, resourceful, and user friendly every day. IoT evolutions include smart homes, smart grids, smart cities, connected cars, wearables, smart industries, smart retail, etc. A variety of key experiments and analyses have been carried out to develop infrastructure through IoT. Intelligent ventilation systems, indoor automated lighting, and auto autonomous decision-making systems are among the most significant and recent implementations of IoT. This article focuses on the design and implementation of an intelligent autonomous lighting and air ventilation system capable of energy-aware self-sufficient lighting, monitoring temperature, humidity, carbon dioxide (CO2) concentration, and smoke. This paper has prescribed an embedded system for autonomous lighting and ventilation system with HTTP protocol based monitoring over smartphones or PCs. Moreover, the system is capable of logging real-time data into the cloud server through which a user can also monitor the real-time over the cloud from anywhere in the world. Since it is a wireless gadget, communication with the control system takes place via the GSM and WiFi network, using the modern HTTP protocol. Moreover, the work has performed energy consumption and economic analysis of the prescribed system to show its effectiveness. The article has also provided an overview of IoT infrastructure at the commencement of the work.}
}
@article{CASTANE2020102012,
title = {Machine learning applied to accelerate energy consumption models in computing simulators},
journal = {Simulation Modelling Practice and Theory},
volume = {102},
pages = {102012},
year = {2020},
note = {Special Issue on IoT, Cloud, Big Data and AI in Interdisciplinary Domains},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.102012},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301455},
author = {Gabriel G. Castañé and Alejandro {Calderón Mateos}},
keywords = {Simulation, Computer simulation, Machine learning, Memoization},
abstract = {The ever-increasing growth of data centres and fog resources makes difficult for current simulation frameworks to model large computing infrastructures. Therefore, a major trade-off for simulators is the balance between abstraction level of the models, the scalability, and the performance of the executions. In order to balance better these, early forays can be found in the literature in which AI techniques are applied, but either lack of generality or are tailored to specific simulation frameworks. This paper describes the methodology to integrate memoization as a technique of supervised learning into any computing simulators framework. In this process, a bespoke kernel was constructed for the analysis of the energy models used in most well known computing simulators -cloud and fog-, but also to avoid simulation overhead. Finally, a detailed evaluation of energy models and its performance is presented showing the impact of applying supervised learning to computing simulator, showing performance improvements when models are more accurate and computations are dense.}
}
@article{JIANG2021259,
title = {Bone Measures by Dual-Energy X-Ray Absorptiometry and Peripheral Quantitative Computed Tomography in Young Women With Type 1 Diabetes Mellitus},
journal = {Journal of Clinical Densitometry},
volume = {24},
number = {2},
pages = {259-267},
year = {2021},
issn = {1094-6950},
doi = {https://doi.org/10.1016/j.jocd.2020.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1094695020300901},
author = {Hongyuan Jiang and Dale L. Robinson and Alison Nankervis and Suzanne M. Garland and Emma T. Callegari and Sarah Price and Peter V.S. Lee and John D. Wark},
keywords = {DXA, Finite element modelling, pQCT, Type 1 diabetes mellitus},
abstract = {Understanding bone fragility in young adult females with type 1 diabetes mellitus (T1DM) is of great clinical importance since the high fracture risk in this population remains unexplained. This study aimed to investigate bone health in young adult T1DM females by comparing relevant variables determined by dual-energy X-ray absorptiometry (DXA), peripheral quantitative computed tomography (pQCT) at the tibia and pQCT-based finite element analysis (pQCT-FEA) between T1DM subjects (n = 21) and age-, height- and weight-matched controls (n = 63). Tibial trabecular density (lower by 7.1%; 228.8 ± 33.6 vs 246.4 ± 31.8 mg/cm3, p = 0.02) and cortical thickness (lower by 7.3%; 3.8 ± 0.5 vs 4.1 ± 0.5 cm, p = 0.03) by pQCT were significantly lower in T1DM subjects than in controls. Tibial shear stiffness by pQCT-FEA was also lower in T1DM subjects than in controls at both the 4% site (by 17.1%; 337.4 ± 75.5 vs 407.1 ± 75.4 kN/mm, p < 0.01) and 66% site (by 7.9%; 1113.0 ± 158.6 vs 1208.8 ± 161.8 kN/mm, p = 0.03). These differences remained statistically significant after adjustment for confounding factors. No difference between groups was observed in DXA-determined variables (all p ≥ 0.08), although there was a trend towards lower aBMD at the lumbar spine in T1DM subjects than in controls after adjustment for confounders (p = 0.053). These novel findings elicited using pQCT and pQCT-FEA suggest a clinically significant impact of T1DM on bone strength in young adult females with T1DM. Peripheral QCT and pQCT-FEA may provide more information than DXA alone on bone fragility in this population. Further longitudinal studies with a larger sample size are warranted to understand the evolution and causes of bone fragility in young T1DM females.}
}
@article{RUAN2019380,
title = {Virtual machine allocation and migration based on performance-to-power ratio in energy-efficient clouds},
journal = {Future Generation Computer Systems},
volume = {100},
pages = {380-394},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18321629},
author = {Xiaojun Ruan and Haiquan Chen and Yun Tian and Shu Yin},
abstract = {The last decade witnessed a dramatic advance in cloud computing research and techniques. One of the key challenges in this field is reducing the massive amount of energy consumption in cloud computing data centers. Many power-aware virtual machine (VM) allocation and consolidation approaches were proposed to reduce energy consumption efficiently. However, most of the existing efficient cloud solutions save energy at the cost of significant performance degradation. In this paper, we propose a strategy to calculate the optimized working utilization levels for host computers. As the performance and power data need to be measured on real platforms, to make our design practical, we propose a strategy named “PPRGear” which is based on the sampling of utilization levels with distinct Performance-to-Power Ratios (PPR) calculated as the number of Server Side Java operations completed during a certain time period divided by the average active power consumption in that period. In addition, we present a framework for virtual machine allocation and migration which leverages the PPR for various host types. By achieving the optimal balance between host utilization and energy consumption, our framework is able to ensure that host computers run at the most power-efficient utilization levels, i.e., the levels with the highest PPR, thus tremendously reducing energy consumption with ignorable sacrifice of performance. Our extensive experiments with real world traces show that compared with three baseline energy-efficient VM allocation and selection algorithms, IqrMc, MadMmt, and ThrRs, our framework is able to reduce the energy consumption up to 69.31% for various host computer types with fewer migration times, shutdown times, and little performance degradation for cloud computing data centers.}
}
@article{FLORESMARQUEZ20157,
title = {Two algorithms to compute the electric resistivity response using Green's functions for 3D structures},
journal = {Geofísica Internacional},
volume = {54},
number = {1},
pages = {7-20},
year = {2015},
issn = {0016-7169},
doi = {https://doi.org/10.1016/j.gi.2015.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0016716915000070},
author = {E. Leticia Flores-Márquez and Andrés Tejero-Andrade and Adrián León-Sánchez and Claudia Arango-Galván and René Chávez-Segura},
keywords = {Modelo eléctrico 3D, funciones de Green, método integral, teorema de Gauss, condiciones de frontera., 3D electrical model, Green's functions, integral method, Gauss theorem, Boundary conditions.},
abstract = {Resumen
Se introduce una solución integral para el problema directo de la respuesta geoeléctrica DC para cuerpos tri-dimensionales en un semi- espacio, mediante las funciones de Green. El primer algoritmo que se presenta se basa en el método integral de volumen (MIV); aquí, únicamente la corriente eléctrica primaria se utiliza para calcular el potencial eléctrico. El segundo caso emplea el método integral de superficie (MIS), en donde se asume que la carga inducida es debida al campo eléctrico primario. Ambos algoritmos son una combinación de integrales de volumen y de condiciones de frontera. Este artículo muestra la aplicabilidad de estos algoritmos para generar imágenes de perfiles de resistividad que reproducen algunos arreglos de electrodos para ejemplos sintéticos tradicionales, y posteriormente estas imágenes se comparan con resultados ya publicados en la literatura. Finalmente, la comparación entre estos resultados muestra que el concepto de carga inducida utilizada en MIS produce una mejor aproximación, que el esquema MIV en el cálculo del potencial eléctrico.
An integral solution of the forward DC geoelectric response for three-dimensional target-bodies in a half-space, based on Green's functions, is introduced. The first algorithm presented is based on a volume integral method (VIM); here, only the primary electrical current is involved to compute the electric potential. The second one employs the surface integral method (SIM), and it is assumed the induced charge is due to the primary electrical field. Both algorithms are a combination of boundary and volume integrals. This paper shows the applicability of these algorithms to generate resistivity profile images reproducing some electrode arrays for traditional synthetic examples, and then these images were compared with already published results. Finally, the comparison between results shows the concept of induced charge used in SIM produces a better approach than VIM scheme in computing the electrical potential.}
}
@article{KHAN201982,
title = {Energy-aware dynamic resource management in elastic cloud datacenters},
journal = {Simulation Modelling Practice and Theory},
volume = {92},
pages = {82-99},
year = {2019},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2018.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X18301825},
author = {Ayaz Ali Khan and Muhammad Zakarya and Rahim Khan},
keywords = {Cloud infrastructure, Resource management, Power efficiency, Elastic datacenters, DVFS},
abstract = {In clouds, placement of on-demand applications on heterogeneous machines, has turned out to be a crucial research problem, particularly, in terms of performance and energy consumption. Techniques like Dynamic Voltage and Frequency Scaling (DVFS), processor speed adjustment and features such as turning off displays, activating sleep modes, etc. are only useful for decreasing the energy consumption of a single machine, at marginal loss in performance. They cannot be used to achieve significant power optimization in High Performance Computing (HPC) systems such as grids, and cloud datacenters; because power saved by scaling down the processor voltage is far less than switching off a machine. Resource management, using dynamic consolidation of VMs, allows cloud service providers to optimize resource usability, performance and decrease power consumption. This paper investigates various resource management techniques, and suggests several heuristic approaches to optimise energy consumption and performance in elastic datacenters. Using real workload datasets, our evaluation suggests that a combination of the proposed VM allocation and consolidation with migration control technique could save approximately 1.96%–9.38% energy, and improve 0.32%–5.96% performance, as compared to its closest rivals.}
}
@article{HAKVOORT20209,
title = {Quantifying near metal visibility using dual energy computed tomography and iterative metal artifact reduction in a fracture phantom},
journal = {Physica Medica},
volume = {69},
pages = {9-18},
year = {2020},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2019.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1120179719304971},
author = {E.T. Hakvoort and R.H.H. Wellenberg and G.J. Streekstra},
keywords = {Metal artifact reduction, iMAR, Dual-energy CT, Fracture modeling, Femur phantom},
abstract = {Purpose
To quantitatively assess CT image quality and fracture visibility using virtual monochromatic imaging and iterative metal artifact reduction (iMAR) in a femoral bone fracture phantom with different fixation implants.
Methods
A custom made phantom was scanned at 120-kVp and 140-kVp single-energy and 100/150-kVp dual-energy. Three stainless steel and two titanium implants with different thicknesses were placed on the phantom containing simulated one and two mm fractures. Single-energy CT images were reconstructed with and without iMAR, while DECT images were reconstructed at monochromatic energies between 70 and 190 keV. Non-metal scans were used as a reference. A Fourier power spectrum method and fracture model were used to analyze several anatomical areas.
Results
CT-value deviations of titanium implants were much lower compared to stainless steel implants. These deviations decreased for both DECT and iMAR. Fracture visibility, measured with the fracture model, improved the most when DECT was used while artifact reduction benefitted more from iMAR. The optimal monochromatic energy for metal artifact reduction, based on CT-value deviation, varied for each metal between 130 and 150 keV. The fracture model provided a signal-to-noise ratio for the near metal fracture visibility, providing the optimal keV.
Conclusion
iMAR and high keV monochromatic images extracted from DECT both reduce metal artifacts caused by different metal fixation implants. Quantitative femoral phantom results show that DECT is superior to iMAR regarding fracture visualization adjacent to metal fixation implants. The introduction of new artifacts when using iMAR impedes its value in near metal fixation implant imaging.}
}
@article{ARENDT2020108666,
title = {Improved coronary artery contrast enhancement using noise-optimised virtual monoenergetic imaging from dual-source dual-energy computed tomography},
journal = {European Journal of Radiology},
volume = {122},
pages = {108666},
year = {2020},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2019.108666},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X1930316X},
author = {Christophe T. Arendt and Rouben Czwikla and Lukas Lenga and Julian L. Wichmann and Moritz H. Albrecht and Christian Booz and Simon S. Martin and Doris Leithner and Patricia Tischendorf and Alfredo Blandino and Thomas J. Vogl and Tommaso D'Angelo},
keywords = {Image enhancement, Computed tomography angiography, Coronary artery disease},
abstract = {Purpose
To define optimal kiloelectron volt (keV) settings for virtual monoenergetic imaging (VMI) reconstruction at dual-energy coronary computed tomography angiography (DE-CCTA).
Method
Fifty-one DE-CCTA data sets (33 men; mean age, 63.9 ± 13.2 years) were reconstructed as standard linearly-blended images (F_0.6; 60% of 90 kVp, 40% of 150 kVpSn), and with traditional (VMI) and noise-optimised (VMI+) algorithms from 40 to 100 keV in 10-keV intervals. Objective image quality was assessed with signal-to-noise ratio (SNR) and contrast-to-noise ratio (CNR) measurements. Three observers subjectively evaluated vascular contrast, image sharpness, noise and delineation of coronary plaques.
Results
Median values for objective image analysis were highest in VMI + series at 40 keV (SNR, 44.5; CNR: 33.5), significantly superior (allp < 0.001) to the best VMI series at 70 keV (SNR, 28.1; CNR, 18.4) and standard F_0.6 images (SNR, 23.2; CNR, 15.6). Overall subjective metrics achieved higher scores at 40-keV VMI+ series in comparison to 70-keV VMI series and F_0.6 images (all p < 0.001), with optimal vascular contrast (5; ICC, 0.90), good image sharpness (4; 0.88), low noise (4; 0.82), and optimal plaque delineation (5; 0.89).
Conclusions
DE-CCTA image reconstruction with 40-keV VMI + allows for significant improvement of both objective and subjective image quality.}
}
@article{EMOTO2021e119,
title = {Myocardial Extracellular Volume Quantification Using Cardiac Computed Tomography: A Comparison of the Dual-energy Iodine Method and the Standard Subtraction Method},
journal = {Academic Radiology},
volume = {28},
number = {5},
pages = {e119-e126},
year = {2021},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2020.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S1076633220301604},
author = {Takafumi Emoto and Seitaro Oda and Masafumi Kidoh and Takeshi Nakaura and Yasunori Nagayama and Daisuke Sakabe and Kiyotaka Kakei and Makoto Goto and Yoshinori Funama and Masahiro Hatemura and Seiji Takashio and Koichi Kaikita and Kenichi Tsujita and Osamu Ikeda},
keywords = {Extracellular volume, Dual-energy, Subtraction method, Iodine method, Cardiac magnetic resonance imaging},
abstract = {Rationale and Objectives
To clarify the accuracy of two measurement methods for myocardial extracellular volume (ECV) quantification (ie, the standard subtraction method [ECVsub] and the dual-energy iodine method [ECViodine]) with the use of cardiac CT in comparison to cardiac magnetic resonance imaging (CMR) as a reference standard.
Materials and Methods
Equilibrium phase cardiac images of 21 patients were acquired with a dual-layer spectral detector CT and CMR, and the images were retrospectively analyzed. CT-ECV was calculated using ECVsub and ECViodine. The correlation between the ECV values measured by each method was assessed. Bland-Altman analysis was used to identify systematic errors and to determine the limits of agreement between the CT-ECV and CMR-ECV values. Root mean squared errors and residual values for the ECVsub and ECViodine were also assessed.
Results
The correlations between ECVsub and ECViodine for both septal and global measurement were r = 0.95 (p < 0.01) and 0.91 (p < 0.01), respectively, while those between the mean ECVsub and CMR-ECV were r = 0.90 (septal, p < 0.01) and 0.84 (global, p < 0.01), and those between ECViodine and CMR-ECV were r = 0.94 (septal, p < 0.01) and 0.95 (global, p < 0.01). Bland-Altman plots showed lower 95% limits of agreement between ECViodine and CMR-ECV compared with that between ECVsub and CMR-ECV in both septal and global measurement. The root mean squared error of ECVsub was higher than that of ECViodine. The mean residual value of ECVsub was significantly higher than that of ECViodine.
Conclusion
ECViodine yielded more accurate myocardial ECV quantification than ECVsub, and provided a comparable ECV value to that obtained by CMR.}
}
@article{CHANG201834,
title = {Optimizing energy consumption for a performance-aware cloud data center in the public sector},
journal = {Sustainable Computing: Informatics and Systems},
volume = {20},
pages = {34-45},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2210537917304043},
author = {Kyungmee Chang and Sangun Park and Hyesoo Kong and Wooju Kim},
keywords = {Cloud data center, Power, Consumption, Optimization, Energy efficiency, Resource management},
abstract = {In general, cloud environments are based on the pay-per-use model, whereby clients pay for the information resources provided by cloud service providers. Users rent and use resources as needed while avoiding the high costs of large-scale resource acquisitions, and providers maximize their profits by managing information resources at a minimum cost while upholding service-level agreements. As the number of resources gradually increases, power supply shortages may arise. This study focuses on the fact that the CPU utilization rate of the server running in the data center is less than 30% and idle servers running only the OS consume more than half of the power consumed by hosts running with maximum CPU utilization and speed. Therefore, this study proposes an approach to enhance data center efficiency through improved management of energy consumption. We present a method to minimize energy consumption while processing the same workload, i.e., ultimately reducing the energy consumed by operating servers. Energy consumption and SLA violation rate were used as evaluation metrics of the optimization model to meet the minimum performance target.}
}
@article{FUJITA2021636,
title = {Combined Assessment of Pulmonary Ventilation and Perfusion with Single-Energy Computed Tomography and Image Processing},
journal = {Academic Radiology},
volume = {28},
number = {5},
pages = {636-646},
year = {2021},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1076633220301963},
author = {Yukio Fujita and Michael Kent and Erik Wisner and Lynelle Johnson and Joshua Stern and Lihong Qi and John Boone and Tokihiro Yamamoto},
keywords = {Pulmonary functional imaging, Ventilation, Perfusion, Single-energy computed tomography (CT), Deformable image registration},
abstract = {Rationale and Objectives
To establish a proof-of-principle for combined assessment of pulmonary ventilation and perfusion using single-energy computed tomography (CT) and image processing/analysis (denoted as single-energy CT ventilation/perfusion imaging).
Materials and Methods
Breath-hold CT scans were acquired at end-expiration and end-inspiration before injection of iodinated contrast agents, and repeated at end-inspiration after contrast injection for 17 canines (8 normal and 9 diseased lung subjects). Ventilation images were calculated with deformable image registration to map the end-expiratory and end-inspiratory CT images and quantitative analysis for regional volume changes as surrogates for ventilation. Perfusion images were calculated by subtracting the end-inspiratory precontrast CT from the deformably registered end-inspiratory postcontrast CT, yielding a map of regional Hounsfield unit enhancement as a surrogate for perfusion. Ventilation-perfusion matching, spatial heterogeneity, and gravitationally directed gradients were compared between two groups using a Wilcoxon rank-sum test.
Results
The normal group had significantly higher Dice similarity coefficients for spatial overlap of segmented functional volumes between ventilation and perfusion (median 0.40 vs. 0.33, p = 0.05), suggesting stronger ventilation-perfusion matching. The normal group also had greater Spearman's correlation coefficients based on 16 regions of interest (median 0.58 vs. 0.40, p = 0.09). The coefficients of variation were comparable (median, ventilation 0.71 vs. 0.91, p = 0.60; perfusion 0.63 vs. 0.75, p = 0.27). The linear regression slopes of gravitationally directed gradient were also comparable for ventilation (median, ventilation −0.26 vs. −0.18, p = 0.19; perfusion −0.17 vs. −0.06, p = 0.11).
Conclusion
These findings provide proof-of-principle for single-energy CT ventilation/perfusion imaging.}
}
@article{WANG201883,
title = {Rockburst characteristics in syncline regions and microseismic precursors based on energy density clouds},
journal = {Tunnelling and Underground Space Technology},
volume = {81},
pages = {83-93},
year = {2018},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2018.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S088677981730963X},
author = {Guifeng Wang and Siyuan Gong and Linming Dou and Hao Wang and Wu Cai and Anye Cao},
keywords = {Rockburst, Microseismicity, Fold structure, Seismic velocity tomography, Energy density clouds},
abstract = {Cases of rockbursts were investigated to analyze the influence of syncline structures on rockbursts. In a syncline structure in Xing’an Coal Mine, the relationship between the activity of mining-induced tremors and a violent rockburst was studied from temporal and spatial distribution graphs of tremors and pictures of energy density clouds. The results indicated that the temporal and spatial distributions of tremors before the rockburst intuitively illustrate areas in which tremors were concentrated; however, the tremors showed more complex spatial distribution characteristics, and it was difficult to identify the evolutionary trends in tremors over time. Energy density clouds exhibited obvious nucleation characteristics, and presented an obvious extension around the nucleus until a rockburst or strong tremor happened around the edges thereof. High-stress regions formed due to the syncline structure can be identified by applying seismic velocity tomography. According to the distribution of wave velocity, large-diameter hole drilling and deep-hole blasting were implemented in targeted areas. The activity of tremors induced by the follow-up mining confirmed the effectiveness of these stress relief measures.}
}
@article{LIN201847,
title = {A cloud server energy consumption measurement system for heterogeneous cloud environments},
journal = {Information Sciences},
volume = {468},
pages = {47-62},
year = {2018},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.08.032},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518306364},
author = {Weiwei Lin and Haoyu Wang and Yufeng Zhang and Deyu Qi and James Z. Wang and Victor Chang},
keywords = {Cloud server, Heterogeneous cloud environment, Power measurement, Power model, Distributed system},
abstract = {With rapid development of cloud computing technologies and applications, the number and scale of cloud data centers have grown exponentially in recent years. One of the major problems with current cloud data centers is their huge energy consumption, which makes energy consumption management one of the hottest research topics in the field of cloud computing. This paper aims at implementing an effective Distributed Energy Meter (DEM) system for heterogeneous cloud environments based on a multi-component power consumption model for cloud servers. Specifically, we propose a modeling method for the energy consumption of key components (CPU, memory and disk) of computer servers and reveal the mathematical relationship between the resource usage of the key components and the system energy consumption. The proposed DEM system cannot only estimate the energy consumption of heterogeneous cluster environments (Linux and Windows NT), but also support various CPU power consumption models. In addition, a unique disk power consumption model that uses different thresholds to distinguish various disk I/O states (sequential/random, read/write) to achieve an accurate estimation of disk power consumption. Experimental studies conducted on a heterogeneous cluster with workloads generated by PCMark and Sysbench demonstrate that the proposed DEM system outperforms the state-of-art models in estimating the energy consumption of heterogeneous cloud environments.}
}
@article{YAO2020101886,
title = {Privacy-preserving and energy efficient task offloading for collaborative mobile computing in IoT: An ADMM approach},
journal = {Computers & Security},
volume = {96},
pages = {101886},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2020.101886},
url = {https://www.sciencedirect.com/science/article/pii/S0167404820301590},
author = {Yuanfan Yao and Ziyu Wang and Pan Zhou},
keywords = {Task offloading, Energy efficient, ADMM, Privacy preserving, Edge computing, Internet of things},
abstract = {The pervasive application of Internet of Things (IoT) has pushed the proliferation of edge computing. There exists potential in edge computing to satisfy the concerns of task delay, network bandwidth, battery endurance and data privacy. The superiority of edge computing is endorsed by the deployment of edge nodes to the device user’s proximity. Within partial capabilities of the cloud server, edge nodes efficaciously alleviate the burden on core network. Considering a mission critical system, enduring battery life is even more accentuated over task latency to maintain the device on operation. So in this paper, we put forward an energy efficient task offloading problem subject to the overall task delay based on Alternating Direction Method of Multipliers (ADMM) in a three-tier MEC network, equipped with both edge nodes and the cloud. The offloading choice is the approximate convergence with demanded precision concluded by persistent ADMM iterations. We also address the privacy disclosure concerns in the data transmission among IoT devices and apply differential privacy to the intricate optimization problem. More specifically, we associate privacy-preserving method with the exhaustive task offloading processes and iteration procedures. At last, simulations and experiments demonstrate the performance and convergence of our proposed algorithm.}
}
@article{CHOU2019166,
title = {Cloud forecasting system for monitoring and alerting of energy use by home appliances},
journal = {Applied Energy},
volume = {249},
pages = {166-177},
year = {2019},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2019.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S030626191930710X},
author = {Jui-Sheng Chou and Ngoc-Son Truong},
keywords = {Energy informatics, Application platform, Home energy consumption, Smart grid, Cloud service, Real-time system, Hybrid artificial intelligence},
abstract = {Inrecentyears,energy information systems have had an important role in the operational optimization of intelligent buildings to provide such benefits as high efficiency, energy savings and smart services. Interest in the intelligent management of home energy consumption using data mining and time series analysis is increasing. Therefore, this work develops an efficient web-based energy information management system for the power consumption of home appliances that monitors the energy load of a home, analyzes its energy consumption based on machine learning, and then sends information to various stakeholders. It interacts with the end-user through energy dashboards and emails. The web-based system includes a novel hybrid artificial intelligence model to improve its prediction of energy usage. An automatic warning function is also developed to identify anomalous energy consumption in a home in real time. The cloud system automatically sends a message to the user's email whenever a warning is necessary. End-users of this system can use forecast information and anomalous data to enhance the efficiency of energy usage in their buildings especially during peak times by adjusting the operating schedule of their appliances and electrical equipment.}
}
@article{KHANDELWAL20191,
title = {State-of-the-Art Dual-Energy Computed Tomography in Gastrointestinal and Genitourinary Imaging},
journal = {Advances in Clinical Radiology},
volume = {1},
pages = {1-17},
year = {2019},
note = {Advances in Clinical Radiology},
issn = {2589-8701},
doi = {https://doi.org/10.1016/j.yacr.2019.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S2589870119300124},
author = {Ashish Khandelwal and Achille Mileto and Shuai Leng and Joel G. Fletcher},
keywords = {Dual energy, Computed tomography, CT, Gastrointestinal, Liver, Kidney, Small bowel, Genitourinary}
}
@article{ADHIKARI2019100053,
title = {Energy efficient offloading strategy in fog-cloud environment for IoT applications},
journal = {Internet of Things},
volume = {6},
pages = {100053},
year = {2019},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2019.100053},
url = {https://www.sciencedirect.com/science/article/pii/S2542660519300265},
author = {Mainak Adhikari and Hemant Gianey},
keywords = {IoT application, Multi-objective optimization, Fog computing, Cloud computing, Quality-of-Service, Computational offloading},
abstract = {Nowadays, cloud computing leverages the capability of Internet-of-Things (IoT) applications by providing computing resources as a form of virtual machine (VM) instances. However, the Cloud data center consumes a large amount of energy while transmitting and computing the IoT applications which lead to a high carbon footprint. On the other hand, the Fog nodes provide various cloud services at the edge of the network which can run the IoT applications locally with minimum energy consumption and delay. Due to the limited resource capacity, the Fog nodes are not suitable for processing the resource-intensive IoT applications. To address these challenges, in this paper, we build sustainable infrastructure in Fog-Cloud environment for processing delay-intensive and resource-intensive applications with an optimal task offloading strategy. The proposed offloading strategy uses Firefly algorithm for finding an optimal computing device based on two Quality-of-Service (QoS) parameters such as energy consumption and computational time. The main objectives of this strategy are to minimize the computational time and the energy consumption of the IoT applications with minimum delay. The effect of the control parameters of the Firefly technique is investigated thoroughly. Through comparisons, we show that the proposed method performs better than the existing ones in terms of various performance metrics including computational time, energy consumption, CO2 emission, and Temperature emission.}
}
@article{GONZALEZMANCEBO2020278,
title = {Design of a nanoprobe for high field magnetic resonance imaging, dual energy X-ray computed tomography and luminescent imaging},
journal = {Journal of Colloid and Interface Science},
volume = {573},
pages = {278-286},
year = {2020},
issn = {0021-9797},
doi = {https://doi.org/10.1016/j.jcis.2020.03.101},
url = {https://www.sciencedirect.com/science/article/pii/S0021979720304021},
author = {Daniel González-Mancebo and Ana Isabel Becerro and Ariadna Corral and Sonia García-Embid and Marcin Balcerzyk and Maria Luisa Garcia-Martin and Jesús M. {de la Fuente} and Manuel Ocaña},
keywords = {Lanthanide nanoparticles, Trimodal, High field MRI, Dual energy CT, Luminescence},
abstract = {The combination of different bioimaging techniques, mainly in the field of oncology, allows circumventing the defects associated with the individual imaging modalities, thus providing a more reliable diagnosis. The development of multimodal endogenous probes that are simultaneously suitable for various imaging modalities, such as magnetic resonance imaging (MRI), X-ray computed tomography (CT) and luminescent imaging (LI) is, therefore, highly recommended. Such probes should operate in the conditions imposed by the newest imaging equipment, such as MRI operating at high magnetic fields and dual-energy CT. They should show, as well, high photoluminescence emission intensity for their use in optical imaging and present good biocompatibility. In this context, we have designed a single nanoprobe, based on a core-shell architecture, composed of a luminescent Eu3+:Ba0.3Lu0.7F2.7 core surrounded by an external HoF3 shell that confers the probe with very high magnetic transverse relaxivity at high field. An intermediate, optically inert Ba0.3Lu0.7F2.7 layer was interposed between the core and the shell to hinder Eu3+–Ho3+ cross-relaxation and avoid luminescence quenching. The presence of Ba and Lu, with different K-edges, allows for good X-ray attenuation at high and low voltages. The core-shell nanoparticles synthesized are good potential candidates as trimodal bioprobes for MRI at high field, dual-energy CT and luminescent imaging.}
}
@article{OHIRA202056,
title = {Stereotactic body radiation therapy planning for liver tumors using functional images from dual-energy computed tomography},
journal = {Radiotherapy and Oncology},
volume = {145},
pages = {56-62},
year = {2020},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167814019335029},
author = {Shingo Ohira and Naoyuki Kanayama and Masayasu Toratani and Yoshihiro Ueda and Yuhei Koike and Tsukasa Karino and Ono Shunsuke and Masayoshi Miyazaki and Masahiko Koizumi and Teruki Teshima},
keywords = {SBRT, Liver, DECT, Fibrosis, Function},
abstract = {Purpose
This study aimed to generate a functional image of the liver using dual-energy computed tomography (DECT) and a functional-image-based stereotactic body radiation therapy plan to minimize the dose to the volume of the functional liver (Vfl).
Material and methods
A normalized iodine density (NID) map was generated for fifteen patients with liver tumors. The volume of liver with an NID < 0.46 was defined as Vfl, and the ratio between Vfl and the total volume of the liver (FLR) was calculated. The relationship between the FLR and Fibrosis-4 (FIB-4) was assessed. For patients with 15% < FLR < 85%, functional volumetric modulated-arc therapy plans (F-VMAT) were retrospectively generated to preserve Vfl, and compared to the clinical plans (C-VMAT).
Results
FLR showed a significantly strong correlation with FIB-4 (r = −0.71, p < 0.01). For ten generated F-VMAT plans, the dosimetric parameters of D99%, D50%, D1% and the conformity index were comparable to those of the C-VMAT (p > 0.05). For Vfl, F-VMAT plans achieved lower V5Gy (122.4 ± 31.7 vs 181.1 ± 57.3 cc), V10Gy (44.4 ± 22.2 vs 98.2 ± 33.3 cc), V15Gy (22.6 ± 20.3 vs 49.8 ± 33.7 cc), V20Gy (11.6 ± 14.1 vs 24.9 ± 25.1 cc), and Dmean (3.9 ± 2.3 vs 5.8 ± 3.0 Gy) values than the C-VMAT plans (p < 0.01).
Conclusions
The functional image derived from DECT was successfully used, allowing for a reduction in the dose to the Vfl without compromising target coverage.}
}
@article{DOMINGUEZEUSEBIO2019201,
title = {Surface energy exchange: Urban and rural forest comparison in a tropical montane cloud forest},
journal = {Urban Forestry & Urban Greening},
volume = {41},
pages = {201-210},
year = {2019},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2019.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S1618866718305090},
author = {Carlo A. Domínguez-Eusebio and Enrique Alarcón and Oscar L. Briones and María del Rosario Pineda-López and Yareni Perroni},
keywords = {Energy flow, Eddy covariance, Micrometeorology, Latent heat flux, Sensible heat flux, Biometeorology},
abstract = {Topical urban expansion has undergone a great increase, affecting land cover and altering ecosystems. It is important to know physical processes alterations such as surface energy balance in ecosystems surrounded by expanding cities in order to make predictions, planning and management of urban forests. This study determines the surface energy balance of a fragment of tropical montane cloud forest surrounded by a city (urban forest) in the central region of eastern Mexico, and compares it to values obtained in a large matrix of continuous forest adjacent to the city (rural forest) with similar topography. Surface energy exchange was measured by the eddy covariance technique. The urban forest presented values of daytime latent heat flux 4% higher compared to the rural forest (equivalent to 24.7 W/m2), and values of surface heat flux 4% lower (equivalent to 6.4 W/m2). Daily values ​​of energy closure ratio​ (ECR, 0.98) and energy balance ratio (EBR, 0.96) indicated that a greater amount of energy is partitioned from the system by sensible and latent heat fluxes of a urban forests of 30 ha. Also, the urban forest presented a higher diurnal air temperature (mean differences of 0.5 °C and maximum of 1.5 °C) and evapotranspiration rate (mean differences of 0.06 mm/h and maximum of 0.12 mm/h). We believe our work is the first one to compare forests in such contrasting situations, increasing our understanding on the behavior of surface energy exchange of an urban tropical cloud forest, and the intensity of the effect of urbanization on this type of ecosystem.}
}
@article{PICOUET2014146,
title = {Partial scanning using computed tomography for fat weight prediction in green hams: Scanning protocols and modelling},
journal = {Journal of Food Engineering},
volume = {142},
pages = {146-152},
year = {2014},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2014.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0260877414002532},
author = {P.A. Picouet and I. Muñoz and E. Fulladosa and G. Daumas and P. Gou},
keywords = {Computed tomography, Non-destructive, X-ray, Fat, Ham, Models for hams},
abstract = {The objective of this work was to study the feasibility of computed tomography (CT) for predicting fat weight using a complete or partial scanning of green hams. Sixty-eight hams covering a wide range of fat weight were divided into calibration (total weight 11.46±0.97kg) and validation (total weight 11.35±1.13kg) sets, fully scanned by CT and dissected. Virtual slices were constructed to standardise the number of slices for hams of different length and their fat weight was estimated. Different predictive models were established with partial least square regression (PLS) and ordinary linear regression (OLR) using all the tomograms and with OLR and multi-linear regression (MLR) using a reduced number of virtual slices. The MLR model with 3 virtual slices gave a better accuracy (RMSEV=145g) than the PLS model which used all the tomograms (RMSE=156g). MLR model using two virtual slice could be accurate enough (RMSEV=205g) for industrial monitoring applications.}
}
@article{KHOKHRIAKOV2020114957,
title = {Multicore processor computing is not energy proportional: An opportunity for bi-objective optimization for energy and performance},
journal = {Applied Energy},
volume = {268},
pages = {114957},
year = {2020},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.114957},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920304694},
author = {Semyon Khokhriakov and Ravi Reddy Manumachu and Alexey Lastovetsky},
keywords = {Energy proportionality, Multicore processor, Energy optimization, Energy predictive models, Bi-objective optimization, Fast Fourier transform, Matrix multiplication},
abstract = {Energy proportionality is the key design goal followed by architects of multicore processors. One of its implications is that optimization of an application for performance will also optimize it for energy. In this work, we show that energy proportionality does not hold true for multicore processors. This finding creates the opportunity for bi-objective optimization of applications for energy and performance. We propose and study a novel application-level bi-objective optimization method for energy and performance for multithreaded dataparallel applications. The method uses two decision variables, the number of identical multithreaded kernels (threadgroups) executing the application and the number of threads per threadgroup, with a given workload partitioned equally between the threadgroups. We experimentally demonstrate the efficiency of the method using four popular and highly optimized multithreaded data-parallel applications, two employing two-dimensional fast Fourier transform and the other two, dense matrix multiplication. The experiments performed on four modern multicore processors show that the optimization for performance alone results in increase in dynamic energy consumption by up to 89% and optimization for dynamic energy alone results in performance degradation by up to 49%. By solving the bi-objective optimization problem, the method determines up to 11 Pareto-optimal solutions. Finally, we propose a qualitative dynamic energy model employing performance events as variables to explain the discovered energy nonproportionality. The model shows that the energy nonproportionality on our experimental platforms for the two data-parallel applications is due to disproportionately energy expensive activity of the data translation lookaside buffer.}
}
@article{SHARMA2020211,
title = {HIGA: Harmony-inspired genetic algorithm for rack-aware energy-efficient task scheduling in cloud data centers},
journal = {Engineering Science and Technology, an International Journal},
volume = {23},
number = {1},
pages = {211-224},
year = {2020},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2019.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S2215098618312023},
author = {Mohan Sharma and Ritu Garg},
keywords = {Cloud computing, Task scheduling, Energy and rack awareness, Genetic algorithm, Harmony search, Hybrid metaheuristic},
abstract = {Reducing energy consumption in cloud data centers is one of the prime issue in the cloud community. It reduces energy related costs and increases lifespan of high performance computing resources deployed in cloud data centers and it also helps in reducing carbon emissions. Along with energy efficiency, problem of task scheduling is also one of the important problem considered in cloud data centers and it belongs to NP-class problems. With the energy consumption consideration, problem of task scheduling becomes more complex to solve. Metaheuristic algorithms are proven to generate near optimal solutions for task scheduling problem but their scheduling overhead increases vastly as the number of tasks or number of resources increases. In addition to this, metaheuristic-based scheduling algorithms lives most of time in local-optimal region of solution space and it is possible that it can finalizes the solution in the local-optimal region only. Primary motivation behind this work is from genetic algorithm itself, we have experimented with genetic algorithm in many ways to solve the problem of energy efficient task scheduling and come up with new hybrid scheme to solve it. In this work, we tackles the problem of energy efficient task scheduling on modern cloud data center architecture and proposes a novel hybrid metaheuristic scheme harmony-inspired genetic algorithm (HIGA). It also addresses issues associated with metaheuristic algorithm. HIGA combines the exploration capability of genetic algorithm and exploitation capability of harmony search by which it intelligently senses local as well as global optimal region without wasting time (iterations) in local or global optimal region and provides quick convergence. Our primary objectives in this work are to reduce makespan and computing energy and secondary objectives are to reduce the energy consumed by the resources other than computing resources and reduce execution overhead associated with scheduler. Collectively these objectives guides HIGA for better energy efficiency and performance while reducing the number of required resources (i.e. active racks). It indirectly also reduces cooling energy as we can switch off the rack components (air blower, cooling inlets) once racks become idle. Simulation analysis has been performed over independent task applications as well as real-world scientific applications like CyberShake, Epigenomics and Montage. The result clearly manifests that proposed HIGA provides by up to 33% of energy savings and 47% of improvement in application performance (makespan) that too with 39% less execution overhead.}
}
@article{FERNANDEZCERERO2018191,
title = {Security supportive energy-aware scheduling and energy policies for cloud environments},
journal = {Journal of Parallel and Distributed Computing},
volume = {119},
pages = {191-202},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518302843},
author = {Damián Fernández-Cerero and Agnieszka Jakóbik and Daniel Grzonka and Joanna Kołodziej and Alejandro Fernández-Montes},
keywords = {Cloud computing, Energy efficiency, Independent task scheduling, Genetic algorithms, VM hibernating, Cloud security},
abstract = {Cloud computing (CC) systems are the most popular computational environments for providing elastic and scalable services on a massive scale. The nature of such systems often results in energy-related problems that have to be solved for sustainability, cost reduction, and environment protection. In this paper we defined and developed a set of performance and energy-aware strategies for resource allocation, task scheduling, and for the hibernation of virtual machines. The idea behind this model is to combine energy and performance-aware scheduling policies in order to hibernate those virtual machines that operate in idle state. The efficiency achieved by applying the proposed models has been tested using a realistic large-scale CC system simulator. Obtained results show that a balance between low energy consumption and short makespan can be achieved. Several security constraints may be considered in this model. Each security constraint is characterized by: (a) Security Demands (SD) of tasks; and (b) Trust Levels (TL) provided by virtual machines. SD and TL are computed during the scheduling process in order to provide proper security services. Experimental results show that the proposed solution reduces up to 45% of the energy consumption of the CC system. Such significant improvement was achieved by the combination of an energy-aware scheduler with energy-efficiency policies focused on the hibernation of VMs.}
}
@article{JIANG2020556,
title = {Energy aware edge computing: A survey},
journal = {Computer Communications},
volume = {151},
pages = {556-580},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S014036641930831X},
author = {Congfeng Jiang and Tiantian Fan and Honghao Gao and Weisong Shi and Liangkai Liu and Christophe Cérin and Jian Wan},
keywords = {Edge computing, Energy efficiency, Computing offloading, Benchmarking, Computation partitioning},
abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.}
}
@article{BANIK20187,
title = {Probing maximum energy of cosmic rays in SNR through gamma rays and neutrinos from the molecular clouds around SNR W28},
journal = {Astroparticle Physics},
volume = {103},
pages = {7-15},
year = {2018},
issn = {0927-6505},
doi = {https://doi.org/10.1016/j.astropartphys.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0927650518300306},
author = {Prabir Banik and Arunava Bhadra},
keywords = {Cosmic rays, Supernova remnants, Molecular cloud, TeV gamma rays, TeV neutrinos},
abstract = {The galactic cosmic rays are generally believed to be originated in supernova remnants (SNRs), produced in diffusive shock acceleration (DSA) process in supernova blast waves driven by expanding SNRs. One of the key unsettled issue in SNR origin of cosmic ray model is the maximum attainable energy by a cosmic ray particle in the supernova shock. Recently it has been suggested that an amplification of effective magnetic field strength at the shock may take place in young SNRs due to growth of magnetic waves induced by accelerated cosmic rays and as a result the maximum energy achieved by cosmic rays in SNR may reach the knee energy instead of  ∼ 200 TeV as predicted earlier under normal magnetic field situation. In the present work we investigate the implication of such maximum energy scenarios on TeV gamma rays and neutrino fluxes from the molecular clouds interacting with the SNR W28. The authors compute the gamma-ray and neutrino flux assuming two different values for the maximum energy reached by cosmic rays in the SNR, from CR interaction in nearby molecular clouds. Both protons and nuclei are considered as accelerated particles and as target material. Our findings suggest that the issue of the maximum energy of cosmic rays in SNRs can be observationally settled by the upcoming gamma-ray experiment the Large High Altitude Air Shower Observatory (LHAASO). The estimated neutrino fluxes from the molecular clouds are , however, out of reach of the present/near future generation of neutrino telescopes.}
}
@article{WU201812,
title = {Soft error-aware energy-efficient task scheduling for workflow applications in DVFS-enabled cloud},
journal = {Journal of Systems Architecture},
volume = {84},
pages = {12-27},
year = {2018},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1383762117304563},
author = {Tingming Wu and Haifeng Gu and Junlong Zhou and Tongquan Wei and Xiao Liu and Mingsong Chen},
keywords = {Workflow, Cloud computing, Task scheduling, Soft error, Dynamic Voltage and Frequency Scaling, Reliability},
abstract = {Dynamic Voltage and Frequency Scaling (DVFS) has been widely used as a promising power management method to reduce the energy consumption of cloud workflows. However, due to the increasing chip density, lowering CPU voltages improperly in cloud data centers may inevitably increase soft error rate during workflow execution. Consequently, failures of timely completion of workflow applications may often take place, which raises serious concerns during the operation and maintenance of cloud data centers. To address such a problem, this paper proposes a soft error-aware energy-efficient task scheduling approach for workflow applications in DVFS-enabled cloud data centers. Under reliability and completion time constraints requested by tenants, our approach can generate energy-efficient task schedules for workflows by allocating tasks to appropriate virtual machines with specific operating frequencies. Comprehensive experiments on various well-known scientific workflow benchmarks show the effectivenss of our approach. Compared with state-of-the-art methods, our approach can achieve more than 30% energy savings while satisfying both reliability and completion time requirements.}
}
@article{ERICKSON2020165,
title = {Insights into grain boundary energy structure-property relationships by examining computed [1 0 0] disorientation axis grain boundaries in Nickel},
journal = {Scripta Materialia},
volume = {185},
pages = {165-169},
year = {2020},
issn = {1359-6462},
doi = {https://doi.org/10.1016/j.scriptamat.2020.03.062},
url = {https://www.sciencedirect.com/science/article/pii/S1359646220302463},
author = {Hunter C. Erickson and Eric R. Homer},
keywords = {Grain boundaries, Atomistic simulations, Nickel},
abstract = {A study of 346 computed [1 0 0] disorientation axis grain boundaries provides insight into grain boundary energy structure-property relationships. The grain boundaries come from 6 coincidence site lattice misorientations at regularly spaced disorientation angles, and cover the full range of boundary plane orientations. The computed energy has clear trends in disorientation angle and boundary plane orientation. The data are used to reexamine the ‘special’ attribute frequently applied to low Σ coincidence site lattice misorientations and low-angle boundaries and to assess the accuracy of a face-centered cubic grain boundary energy function developed by Bulatov, Reed, and Kumar (Acta Mater. 65 (2014) 161-175).}
}
@article{GARWOOD2018527,
title = {A framework for producing gbXML building geometry from Point Clouds for accurate and efficient Building Energy Modelling},
journal = {Applied Energy},
volume = {224},
pages = {527-537},
year = {2018},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2018.04.046},
url = {https://www.sciencedirect.com/science/article/pii/S0306261918306044},
author = {Tom Lloyd Garwood and Ben Richard Hughes and Dominic O'Connor and John K. Calautit and Michael R. Oates and Thomas Hodgson},
keywords = {Building energy modelling, Point cloud, Manufacturing, Industrial energy demand, Thermal simulation, Sustainability},
abstract = {The industrial sector accounts for 17% of end-use energy in the United Kingdom, and 54% globally. Therefore, there is substantial scope to accurately simulate and efficiently assess potential energy retrofit options for industrial buildings to lower end use energy. Due to potentially years of facility renovation and expansion Building Energy Modelling, also called Building Energy Simulation, applied to industrial buildings poses a complex challenge; but it is an important opportunity for reducing global energy demand especially considering the increase of readily available computational power compared with a few years ago. Large and complex industrial buildings make modelling existing geometry for Building Energy Modelling difficult and time consuming which impacts analysis workflow and assessment options available within reasonable budgets. This research presents a potential framework for quickly capturing and processing as-built geometry of a factory, or other large scale buildings, to be utilised in Building Energy Modelling by storing the geometry in a green building eXtensible Mark-up Language (gbXML) format, which is compatible with most commercially available Building Energy Modelling tools. Laser scans were captured from the interior of an industrial facility to produce a Point Cloud. The existing capabilities of a Point Cloud processing software and previous research were assessed to identify the potential development opportunities to automate the conversion of Point Clouds to building geometry for Building Energy Modelling applications. This led to the novel identification of a framework for storing the building geometry in the gbXML format and plans for verification of a future Point Cloud processing solution. This resulted in a sample Point Cloud, of a portion of a building, being converted into a gbXML model that met the validation requirements of the gbXML definition schema. In conclusion, an opportunity exists for increasing the speed of 3D geometry creation of existing industrial buildings for application in BEM and subsequent thermal simulation.}
}
@article{KUMAR2018147,
title = {PSO-COGENT: Cost and energy efficient scheduling in cloud environment with deadline constraint},
journal = {Sustainable Computing: Informatics and Systems},
volume = {19},
pages = {147-164},
year = {2018},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S2210537918300350},
author = {Mohit Kumar and S.C. Sharma},
keywords = {Virtual machine, Execution time, Makespan time, Execution cost, Energy consumption},
abstract = {The main goal of cloud service provider is to maximize the profit from cloud infrastructure, while cloud users want to execute their applications in minimum execution cost and time. The rapid growth in demand of computational power invites the massive growth in cloud data centers and requirement of large amount of energy consumption in cloud data centers, becomes a serious threat to the environment. To reduce the energy consumption and gain the maximum profit in cloud environment is a challenging problem due to incompatibility between workstation (physical machine) and unpredictable user demand. In this paper, we have proposed a resource allocation model for processing the applications efficiently and Particle Swarm Optimization based scheduling (PSO) algorithm named as PSO-COGENT algorithm that not only optimize execution cost and time but also reduces the energy consumption of cloud data centers, considering deadline as constraint. The developed PSO-COGENT algorithm has been simulated at cloudsim and observed that it reduces the execution time, execution cost, task rejection ratio, energy consumption and increase the throughput in comparison to PSO, honey bee and min-min algorithm.}
}
@article{NAZAR2016152,
title = {Synthesis, spectroscopic characterization, and computed optical analysis of green fluorescent cyclohexenone derivatives},
journal = {Journal of Physical Organic Chemistry},
volume = {29},
number = {3},
pages = {152-160},
year = {2016},
issn = {0894-3230},
doi = {https://doi.org/10.1002/poc.3512},
url = {https://www.sciencedirect.com/science/article/pii/S0894323022009109},
author = {Muhammad Faizan Nazar and Amir Badshah and Asif Mahmood and Muhammad Naveed Zafar and Muhammad Ramzan Saeed Ashraf Janjua and Muhammad Asam Raza and Riaz Hussain},
keywords = {crystallizability, cyclohexenones, fluorescent, hyperpolarizability, nonlinear optical},
abstract = {Cyclohexenone containing chalcones core is one important class of materials, which exhibit high nonlinear optical (NLO) responses and good crystallizability. The present study reports the successful development of six new fluorescent cyclohexenone derivatives (CDs) via conventional Robinson annulation method. The molecular structures of these newly synthesized CDs were confirmed by using various analytical techniques such as 1H NMR, 13C NMR, FTIR, EIMS, UV–Vis spectroscopy and single crystal X‐ray diffraction. The crystallographic data revealed that the spatial structure of the representative CD (4BE) belongs to monoclinic, P21/c space group. The results from luminescence studies show that the CDs molecules apparently emit intense green light at room temperature in aqueous media. The relative polarity and molecular chemical stability of the CDs molecules were predicted by measuring the molecular electrostatic potential and frontier molecular orbital energy. In addition, the UV–Vis spectra, transition character and electronic structures of these CDs were computed by using quantum chemical methodology. It was interesting to note that the values of computed and experimental electronic transitions (λmax) were in good agreement and these CDs display high hyperpolarizability (β) values. The present work will be helpful for systematical understanding of the structures and the optical properties of CDs for studying the structure–activity relationship that will suggest their potential application in photonic devices. Copyright © 2015 John Wiley & Sons, Ltd.}
}
@article{NG20201,
title = {Comparison of tumor delineation using dual energy computed tomography versus magnetic resonance imaging in head and neck cancer re-irradiation cases},
journal = {Physics and Imaging in Radiation Oncology},
volume = {14},
pages = {1-5},
year = {2020},
issn = {2405-6316},
doi = {https://doi.org/10.1016/j.phro.2020.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2405631620300105},
author = {Sweet Ping Ng and Carlos E Cardenas and Hesham Elhalawani and Courtney Pollard and Baher Elgohari and Penny Fang and Mohamed Meheissen and Nandita Guha-Thakurta and Houda Bahig and Jason M. Johnson and Mona Kamal and Adam S Garden and Jay P. Reddy and Shirley Y. Su and Renata Ferrarotto and Steven J. Frank and G. {Brandon Gunn} and Amy C. Moreno and David I. Rosenthal and Clifton D. Fuller and Jack Phan},
keywords = {Re-irradiation, Head and neck, Dual energy computed tomography, Magnetic resonance imaging, Delineation},
abstract = {In treatment planning, multiple imaging modalities can be employed to improve the accuracy of tumor delineation but this can be costly. This study aimed to compare the interobserver consistency of using dual energy computed tomography (DECT) versus magnetic resonance imaging (MRI) for delineating tumors in the head and neck cancer (HNC) re-irradiation scenario. Twenty-three patients with recurrent HNC and had planning DECT and MRI were identified. Contoured tumor volumes by seven radiation oncologists were compared. Overall, T1c MRI performed the best with median DSC of 0.58 (0–0.91) for T1c. T1c MRI provided higher interobserver agreement for skull base sites and 60 kV DECT provided higher interobserver agreement for non-skull base sites.}
}
@article{GROVES2020510,
title = {Performance of single-energy metal artifact reduction in cardiac computed tomography: A clinical and phantom study},
journal = {Journal of Cardiovascular Computed Tomography},
volume = {14},
number = {6},
pages = {510-515},
year = {2020},
issn = {1934-5925},
doi = {https://doi.org/10.1016/j.jcct.2020.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1934592520301350},
author = {Daniel W. Groves and Tushar Acharya and Chloe Steveson and John L. Schuzer and Shirley F. Rollison and Evan A. Nelson and Arlene Sirajuddin and Bharath Sathya and Kathie Bronson and Sujata M. Shanbhag and Marcus Y. Chen},
keywords = {Metal artifact, Cardiac computed tomography},
abstract = {Background
To investigate the performance of a reconstruction algorithm, single-energy metal artifact reduction (SEMAR), against standard reconstruction in cardiac computed tomography (CT) studies of patients with implanted metal and in a defibrillator lead phantom.
Methods
From a retrospective, cross-sectional clinical study with institutional review board approval of 118 patients with implanted metal, 122 cardiac CT studies from November 2009 to August 2016 performed on a 320-detector row scanner with standard and SEMAR reconstructions were included. The maximum beam hardening artifact radius, artifact attenuation variation surrounding the implanted metal, and image quality on a 4-point scale (1-no/minimal artifact to 4-severe artifact) were assessed for each reconstruction. A defibrillator lead phantom study was performed at different tube potentials and currents with both reconstruction methods. Maximum beam hardening artifact radius and average artifact attenuation variation were measured.
Results
In the clinical study, SEMAR markedly reduced the maximum beam hardening artifact radius by 77% (standard: 14.8 mm [IQR 9.7–22.2] vs. SEMAR: 3.4 mm [IQR 2.2–7.1], p < 0.0001) and artifact attenuation variation by 51% (standard: 130.0 HU [IQR 75.9–184.4] vs. SEMAR: 64.3 HU [IQR 48.2–89.2], p < 0.0001). Image quality improved with SEMAR (standard: 3 [IQR 2–3.5] vs. SEMAR: 2 [IQR 1–2.5], p < 0.0001). The defibrillator lead phantom study confirmed these results across varying tube potentials and currents.
Conclusions
SEMAR reconstruction achieved superior image quality and markedly reduced maximum beam hardening artifact radius and artifact attenuation variation compared to standard reconstruction in 122 clinical cardiac CT studies of patients with implanted metal and in a defibrillator lead phantom study.}
}
@article{SUN2019162681,
title = {A detector system for a high-energy phase-contrast human computed tomography experimental device},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {946},
pages = {162681},
year = {2019},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2019.162681},
url = {https://www.sciencedirect.com/science/article/pii/S016890021931160X},
author = {Rongqi Sun and Lian Chen and Wenbin Wei and Ge Jin},
keywords = {Phase contrast, Scintillator detector, Computed tomography, Read-out electronics},
abstract = {A detector system is designed for a high-energy phase-contrast computed tomography (CT) experimental device with an X-ray tube voltage of 80 kV. The spot size of the X-ray tube is 0.4 mm*0.7 mm. The Gd2O2S scintillator detector is adopted in our design, which is characterized by a high detection efficiency and high light yield. This system contains 2304 channels and the corresponding high-density read-out electronics. Considering the influence of the large spot size of the X-ray tube source, a detector pixel size of 0.75 mm*1 mm is adopted to obtain sufficient spatial resolution. Because the light intensity fluctuation caused by the phase shift is very weak, a 20bit ADC is used to achieve sufficient efficient resolution (ER). The electronics test shows that the SNR can reach 72. Compared to a commercial detector board, the detector system has better resolution for the sample used in the phase-contrast imaging experiment.}
}
@article{HAN2018108,
title = {Novel method for hybrid gas-dust cloud ignition using a modified standard minimum ignition energy device},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {52},
pages = {108-112},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950423017310574},
author = {Haitian Han and Purvali Chaudhari and Pranav Bagaria and Chad Mashuga},
keywords = {Minimum ignition energy, Dust explosion, Hybrid explosion, Kühner MIKE3 MIE device},
abstract = {Hybrid dust-gas explosions are a persistent problem in the process industries because of their low ignition energy and serious consequences. The probability of ignition of a hybrid dust-gas mixture is quantified by its minimum ignition energy (MIE). This study aims at improving the MIE measurement of hybrid dust-gas systems using a modified Kühner MIKE3 MIE apparatus. The modification includes an add-on device in order to purge the Hartmann tube with the gas of the hybrid mixture before the dust dispersion and ignition attempt. This ensures the gas composition in the Hartmann tube is the same as that of the gas used for dispersing the dust. In this study, the MIE for a hybrid system consisting of Pittsburgh Pulverized Coal (PPC), methane and air was examined. MIE testing was conducted for the two cases: case (a) followed the ASTM E2019-03 standard procedure, while case (b) applied a pre-dispersion purge of the Hartmann tube with the methane-air mixture. In addition, this hybrid system MIE study was conducted with two different particle sizes of PPC, having equivalent polydispersity. Certified cylinders consisting of methane (1, 2, and 3 vol %) blended with ultra-high purity (UHP) air (21.00% oxygen and 79.00% nitrogen) were used. Significant differences in the MIE values of the hybrid dust-gas system were found between the two cases for both particle sizes. The hybrid MIE values obtained with a pre-dispersion purge of the Hartmann tube were found to be lower than those without the purge (which is the most common testing approach); while a smaller particle size also resulted in a lower hybrid MIE. Additionally, it was found that for experiments with or without pre-dispersion purge, and for both particle sizes, the greater the methane concentration, the lower the MIE. This study proves that many of the previous hybrid MIE studies in the Hartmann tube are non-conservative. Future studies should be conducted by including a pre-dispersion purge in the testing procedure. This work lays the foundation for improving the existing ASTM E2019-03 or developing a standalone standard for hybrid dust-gas system MIE testing.}
}
@article{GUO2019130,
title = {Hot-N-Cold model for energy aware cloud databases},
journal = {Journal of Parallel and Distributed Computing},
volume = {123},
pages = {130-144},
year = {2019},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518306919},
author = {Chaopeng Guo and Jean-Marc Pierson and Jie Song and Christina Herzog},
keywords = {Energy efficiency, Hot-N-Cold, DVFS, Cloud database, Bounded problem},
abstract = {A lot of cloud computing and cloud database techniques are adopted in industry and academia to face the explosion of the arrival of the big data era. Meanwhile, energy efficiency and energy saving become a major concern in data centers, which are in charge of large distributed systems and cloud databases. However, the phenomenon of energy wasting is related to resource provisioning. Hot-N-Cold model is introduced in this paper, which uses workload predictions and DVFS(Dynamic Voltage and Frequency Scaling) to cope with the resource provisioning problem within energy aware cloud database systems. In this model, the resource provisioning problem is considered as two bounded problems. A nonlinear programming algorithm and a multi-phase algorithm are proposed to solve them. The experimental results show that one of the proposed algorithms has great scalability which can be applied to a cloud database system deployed on 70 nodes. Using Hot-N-Cold model can save up to 21.5% of the energy of the running time.}
}
@article{ZENG2020102060,
title = {Energy minimization for delay constrained mobile edge computing with orthogonal and non-orthogonal multiple access},
journal = {Ad Hoc Networks},
volume = {98},
pages = {102060},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.102060},
url = {https://www.sciencedirect.com/science/article/pii/S1570870518308709},
author = {Ming Zeng and Viktoria Fodor},
keywords = {Non-orthogonal multiple access (NOMA), Mobile edge computing (MEC), Energy minimization, Resource allocation, Power allocation},
abstract = {Mobile edge computing (MEC) is envisioned as a promising technology for enhancing the computation capacities and prolonging the lifespan of mobile devices, by enabling mobile devices to offload computation-intensive tasks to servers in close proximity. For wireless communication, MEC introduces a new scenario, where computations are performed directly at the receiving side of the wireless links. Our objective is therefore to evaluate the importance of joint radio-and-computational resource allocation and spectral efficiency enhancing techniques in this new scenario. We formulate the resource allocation problem to minimize the energy consumption of computation offloading of delay sensitive tasks and propose near-optimal solutions for both orthogonal and non-orthogonal multiple access schemes, with the optimal joint allocation of computing resources and transmission power. Our numerical results demonstrate the superiority of non-orthogonal multiple access over its orthogonal counterpart and the importance of joint resource allocation, especially in scenarios with strict delay limits, where both the transmission and the computational resources are scarce.}
}
@article{FELDERER2018106,
title = {Special Section: Automation and Analytics for Greener Software Engineering},
journal = {Information and Software Technology},
volume = {95},
pages = {106-107},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917305165},
author = {Michael Felderer and Dietmar Pfahl}
}
@article{FICCO2019474,
title = {Could emerging fraudulent energy consumption attacks make the cloud infrastructure costs unsustainable?},
journal = {Information Sciences},
volume = {476},
pages = {474-490},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518303955},
author = {Massimo Ficco},
keywords = {Cloud security, fraudulent energy consumption, resource exhausting attacks, cloud elasticity, abusive migration},
abstract = {Cloud paradigm is vulnerable to emerging breeds of fraudulent energy-related threats, which seek to exploit the cloud elasticity and the multi-tenant model. Recently, several sophisticated attacks have been reported by the cloud customers, which induced sustained and prolonged fraudulent resource consumptions, making the cloud costs unsustainable. If properly orchestrated, such attacks can also significantly affect the cloud service providers, forcing a frequent scaling and migration of virtual machines in the cloud. Such attacks aim at exploiting the elasticity and multi-tenacity of the cloud paradigm, in order to compromise the long-term financial viability of operating in the cloud, and thus, inflicting significant energy cost and loss of reputation to the cloud provider. This paper discusses the vulnerabilities associated to such a new breed of attacks, paying special emphasis to the risks for the cloud service providers. Practical experiments and simulations have been used to demonstrate the vulnerability of the cloud resource manager against emerging energy-related threats, named Fraudulent Energy Consumption attacks. Finally, some countermeasures are also discussed.}
}
@article{MENON2020103078,
title = {Aeroelastic response of an airfoil to gusts: Prediction and control strategies from computed energy maps},
journal = {Journal of Fluids and Structures},
volume = {97},
pages = {103078},
year = {2020},
issn = {0889-9746},
doi = {https://doi.org/10.1016/j.jfluidstructs.2020.103078},
url = {https://www.sciencedirect.com/science/article/pii/S0889974620301481},
author = {Karthik Menon and Rajat Mittal},
keywords = {Fluid–structure interaction, Aeroelastic flutter, Pitching wings, Gust response, Energy maps},
abstract = {A method to predict the aeroelastic pitch response of an airfoil to gusts is presented. The prediction is based on energy maps generated by high-fidelity fluid dynamic simulations of the airfoil with prescribed pitch oscillations. The energy maps quantify the exchange of energy between the pitching airfoil and the flow, and serve as manifolds over which the dynamical states of aeroelastic airfoil system grow, decay and attain stationary states. This method allows us to study the full nonlinear response of the system to large gusts, and predict the growth and saturation of aeroelastic pitch instabilities. We also show that the manifold topology in these maps can be used to make informed modifications to the system parameters in order to control the response to gusts.}
}
@article{KHOMH2018151,
title = {Understanding the impact of cloud patterns on performance and energy consumption},
journal = {Journal of Systems and Software},
volume = {141},
pages = {151-170},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.03.063},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218300621},
author = {Foutse Khomh and S. Amirhossein Abtahizadeh},
keywords = {Cloud patterns, Energy consumption, Performance optimization, Energy efficiency},
abstract = {Cloud patterns are abstract solutions to recurrent design problems in the cloud. Previous work has shown that these patterns can improve the Quality of Service (QoS) of cloud applications but their impact on energy consumption is still unknown. In this work, we conduct an empirical study on two multi-processing and multi-threaded applications deployed in the cloud, to investigate the individual and the combined impact of six cloud patterns (Local Database Proxy, Local Sharding Based Router, Priority Queue, Competing Consumers, Gatekeeper and Pipes and Filters) on the energy consumption. We measure the energy consumption using Power-API; an application programming interface (API) written in Java to monitor the energy consumed at the process-level. Results show that cloud patterns can effectively reduce the energy consumption of a cloud-based application, but not in all cases. In general, there appear to be a trade-off between an improved response time of the application and the energy consumption. Moreover, our findings show that migrating an application to a microservices architecture can improve the performance of the application, while significantly reducing its energy consumption. We summarize our contributions in the form of guidelines that developers and software architects can follow during the implementation of a cloud-based application.}
}