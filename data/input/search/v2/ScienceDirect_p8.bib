@article{GALBEIRO2014160,
title = {A green and efficient procedure for the preconcentration and determination of cadmium, nickel and zinc from freshwater, hemodialysis solutions and tuna fish samples by cloud point extraction and flame atomic absorption spectrometry},
journal = {Journal of Trace Elements in Medicine and Biology},
volume = {28},
number = {2},
pages = {160-165},
year = {2014},
note = {Special Section: Organometal(loid)s},
issn = {0946-672X},
doi = {https://doi.org/10.1016/j.jtemb.2013.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0946672X13002058},
author = {Rafaela Galbeiro and Samara Garcia and Ivanise Gaubeur},
keywords = {Metals ions, Cloud point extraction, Freshwater, Hemodialysis solutions, Tuna fish samples},
abstract = {Cloud point extraction (CPE) was used to simultaneously preconcentrate trace-level cadmium, nickel and zinc for determination by flame atomic absorption spectrometry (FAAS). 1-(2-Pyridilazo)-2-naphthol (PAN) was used as a complexing agent, and the metal complexes were extracted from the aqueous phase by the surfactant Triton X-114 ((1,1,3,3-tetramethylbutyl)phenyl-polyethylene glycol). Under optimized complexation and extraction conditions, the limits of detection were 0.37μgL−1 (Cd), 2.6μgL−1 (Ni) and 2.3μgL−1 (Zn). This extraction was quantitative with a preconcentration factor of 30 and enrichment factor estimated to be 42, 40 and 43, respectively. The method was applied to different complex samples, and the accuracy was evaluated by analyzing a water standard reference material (NIST SRM 1643e), yielding results in agreement with the certified values.}
}
@article{KARIMIAFSHAR2021631,
title = {A request dispatching method for efficient use of renewable energy in fog computing environments},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {631-646},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19327979},
author = {Aref Karimiafshar and Massoud Reza Hashemi and Mohammad Reza Heidarpour and Adel N. Toosi},
keywords = {Fog computing, Lyapunov optimization technique, Request dispatching, Renewable energy, Virtual queue},
abstract = {In the emerging era of Internet of Things (IoT), fog computing plays a critical role in serving delay-sensitive and location-aware applications. As a result, fog nodes are envisioned to be heavily deployed and form future distributed data centers. Powering fog nodes with green energy sources (such as solar and wind), not only helps in environmental and CO2 emission control but also paves the way towards a “sustainable IoT technology”. However, the downside of green energy is its variation and unpredictability, which needs to be engineered. In this paper, we use the Lyapunov optimization technique to derive algorithms for dynamic dispatching of the users’ requests among the nearby fog nodes and remote data centers. The proposed algorithms take into account the time constraints of the requests and maintain the system stability while efficiently utilize the available green energy sources. Exhaustive simulation results, based on solar radiation data supplied by the Australian Bureau of Meteorology, confirm the efficiency of the proposed algorithms. In particular, in terms of service time, the number of deadline misses and green energy utilization, the proposed algorithms outperform the state-of-the-art alternative up to 6%, 17% and 12%, respectively.}
}
@article{PAWALOWSKI202092,
title = {Combination of dual-energy computed tomography and iterative metal artefact reduction to increase general quality of imaging for radiotherapy patients with high dense materials. Phantom study},
journal = {Physica Medica},
volume = {77},
pages = {92-99},
year = {2020},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2020.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1120179720302015},
author = {Bartosz Pawałowski and Rafał Panek and Hubert Szweda and Tomasz Piotrowski},
keywords = {Metal artefact reduction, Dual energy CT, iMAR, Quality of CT images, Imaging for radiation therapy},
abstract = {Purpose
To evaluate the use of pseudo-monoenergetic reconstructions (PMR) from dual-energy computed tomography, combined with the iterative metal artefact reduction (iMAR) method.
Methods
Pseudo-monoenergetic CT images were obtained using the dual-energy mode on the Siemens Somatom Definition AS scanner. A range of PMR combinations (70–130 keV) were used with and without iMAR. A Virtual Water™ phantom was used for quantitative assessment of error in the presence of high density materials: titanium, alloys 330 and 600. The absolute values of CT number differences (AD) and normalised standard deviations (NSD) were calculated for different phantom positions. Image quality was assessed using an anthropomorphic pelvic phantom with an embedded hip prosthesis. Image quality was scored blindly by five observers.
Results
AD and NSD values revealed differences in CT number errors between tested sets. AD and NSD were reduced in the vicinity of metal for images with iMAR (p < 0.001 for AD/NSD). For ROIs away from metal, with and without iMAR, 70 keV PMR and pCT AD values were lower than for the other reconstructions (p = 0.039). Similarly, iMAR NSD values measured away from metal were lower for 130 keV and 70 keV PMR (p = 0.002). Image quality scores were higher for 70 keV and 130 keV PMR with iMAR (p = 0.034).
Conclusion
The use of 70 keV PMR with iMAR allows for significant metal artefact reduction and low CT number errors observed in the vicinity of dense materials. It is therefore an attractive alternative to high keV imaging when imaging patients with metallic implants, especially in the context of radiotherapy planning.}
}
@article{VEMIREDDY2021108463,
title = {Fuzzy Reinforcement Learning for energy efficient task offloading in Vehicular Fog Computing},
journal = {Computer Networks},
volume = {199},
pages = {108463},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108463},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004163},
author = {Satish Vemireddy and Rashmi Ranjan Rout},
keywords = {Vehicular Fog Computing, Fuzzy logic, Reinforcement learning, Scheduling, Energy, Optimization},
abstract = {Vehicular Fog Computing (VFC) has been envisioned as a potential fog computing paradigm which aims to offload delay sensitive tasks to mobile fog vehicles instead of remote cloud in order to facilitate computational demands of smart villages close to rural highways. There exists challenges related to task offloading in VFC that need to be addressed. Most often, Road Side Units (RSUs) deployed along rural highways are energy constrained and they need to provide energy efficient scheduling services for the allocation of tasks to fog vehicles. On the other hand, energy consumption optimization is challenging, since scheduling decision of local processing of tasks incur computation cost while the allocation of tasks to fog vehicles incurs communication cost. Although the task offloading to VFC reduces response latency, it leads to higher RSU energy consumption contributed by the communication of task data to fog vehicles. Therefore, this paper presents an energy efficient vehicle scheduling problem for offloading of tasks to mobile fog nodes subject to satisfy constraints of task deadline and resource availability. To resolve high dimensionality issue caused by increased number of vehicles in RSU coverage, we propose an on-policy reinforcement leaning based scheduling algorithm combined with fuzzy logic based greedy heuristic, named as Fuzzy Reinforcement Learning (FRL). This greedy heuristic not only accelerates learning process, but also improves long term reward when compared to Q-learning algorithm. Extensive experiments have been performed to evaluate the proposed algorithm and the simulation results show that the proposed FRL algorithm outperforms other scheduling algorithms such as First Come First Serve (FCFS), Rate Monotonic Scheduling (RMS), Fuzzy and Distributed Task Allocation with Distributed Process (DTA_DP).}
}
@article{STANGA2021109689,
title = {Novel integral expressions for computing the full energy peak efficiency of gamma spectrometry systems},
journal = {Applied Radiation and Isotopes},
volume = {172},
pages = {109689},
year = {2021},
issn = {0969-8043},
doi = {https://doi.org/10.1016/j.apradiso.2021.109689},
url = {https://www.sciencedirect.com/science/article/pii/S0969804321000993},
author = {D. Stanga and D. Gurau},
keywords = {Gamma spectrometry, Monte Carlo method, Full-energy peak efficiency},
abstract = {A new theoretical approach was developed for computing the FEP efficiency of HPGe and scintillation detectors using the concept of probability function. Thus integral expressions for the FEP efficiency were developed using three probability functions. Starting from the integral expression of the FEP efficiency and using integral means and Chebyshev functional, new expressions of the FEP efficiency, detector response, and self-attenuation factors were obtained. Also, the new approach provides new insights useful for gamma spectrometry measurements. Two practical applications of this approach are described.}
}
@article{DWIJENDRA2020273,
title = {Application of Dual-Energy Computed Tomography in Bone Lesion Biopsy},
journal = {Advances in Clinical Radiology},
volume = {2},
pages = {273-284},
year = {2020},
note = {Advances in Clinical Radiology},
issn = {2589-8701},
doi = {https://doi.org/10.1016/j.yacr.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589870120300109},
author = {Sean Dwijendra and Michael Burke},
keywords = {Dual-energy CT, Bone biopsy, Incidental lesion, Metastatic lesion, Monoenergetic CT}
}
@article{GUPTA2021117050,
title = {Energy, exergy and computing efficiency based data center workload and cooling management},
journal = {Applied Energy},
volume = {299},
pages = {117050},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117050},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921005080},
author = {Rohit Gupta and Sahar Asgari and Hosein Moazamigoodarzi and Douglas G. Down and Ishwar K. Puri},
keywords = {Data center, Workload assignment, HVAC, Energy analysis, Exergy analysis, Multi-objective optimization},
abstract = {The rapidly rising computing workloads in data centers (DCs) have necessitated new approaches to ensure effective performance and resilience that minimize the associated cooling energy. The literature on thermally-aware workload management provides strategies to reduce this energy cost, while typically ignoring the reduction in cooling capacity due to thermodynamic irreversibility and computing performance per unit energy consumption. Hence, we provide an approach that considers coefficient of performance COPc, exergy efficiency ηex, and a new metric, computing performance ratio CPR. In contrast to existing methods that consider one-dimensional workload distributions, the temperature predictions from a physics-based zonal model are used to optimize cooling for two-dimensional workload distributions in a multi-rack DC. The investigation reveals physics associated with two-dimensional workload management for multi-rack DCs, provides a framework for trade-offs between COPc, ηex, and CPR, explains the influence of IT load factor LF on different objectives, and describes how parameters obtained from single- and multi-objective problems can vary. Our findings show that COPc, and ηex can be improved by up to 20% and 8% by regulating the chilled water temperature and airflow setpoints while increasing the LF degrades the CPR by 7.5%. These results enable an extended approach for heterogeneous LF management in large-scale DCs.}
}
@article{FAN2020107570,
title = {Latency-energy optimization for joint WiFi and cellular offloading in mobile edge computing networks},
journal = {Computer Networks},
volume = {181},
pages = {107570},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107570},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620312159},
author = {Wenhao Fan and Junting Han and Le Yao and Fan Wu and Yuan’an Liu},
keywords = {Mobile edge computing, Computation offloading, Non-convex optimization},
abstract = {Mobile terminals (MTs) within the coverage of both the WiFi and cellular network can offload computation tasks via the WiFi access point to efficiently release the cellular network congestion and reduce the load on base stations. In this paper, a novel scheme of joint WiFi and cellular offloading is proposed to optimally reduce the latency and energy consumption of MTs in task processing. Based on the statistical characteristics of MTs’ task generation, our scheme serves as strategic guidance for computation offloading without frequent execution of the optimization algorithm. MTs with the different channel access priorities are also considered in our scheme. A balance factor is introduced to flexibly adjust the minimization between the energy consumption of MT and the processing latency of its tasks. We design an iterative algorithm combining the linear programming with the Alternating Optimization technique to efficiently solve the non-convex problem for computation tasks offloading decision. The numerical results demonstrate that our scheme can largely improve the system performance, and the superiority of our scheme is shown in all scenarios.}
}
@article{HUANG2018340,
title = {Computing the full spectrum of large sparse palindromic quadratic eigenvalue problems arising from surface Green's function calculations},
journal = {Journal of Computational Physics},
volume = {356},
pages = {340-355},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2017.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999117308951},
author = {Tsung-Ming Huang and Wen-Wei Lin and Heng Tian and Guan-Hua Chen},
keywords = {Palindromic quadratic eigenvalue problem, G⊤SHIRA, Non-equivalence deflation, Surface Green's function, Quantum transport},
abstract = {Full spectrum of a large sparse ⊤-palindromic quadratic eigenvalue problem (⊤-PQEP) is considered arguably for the first time in this article. Such a problem is posed by calculation of surface Green's functions (SGFs) of mesoscopic transistors with a tremendous non-periodic cross-section. For this problem, general purpose eigensolvers are not efficient, nor is advisable to resort to the decimation method etc. to obtain the Wiener–Hopf factorization. After reviewing some rigorous understanding of SGF calculation from the perspective of ⊤-PQEP and nonlinear matrix equation, we present our new approach to this problem. In a nutshell, the unit disk where the spectrum of interest lies is broken down adaptively into pieces small enough that they each can be locally tackled by the generalized ⊤-skew-Hamiltonian implicitly restarted shift-and-invert Arnoldi (G⊤SHIRA) algorithm with suitable shifts and other parameters, and the eigenvalues missed by this divide-and-conquer strategy can be recovered thanks to the accurate estimation provided by our newly developed scheme. Notably the novel non-equivalence deflation is proposed to avoid as much as possible duplication of nearby known eigenvalues when a new shift of G⊤SHIRA is determined. We demonstrate our new approach by calculating the SGF of a realistic nanowire whose unit cell is described by a matrix of size 4000×4000 at the density functional tight binding level, corresponding to a 8×8nm2 cross-section. We believe that quantum transport simulation of realistic nano-devices in the mesoscopic regime will greatly benefit from this work.}
}
@article{AN2010883,
title = {Simultaneous spectrophotometric determination of trace amount of malachite green and crystal violet in water after cloud point extraction using partial least squares regression},
journal = {Journal of Hazardous Materials},
volume = {175},
number = {1},
pages = {883-888},
year = {2010},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2009.10.092},
url = {https://www.sciencedirect.com/science/article/pii/S0304389409017439},
author = {Lin An and Jian Deng and Liang Zhou and Hui Li and Fei Chen and Hui Wang and Yating Liu},
keywords = {Cloud point extraction, Spectrophotometry, Malachite green, Crystal violet, Partial least squares regression},
abstract = {In this work, a new method has been proposed to simultaneously determine the trace amount of malachite green and crystal violet from aqueous solution by spectrophotometry after cloud point extraction (CPE) using partial least squares regression. The optimal extraction and operating conditions, such as pH, reagents concentration and effect of time and temperature, and so on, have been investigated using the non-ionic surfactant Triton X-114. The maximum absorption wavelength for malachite green and crystal violet is 624 and 579nm, respectively; linearity is obeyed in the range of 9.9–800 and 16–1000ngmL−1 with detection limit of 2.9 and 4.8ngmL−1, and the root mean square error of prediction (RMSEP) are 0.0197 and 0.0343, respectively. The proposed method has been applied successfully to simultaneously determine the trace amount of malachite green and crystal violet in real matrix samples with the recoveries of 92.45–102.5%.}
}
@article{KOTLYAROV2021109692,
title = {Dual-energy computed tomography: Tube current settings and detection of uric acid tophi},
journal = {European Journal of Radiology},
volume = {139},
pages = {109692},
year = {2021},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2021.109692},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X21001728},
author = {Maximilian Kotlyarov and Kay Geert A. Hermann and Jürgen Mews and Bernd Hamm and Torsten Diekhoff},
keywords = {Tomography, X-Ray computed, Arthritis, Gouty, Uric acid, Phantoms, Imaging},
abstract = {Purpose
To derive optimal scanning parameters for single-source dual-energy computed tomography (DECT) in the detection of urate by analyzing influence of tube current ratio (TCR) and total radiation exposure in a phantom.
Method
Specimens with different urate concentrations in a realistic porcine bio-phantom were repeatedly imaged with sequential single-source DECT scans at 80 kVp (16.5–220 mA s) and 135 kVp (2.75–19.25 mA s). Detection index (DI - true positive minus false positive urate volume) was calculated for every possible tube current combination. Optimal tube current combinations reaching at least 85 % of the highest measured DI of all combinations without exceeding 150 % of equivalent single-energy radiation dose were identified. TCR, DLP and DI were plotted and compared.
Results
Cubic regression analysis showed a flattening increase in the DI with increasing tube currents. Five out of the 100 tube current combinations analyzed achieved the detection target: the lowest DLP of 53.9 mGy*cm at 19.25/16.5 mAs (135/80 kVp) achieved a DI of 2.07 mL and the highest DI of 2.11 mL at a dose of 65.3 mGy*cm and 8.25/79.75 mAs. The optimal TCR is between two and four, while both, higher and lower ratios decreased DI.
Conclusions
A minimum tube current of the high-energy scans is needed before an acceptable overall sensitivity is achieved and before increases in low-energy exposure result in more urate detection. High TCRs above 10 are not beneficial while the optimal TCR ranges between two and four, indicating that special care has to be taken in designing a suitable DECT protocol.}
}
@article{MORO2020102511,
title = {Emerging technologies in the renewable energy sector: A comparison of expert review with a text mining software},
journal = {Futures},
volume = {117},
pages = {102511},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2020.102511},
url = {https://www.sciencedirect.com/science/article/pii/S001632872030001X},
author = {Alberto Moro and Geraldine Joanny and Christian Moretti},
keywords = {Bibliometrics, Text mining, Emerging technologies, Renewable energy, Quantitative and qualitative, Policy support},
abstract = {This paper compares the results from quantitative text mining to qualitative expert reviews to identify emerging technologies in the fields of solar photovoltaics (PV), wind power, ocean and tidal energy, hydropower. The text mining analysis is based on the software “Tools for Innovation Monitoring” (TIM). The TIM software extracts a set of relevant keywords from a corpus of pertinent scientific publications. TIM outputs are compared to those extracted by the software VOSviewer, showing agreement. The top 300 ranked keywords are the optimum trade-off between retrieved technologies and analyst efforts. The emerging technologies identified by the experts can be retrieved in the top 300 keywords with a probability of 65 %, 25 %, depending on the technology sector and the algorithm adopted. The more salient keywords tend to correspond to technologies with an established and univocal jargon such as: "dye sensitised solar cells" or "vertical axis wind turbines". Two methods are here used and compared: the frequency of author keywords and the term frequency-inverse document frequency (TF-IDF) algorithm. The comparison of their performances is not showing a general prevalence of one method against the other, but rather a different suitability to different technology sectors.}
}
@article{OHIRA20218,
title = {Improvement of image quality for pancreatic cancer using deep learning-generated virtual monochromatic images: Comparison with single-energy computed tomography},
journal = {Physica Medica},
volume = {85},
pages = {8-14},
year = {2021},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2021.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S1120179721001526},
author = {Shingo Ohira and Yuhei Koike and Yuichi Akino and Naoyuki Kanayama and Kentaro Wada and Yoshihiro Ueda and Akira Masaoka and Hayate Washio and Masayoshi Miyazaki and Masahiko Koizumi and Kazuhiko Ogawa and Teruki Teshima},
keywords = {Dual-energy CT, Deep learning, Virtual monochromatic image, Pancreas},
abstract = {Purpose
To construct a deep convolutional neural network that generates virtual monochromatic images (VMIs) from single-energy computed tomography (SECT) images for improved pancreatic cancer imaging quality.
Materials and methods
Fifty patients with pancreatic cancer underwent a dual-energy CT simulation and VMIs at 77 and 60 keV were reconstructed. A 2D deep densely connected convolutional neural network was modeled to learn the relationship between the VMIs at 77 (input) and 60 keV (ground-truth). Subsequently, VMIs were generated for 20 patients from SECT images using the trained deep learning model.
Results
The contrast-to-noise ratio was significantly improved (p < 0.001) in the generated VMIs (4.1 ± 1.8) compared to the SECT images (2.8 ± 1.1). The mean overall image quality (4.1 ± 0.6) and tumor enhancement (3.6 ± 0.6) in the generated VMIs assessed on a five-point scale were significantly higher (p < 0.001) than that in the SECT images (3.2 ± 0.4 and 2.8 ± 0.4 for overall image quality and tumor enhancement, respectively).
Conclusions
The quality of the SECT image was significantly improved both objectively and subjectively using the proposed deep learning model for pancreatic tumors in radiotherapy.}
}
@article{OHIRA2021328,
title = {Dual-energy computed tomography image-based volumetric-modulated arc therapy planning for reducing the effect of contrast-enhanced agent on dose distributions},
journal = {Medical Dosimetry},
volume = {46},
number = {4},
pages = {328-334},
year = {2021},
issn = {0958-3947},
doi = {https://doi.org/10.1016/j.meddos.2021.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0958394721000261},
author = {Shingo Ohira and Riho Komiyama and Yuhei Koike and Hayate Washio and Naoyuki Kanayama and Shoki Inui and Yoshihiro Ueda and Masayoshi Miyazaki and Masahiko Koizumi and Teruki Teshima},
abstract = {To compare the effect of a contrast-enhanced (CE) agent on volumetric-modulated arc therapy plans based on four types of images—virtual monochromatic images (VMIs) captured at 70 and 140 keV (namely VMI70 and VMI140, respectively), water density image (WDI), and virtual non-contrast image (VNC) generated using a dual-energy computed tomography (DECT) system. A tissue characterization phantom and a multi-energy phantom were scanned, and VMI70, VMI140, WDI, and VNC were retrospectively reconstructed. For each image, a lookup table (LUT) was created. For 13 patients with nasopharyngeal cancer, non-CE and CE scans were performed, and volumetric-modulated arc therapy plans were generated on the basis of non-CE VMI70. Subsequently, the doses were re-calculated using the four types of DECT images and their corresponding LUTs. The maximum differences in the physical density estimation were 21.3, 5.2, −3.9, and 0.5% for VMI70, VMI140, WDI, and VNC, respectively. Compared with VMI70, the WDI approach significantly reduced (p < 0.05) the dosimetric difference due to the CE agent for the planning target volume (PTV) (D50%), whereas the difference was significantly increased for D1%. Except for PTV (D1%), the differences were significantly lower (p < 0.05) in the treatment plans based on VMI140 and VNC than that based on VMI70. For the VNC, the mean difference was less than 0.2% for all dosimetric parameters for the PTV. For patients with NPC, treatment plans based on the VNC derived from CE scan showed the best agreement with those based on the non-CE VMI70. Ideally, the effect of CE agent on dose distribution does not appear in treatment planning procedures.}
}
@article{AHERN20211738,
title = {A meta-analysis of the diagnostic accuracy of Hounsfield units on computed topography relative to dual-energy X-ray absorptiometry for the diagnosis of osteoporosis in the spine surgery population},
journal = {The Spine Journal},
volume = {21},
number = {10},
pages = {1738-1749},
year = {2021},
issn = {1529-9430},
doi = {https://doi.org/10.1016/j.spinee.2021.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1529943021001194},
author = {Daniel P. Ahern and Jake M. McDonnell and Mathieu Riffault and Shane Evans and Scott C. Wagner and Alexander R. Vaccaro and David A. Hoey and Joseph S. Butler},
abstract = {BACKGROUND
The preoperative identification of osteoporosis in the spine surgery population is of crucial importance. Limitations associated with dual-energy x-ray absorptiometry, such as access and reliability, have prompted the search for alternative methods to diagnose osteoporosis. The Hounsfield Unit(HU), a readily available measure on computed tomography, has garnered considerable attention in recent years as a potential diagnostic tool for reduced bone mineral density. However, the optimal threshold settings for diagnosing osteoporosis have yet to be determined.
METHODS
We selected studies that included comparison of the HU(index test) with dual-energy x-ray absorptiometry evaluation(reference test). Data quality was assessed using the standardised QUADAS-2 criteria. Studies were characterised into 3 categories, based on the threshold of the index test used with the goal of obtaining a high sensitivity, high specificity or balanced sensitivity-specificity test.
RESULTS
9 studies were eligible for meta-analysis. In the high specificity group, the pooled sensitivity was 0.652 (95% CI 0.526 – 0.760), specificity 0.795 (95% CI 0.711 – 0.859) and diagnostic odds ratio was 6.652 (95% CI 4.367 – 10.133). In the high sensitivity group, the overall pooled sensitivity was 0.912 (95% CI 0.718 – 0.977), specificity was 0.67 (0.57 – 0.75) and diagnostic odds ratio was 19.424 (5.446 – 69.275). In the balanced sensitivity-specificity group, the overall pooled sensitivity was 0.625 (95% CI 0.504 – 0.732), specificity was 0.914 (0.823 – 0.960) and diagnostic odds ratio was 14.880 (7.521 – 29.440). Considerable heterogeneity existed throughout the analysis.
CONCLUSION
In conclusion, the HU is a clinically useful tool to aide in the diagnosis of osteoporosis. However, the heterogeneity seen in this study warrants caution in the interpretation of results. We have demonstrated the impact of differing HU threshold values on the diagnostic ability of this test. We would propose a threshold of 135 HU to diagnose OP. Future work would investigate the optimal HU cut-off to differentiate normal from low bone mineral density.}
}
@article{SHARMA2021100592,
title = {Scheduling computing loads for improved utilization of solar energy},
journal = {Sustainable Computing: Informatics and Systems},
volume = {32},
pages = {100592},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100592},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000810},
author = {Divya Sharma and Shrisha Rao},
keywords = {Green energy, Brown energy, Ren-percent, Data centers, Renewable energy, Job scheduling, Solar energy, Computing load},
abstract = {The rise in the penetration of the internet across the world has led to a rapid increase in the consumption of energy at the data centers established by leading cloud data service providers. High power consumption by these data centers [DCs] leads to high operational costs and high carbon emissions into the environment. From a sustainability point of view, the ultimate goal is to maximize the productivity and efficiency of these data centers while keeping greenhouse gas emissions to the minimum and maximize data center productivity. This goal can be achieved by better resource utilization and replacing carbon-intensive approaches of energy production with green sources of energy. Due to the limited intermittent availability of renewable sources of energy, the ideal ‘Green’ design for the DCs, should incorporate inter-operability with both renewable and non-renewable sources of energy. In this paper, we propose a ren-aware scheduler to schedule computational workload by prioritizing their execution within the duration of green energy availability on the basis of the predicted hourly green energy and workload data of DCs. Our results demonstrate that our ren-aware scheduler can increase the green energy consumption by 51% compared to the conventional randomized scheduler that distributes load without considering green energy and load. It can also reduce the total energy consumption by 25% by putting the DCs to sleep during their idle time, as it saves 4.5 times more idle energy than the randomized scheduler. Additionally, the results also demonstrate how the role of time zones of the DCs and the duration of green energy availability in them is pivotal in our ren-aware scheduler's performance.}
}
@article{HAMEED2021100454,
title = {Energy- and performance-aware load-balancing in vehicular fog computing},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100454},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100454},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920301797},
author = {Ahmad Raza Hameed and Saif ul Islam and Ishfaq Ahmad and Kashif Munir},
keywords = {Vehicular fog computing, VANET, Fog computing, Internet of things, Green computing, Quality of service, Energy efficiency, Load-balancing},
abstract = {An IoT-enabled cluster of automobiles provides a rich source of computational resources, in addition to facilitating efficient collaboration with vehicle-to-vehicle and vehicle-to-infrastructure communication. This is enabled by vehicular fog computing where vehicles are used as fog nodes and provide cloud-like services to the Internet of things (IoT) and are further integrated with the traditional cloud to collaboratively complete the tasks. However, efficient load management in vehicular fog computing is a challenging task due to the dynamic nature of the vehicular ad-hoc network (VANET). In this context, we propose a cluster-enabled capacity-based load-balancing approach to perform energy- and performance-aware vehicular fog distributed computing for efficiently processing the IoT jobs. The paper proposes a dynamic clustering approach that takes into account the position, speed, and direction of vehicles to form their clusters that act as the pool of computing resources. The paper also proposes a mechanism for identifying a vehicle's departure time from the cluster, which allows predicting the future position of the vehicle within the dynamic network. Furthermore, the paper provides a capacity-based load-distribution mechanism for performing load-balancing at the intra- as well as the inter-cluster level of the vehicular fog network. The simulation results are obtained using the state-of-the-art NS2 network simulation environment. The results show that the proposed scheme achieves balanced network energy consumption, reduced network delay, and improved network utilization.}
}
@incollection{WAGH2022111,
title = {Chapter 6 - Comparison of open access multi-objective optimization software tools for standalone hybrid renewable energy systems},
editor = {Pandian Vasant and Joshua Thomas and Elias Munapo and Gerhard-Wilhelm Weber},
booktitle = {Advances of Artificial Intelligence in a Green Energy Environment},
publisher = {Academic Press},
pages = {111-128},
year = {2022},
isbn = {978-0-323-89785-3},
doi = {https://doi.org/10.1016/B978-0-323-89785-3.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323897853000104},
author = {Mahesh Wagh and Purshottam Acharya and Vivek Kulkarni},
keywords = {Comparison of open source software, Hybrid renewable energy system, Levelized cost of energy, Multiobjective optimization tools},
abstract = {Since time immemorial, human beings are using energy for growth and survival. Energies are categorized mainly as renewable and nonrenewable energies. Renewable energy proved to be more reliable and pollution-free. However, renewable energy systems when used along with some conventional resources such as diesel generator sets or with other renewable energy sources give the maximum benefit as well as reliability and lower cost than a single renewable energy system. The most used systems nowadays are PV-wind-battery, PV-wind-diesel-battery and PV-wind-diesel systems. The optimization is required in these systems for lowering the two main parameters which are the total levelized cost of energy and the net present value of the system. So, the main objective is to minimize the total levelized cost of energy or to minimize net present value. The optimization of these systems is sometimes multiobjective type, because in the case of bigger projects, the engineer needs to consider the environmental effects like total emission of CO2 in the atmosphere. The optimized solution of this kind of problem requires a software tool because of a greater number of variables and the nonlinear behavior of some system components. In this chapter, a comparison of open access software tools has been carried out.}
}
@article{DANGELO2020109166,
title = {Carotid and cerebrovascular dual-energy computed tomography angiography: Optimization of window settings for virtual monoenergetic imaging reconstruction},
journal = {European Journal of Radiology},
volume = {130},
pages = {109166},
year = {2020},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2020.109166},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X20303557},
author = {Tommaso D'Angelo and Lukas Lenga and Christophe T. Arendt and Andreas M. Bucher and Julia L. Peterke and Damiano Caruso and Silvio Mazziotti and Giorgio Ascenti and Alfredo Blandino and Ahmed E. Othman and Simon S. Martin and Moritz H. Albrecht and Boris Bodelle and Thomas J. Vogl and Julian L. Wichmann},
keywords = {Carotid disease, Dual-energy, Computed tomography, Computed tomography angiography, Image postprocessing},
abstract = {Purpose
Dedicated post-processing of dual-energy computed tomography angiography (DE-CTA) datasets has been shown to allow for increased vascular contrast. The goal of our study was to define optimal window settings for displaying virtual monoenergetic images (VMI) reconstructed from dual-energy carotid and cerebrovascular DE-CTA.
Methods
Fifty-seven patients who underwent clinically-indicated carotid and cerebrovascular third-generation dual-source DE-CTA were retrospectively evaluated. Standard linearly-blended (M_0.6), 70-keV traditional VMI (M70), and 40-keV noise-optimized VMI (M40+) reconstructions were analyzed. For M70 and M40+ datasets, the subjectively best window setting (width and level, B–W/L) was independently determined by two observers and subsequently related with aortic arch attenuation to calculate optimized values (O–W/L) using linear regression. Subjective evaluation of image quality (IQ) between W/L settings were assessed by two additional readers. Repeated measures analysis of variance were performed to compare W/L settings and IQ indices between M_0.6, M70, and M40 + .
Results
B–W/L and O–W/L for M70 were 580/210 and 560/200, and for M40+ were 1630/570 and 1560/550, respectively, higher than standard DE-CTA W/L settings (450/100). Highest subjective scores were observed for M40+ regarding overall IQ (all p < 0.001).
Conclusion
Application of O–W/L settings is mandatory to optimize subjective IQ of VMI reconstructions of DE-CTA.}
}
@article{XIA2021101991,
title = {SparkNoC: An energy-efficiency FPGA-based accelerator using optimized lightweight CNN for edge computing},
journal = {Journal of Systems Architecture},
volume = {115},
pages = {101991},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.101991},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000138},
author = {Ming Xia and Zunkai Huang and Li Tian and Hui Wang and Victor Chang and Yongxin Zhu and Songlin Feng},
keywords = {FPGA, CNN, SparkNet, Hardware acceleration, Edge computing},
abstract = {Over the past few years, Convolution Neural Networks (CNN) have been extensively adopted in broad AI applications and have achieved noticeable effect. Deploying the feedforward inference of CNN on edge devices has now been considered a research hotspot in Edge Computing. In terms of the mobile embedded devices that exhibit constrained resources and power budget, the considerable parameters and computational bottlenecks raised rigorous requirements of deploying the CNN feedforward inference. To address this challenge, the present study develops a lightweight neural network architecture termed as SparkNet, capable of significantly reducing the weight parameters and computation demands. The feasibility of the SparkNet is verified on four datasets, i.e., MINIST, CIFAR-10, CIFAR-100 and SVHN. Besides, the SparkNet is reported exhibiting the ability to effectively compress the convolutional neural network by a factor of 150x. Compared with GPU and ASIC, an FPGA-based accelerator exhibits obvious advantages for its reconfigurable property, flexibility, power efficiency, as well as massive parallelism. Moreover, the network model of the SparkNet and the proposed accelerator architecture are both specifically built for FPGA. The SparkNet on chip (SparkNOC) that maps all the layers of the network to their own dedicated hardware unit for simultaneous pipelined work has been implemented on FPGA. The proposals of this study are assessed by deploying SparkNet model on Intel Arria 10 GX1150 FPGA platform. As revealed from the experimental results, the fully pipelined CNN hardware accelerator achieves 337.2 GOP/s performance under the energy efficiency of 44.48 GOP/s/w, indicating that it outperforms the previous methods.}
}
@article{RUIZMUNOZ2021100721,
title = {Diagnostic value of quantitative parameters for myocardial perfusion assessment in patients with suspected coronary artery disease by single- and dual-energy computed tomography myocardial perfusion imaging},
journal = {IJC Heart & Vasculature},
volume = {32},
pages = {100721},
year = {2021},
issn = {2352-9067},
doi = {https://doi.org/10.1016/j.ijcha.2021.100721},
url = {https://www.sciencedirect.com/science/article/pii/S2352906721000099},
author = {Aroa Ruiz-Muñoz and Filipa Valente and Lydia Dux-Santoy and Andrea Guala and Gisela Teixidó-Turà and Laura Galián-Gay and Laura Gutiérrez and Rubén Fernández-Galera and Guillem Casas and Teresa González-Alujas and Ignacio Ferreira-González and Arturo Evangelista and José Rodríguez-Palomares},
keywords = {Coronary artery disease, Myocardial ischaemia, CT myocardial perfusion imaging, Transmural perfusion ratio, Dual-energy CT-based iodine imaging},
abstract = {Purpose
To compare performance of visual and quantitative analyses for detecting myocardial ischaemia from single- and dual-energy computed tomography (CT) in patients with suspected coronary artery disease (CAD).
Methods
Eighty-four patients with suspected CAD were scheduled for dual-energy cardiac CT at rest (CTA) and pharmacological stress (CTP). Myocardial CT perfusion was analysed visually and using three parameters: mean attenuation density (MA), transmural perfusion ratio (TPR) and myocardial perfusion reserve index (MPRI), on both single-energy CT and CT-based iodine images. Significant CAD was defined in AHA-segments by concomitant myocardial hypoperfusion identified visually or quantitatively (parameter < threshold) and coronary stenosis detected by CTA. Single-photon emission CT and invasive coronary angiography were used as reference. Perfusion-parameter cut-off values were calculated in a randomly-selected subgroup of 30 patients.
Results
The best-performing thresholds for TPR, MPRI and MA were 0.96, 23 and 0.5 for single-energy CT and 0.97, 47 and 0.3 for iodine imaging. For both CT-imaging modalities, TPR yielded the highest area under receiver operating characteristic curve (AUC) (0.99 and 0.97 for single-energy CT and iodine imaging, respectively, in vessel-based analysis) compared to visual analysis, MA and MPRI. Visual interpretation on iodine imaging resulted in higher AUC compared to that on single-energy CT in per-vessel (AUC: 0.93 vs 0.86, respectively) and per-patient (0.94 vs 0.93) analyses.
Conclusion
Transmural perfusion ratio on both CT-imaging modalities is the best-performing parameter for detecting myocardial ischaemia compared to visual method and other perfusion parameters. Visual analysis on CT-based iodine imaging outperforms that on single-energy CT.}
}
@article{HAO20191142,
title = {Adaptive energy-aware scheduling method in a meteorological cloud},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1142-1157},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.061},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19309471},
author = {Yongsheng Hao and Jie Cao and Tinghuai Ma and Sai Ji},
keywords = {Energy-aware, Moldable parallel tasks, Scheduling resources, Speedup, Tradeoff},
abstract = {This paper focuses on the energy-aware scheduling problem of moldable non-linear parallel tasks in a meteorological cloud. Such a meteorological Cloud mainly provides computing resources for the execution of meteorological models, such as Weather Research and Forecasting model (WRF). In a meteorological Cloud, the parallelism of tasks (i.e., meteorological models) can only be configured in the beginning, and the assigned resources retained exclusively until all sub-tasks have been finished. For the scheduling of those tasks, one key challenge is how to reduce the average energy consumption while guaranteeing others requirements of such tasks. We address this challenge by considering simultaneously the deadlines of tasks, the energy consumption, the system load, and the non-linear speedup of parallel tasks when we make the scheduling decision. Specifically, we propose an adaptive energy-aware scheduling method called ASSD, that is based on the Dynamic Voltage and Frequency Scaling (DVFS) model of computing resources and the speedup of tasks under different parallelisms. We evaluate our method via simulations on a meteorological cloud. Our results show that the proposed method not only increases the number of completed tasks but also significantly reduces the average energy consumption.}
}
@article{DAS2021107350,
title = {Multi-objective optimization of hybrid renewable energy system by using novel autonomic soft computing techniques},
journal = {Computers & Electrical Engineering},
volume = {94},
pages = {107350},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107350},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621003219},
author = {Gourab Das and M. De and K.K. Mandal},
keywords = {Micro grid, Economic scheduling, Environmental scheduling, Multi-objective Particle Swarm Optimization},
abstract = {An increasing demand in electric power consumption has clearly led to an exhaustion of alternating energy resources. Undoubtedly, it has harmful environmental effects. Hybrid energy and Micro grid can solve this kind of problem. The concept of micro grid is quite significant in cases where transmission of electric power is nither feasible nor profitable. An efficient scheduling of micro grid is able to meet load demand without shedding any load and the optimization is required to make it profitable and eco-friendly. In this regard this work implements a twenty four hours based environmental/economic scheduling of distributed generating units with renewable energy sources in a micro grid connected with main grid .This work proposes a framework for optimal scheduling of micro grid which minimize the cost of generating units as well as emission. Particle Swarm Optimization technique has been employed to solve this problem. Weighting factor is used for optimization in multi-objective framework where both cost and emission are minimized simultaneously. In this paper, a comparative study of employing different types of Particle Swarm Optimization has been made where Hierarchical Particle Swarm Optimization (HPSO) performs better incorporating different constraints. The results of proposed Particle Swarm Optimization method are compared and verified with results of others method which is recently employed. Finally, the comparative study indicates that proposed method gives superior solution than previous method in case of operating cost and emission.}
}
@incollection{SAMADDDER2022475,
title = {Subchapter 7.3 - Why shale gas is a prime option for us from energy perspective and the multitasking software to address the related issues},
editor = {A.K. Moitra and J.R. Kayal and Biplab Mukerji and Jayanta Bhattacharya and A.K. Das},
booktitle = {Innovative Exploration Methods for Minerals, Oil, Gas, and Groundwater for Sustainable Development},
publisher = {Elsevier},
pages = {475-483},
year = {2022},
isbn = {978-0-12-823998-8},
doi = {https://doi.org/10.1016/B978-0-12-823998-8.00097-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239988000971},
author = {Asit Kumar Samaddder},
keywords = {Reservoir, Energy resource, TOC (total organic content), NMR porosity, Organic maturity, DTC and DTS, Unhindered perforation},
abstract = {Ever increasing requirement of energy has led to investigation of unconventional energy resource, among them Shale Gas prospect is emerging rapidly as promising energy source globally. Unlocking of domestic shale gas reserves could help India meet its growing energy demand, besides reducing its dependence on expensive energy imports of crude oil and natural gas. In addition, development of the domestic shale gas industry could boost economy and make it self-reliant of future growth. Exploration of shale gas reserves in India will help it meet its growing energy demand. The first-ever R&D shale gas well was drilled by ONGC in Damodar valley in 2010. Evaluating the shale gas and oil resources of India posed many challenges. Available data were limited on the geologic setting and reservoir properties of the shale formations in India. The author has substantiated success story of shale gas prospect in India by application of scientific software.}
}
@article{FOTI2021341,
title = {Identification of bone marrow edema around the ankle joint in non-traumatic patients: Diagnostic accuracy of dual-energy computed tomography},
journal = {Clinical Imaging},
volume = {69},
pages = {341-348},
year = {2021},
issn = {0899-7071},
doi = {https://doi.org/10.1016/j.clinimag.2020.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0899707120303545},
author = {Giovanni Foti and Massimo Guerriero and Niccolò Faccioli and Alessandro Fighera and Luigi Romano and Claudio Zorzi and Giovanni Carbognin},
keywords = {Ankle joint, Magnetic resonance imaging, Multidetector computed tomography, Bone marrow},
abstract = {Objectives
To evaluate the diagnostic accuracy of DECT in the identification of BME of the ankle in non-traumatic patients.
Methods
This prospective institutional review board approved study included 40 consecutive patients (21 males and 19 females, mean age 56.8 years, SD = 11.37) that were examined using DECT and MRI in the period between April 2019 and January 2020. Two radiologists (7 and 16 years of experience) evaluated the presence of BME on DECT mages. Diagnostic accuracy values for diagnosing BME on a per-patient and on a per-partition basis analysis were calculated for DECT images by two readers (R1 and R2, with 16 and 7 years of experience, respectively), using MRI as a gold-standard for diagnosis. Inter-observer agreements were calculated with k-statistics. A p-value of <0.05 was considered as statistically significant.
Results
MRI depicted BME in 29/40 patients (72.50%) and in 43/240 partitions (17.91%). The consensus reading by R1 and R2 of DECT images allowed us to achieve 89.7% sensitivity (26/29 patients) and 81.8% specificity (9/11 patients). Regarding the partitions-basis analysis, BME was depicted by DECT in 39/43 partitions (90.69% sensitivity), and ruled out in 189/197 partitions (95.93% specificity). Sensitivity and specificity for the most involved partitions (talar dome) were both 95%. The inter-observer agreement for patients' analysis was substantial (k = 0.697), whereas for the partitions' analysis, it ranged from substantial (k = 0.724) to near perfect (k = 0.950).
Conclusions
DECT can accurately diagnose BME of the ankle in a cohort of non-traumatic patients.}
}
@article{CHEN2009231,
title = {Study of adsorption behavior of malachite green on polyethylene glycol micelles in cloud point extraction procedure},
journal = {Colloids and Surfaces A: Physicochemical and Engineering Aspects},
volume = {345},
number = {1},
pages = {231-236},
year = {2009},
issn = {0927-7757},
doi = {https://doi.org/10.1016/j.colsurfa.2009.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0927775709003112},
author = {Jianwei Chen and Jianwei Mao and Xiaorong Mo and Juying Hang and Mingmin Yang},
keywords = {Adsorption, Cloud point extraction, Langmuir isotherm, Polyethylene glycol, Malachite green},
abstract = {Cloud point extraction (CPE) was carried out to extract malachite green (MG) from aqueous solution and shrimp samples using a series of polyethylene glycol (PEG) surfactants: PEG10000, PEG6000, PEG2000 and PEG600. The adsorption mechanism between PEG micelles and MG molecules was studied. The data of equilibrium concentrations and adsorption amounts in the four PEG–MG systems followed the Langmuir type isotherm. On some assumptions, a developed Langmuir isotherm was used to calculate the feed surfactant concentration required for the removal of MG up to an extraction efficiency of 90%. The calculated PEG concentrations were used in CPE process, and other influence factors on phase behavior were investigated. Under the optimal conditions, recoveries of MG were 82.16–92.41% in aqueous solution and 78.16–86.54% in shrimp samples.}
}
@article{VATTAY2022S14,
title = {428 The Impact Of Virtual Monoenergetic Image Energy Levels On Pericoronary Adipose Tissue Attenuation Using Dual-source Photon-counting Detector Computed Tomography},
journal = {Journal of Cardiovascular Computed Tomography},
volume = {16},
number = {4, Supplement },
pages = {S14},
year = {2022},
issn = {1934-5925},
doi = {https://doi.org/10.1016/j.jcct.2022.06.033},
url = {https://www.sciencedirect.com/science/article/pii/S1934592522001319},
author = {B. Vattay and M. Boussoussou and A. Bartykowszki and M. Kolossváry and G. Konkoly and M. Vecsey-Nagy and A. Kubovje and B. Merkely and P. Maurovich-Horvat and B. Szilveszter}
}
@article{FLORESLARSEN201556,
title = {Modeling double skin green façades with traditional thermal simulation software},
journal = {Solar Energy},
volume = {121},
pages = {56-67},
year = {2015},
note = {ISES Solar World Congress 2013 (SWC2013) Special Issue},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2015.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X15004697},
author = {Silvana {Flores Larsen} and Celina Filippín and Graciela Lesino},
keywords = {Green wall, Evapotranspiration, Double façades, Plants},
abstract = {The use of plants attached to the building walls is a bioclimatic strategy that has grown in popularity due to the savings in building energy consumption. The plant is a living component of the façade that responds to the environment in a very complicated way, by regulating their transpiration levels. The simulation of this response is generally not included in the available software for transient thermal simulation of buildings, thus making difficult the simulation of green walls by architects and building designers. The aim of this paper is to present a simplified method to simulate a green wall using a traditional wall/glazing element, with fictitious properties, whose thermal model is included in transient simulation softwares. Thus, green walls can be simulated with softwares that do not provide specific modules for plant calculation. The model is more accurate under humid conditions and for low wind speeds. An application example is presented, consisting of a building prototype with a green façade that was simulated through EnergyPlus software. Inside and outside glass temperatures, plant foliage temperature, and window heat gain and losses were calculated. The results were discussed and recommendations for simulating green façades were done.}
}
@article{DOROTA2021102538,
title = {Corrigendum to “An analysis method for data taken by Imaging Air Cherenkov Telescopes at very high energies under the presence of clouds” [Astroparticle Physics 120 (2020) 102450]},
journal = {Astroparticle Physics},
volume = {132},
pages = {102538},
year = {2021},
issn = {0927-6505},
doi = {https://doi.org/10.1016/j.astropartphys.2020.102538},
url = {https://www.sciencedirect.com/science/article/pii/S0927650520301109},
author = {Sobczyńska Dorota and Adamczyk Katarzyna and Sitarek Julian and Szanecki Michał}
}
@article{LAMBERTI2021102833,
title = {Carbon dioxide diffuse degassing as a tool for computing the thermal energy release at Cerro Blanco Geothermal System, Southern Puna (NW Argentina)},
journal = {Journal of South American Earth Sciences},
volume = {105},
pages = {102833},
year = {2021},
issn = {0895-9811},
doi = {https://doi.org/10.1016/j.jsames.2020.102833},
url = {https://www.sciencedirect.com/science/article/pii/S089598112030376X},
author = {M.C. Lamberti and A. Chiodi and M. Agusto and R. Filipovich and A. Massenzio and W. Báez and F. Tassi and O. Vaselli},
keywords = {CO flux, Caldera-hosted geothermal system, Geothermal energy, Renewable resources, Puna plateau},
abstract = {This work presents the first carbon dioxide diffuse degassing survey carried out in the Cerro Blanco Geothermal System (CBGS; Southern Puna plateau, NW Argentina) with the aim to estimate the thermal energy release. The survey was divided into (i) a prospecting stage of the degassing sites within the CBGS and (ii) mapping of the selected diffuse degassing sites. The purpose of the prospecting stage was to elaborate two transects, crosscutting the nested caldera of Cerro Blanco Volcanic Complex (CBVC) and Los Hornitos thermal site, both belonging to CBGS. More than 60 soil diffuse CO2 flux and soil temperature measurements were carried out in the 16-km and 1-km long transects. Significant soil diffuse emissions were only found within the Cerro Blanco caldera, where a detailed mapping of CO2 flux was produced. In this site, named CBa, a single diffuse degassing structure releasing 22.44 kg d−1 of deep-sourced CO2 into the atmosphere was identified. Any other geologic feature of the CBVC and Los Hornitos hydrothermal site presented very low CO2 flux values. According to statistical and geochemical analyses, soil diffuse CO2 degassing is fed by a deep-seated (hydrothermal) and a soil respiration source, which mix each other at different degree. The thermal energy release associated with the diffuse degassing process at CBa is estimated to be ~2.4 kJ/s. This low magnitude thermal energy release is probably a consequence of an efficient cap-rock that likely buffer the surficial expressions of the geothermal resource potentially occurring at depth.}
}
@article{WIJESURIYA2020106750,
title = {Empirical validation and comparison of PCM modeling algorithms commonly used in building energy and hygrothermal software},
journal = {Building and Environment},
volume = {173},
pages = {106750},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.106750},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320301086},
author = {Sajith Wijesuriya and Paulo Cesar Tabares-Velasco and Kaushik Biswas and Dariusz Heim},
keywords = {Building envelope, Thermal storage, PCM, Modeling, Validation},
abstract = {Whole building energy modeling has become extremely important for designers, architects, engineers, and researchers to predict energy performance of buildings. This is particularly important for phase change materials (PCMs) due to their variable properties. For this reason, building energy modeling tools have been developed and validated against different sources of experimental data. However, an IEA Annex 23 surveyed over 250 research publications concluding that the general confidence in currently used numerical models is still too low to use them for designing and code purposes. The objective of this study is to assess the capability of different simulation programs to model the PCMs in building envelope using data from two independent studies using Nano-encapsulated PCMs (Nano-PCM) and shape-stabilized PCMs. The study finds that the investigated PCM models accurately predict the PCM behavior in the building envelope.}
}
@article{BALAKIRUTHIGA2020146,
title = {Segment routing based energy aware routing for software defined data center},
journal = {Cognitive Systems Research},
volume = {64},
pages = {146-163},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300553},
author = {B. Balakiruthiga and P. Deepalakshmi and Sachi Nandan Mohanty and Deepak Gupta and P. {Pavan Kumar} and K. Shankar},
keywords = {Software Defined Data Center (SDDC), Multipath TCP (MPTCP), Segment Routing (SR), Data Center Network (DCN), Software Defined Network (SDN), Segment Label Stack (SLS), Open Daylight (ODL)},
abstract = {Despite the fact that most of the data centers are software-defined, the multifaceted network architecture and increase in network traffic make data centers to suffer from overhead. Multipath TCP supports multiple paths for a single routing session and ensures proper utilization of bandwidth over all available links. As rise in number of nodes in data center is frequent and drastic, scalability issue limits the performance of many existing techniques. Segment Routing is vibrant in reducing scalability disputes and routing overhead. Segment routing approach combined with MPTCP traffic result in efficient routing approach. The downfall of the link capacity due to drastic incoming traffic remains as a major concern in data center network which enforces preventing link energy depletion due to high network traffic. Our proposed work, segment routing based energy aware routing approach for software defined data center aims to achieve throughput maximization through preserving link residual capacity and proper utilization of links. As well, our approach shows a decrease in length of segment label stack with respect to maximum segment label depth. Analysis is done by comparing the executions of other existing approaches in a single-controller environment with our energy-aware routing approach in a distributed environment. Distributed controller setup prevents network from single point of failure. It helps to prevent controller overhead and provides improved network performance through throughput.}
}
@article{SIMONSEN20212809,
title = {Assessment of sarcopenia in patients with upper gastrointestinal tumors: Prevalence and agreement between computed tomography and dual-energy x-ray absorptiometry},
journal = {Clinical Nutrition},
volume = {40},
number = {5},
pages = {2809-2816},
year = {2021},
issn = {0261-5614},
doi = {https://doi.org/10.1016/j.clnu.2021.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0261561421001618},
author = {Casper Simonsen and Thomas S. Kristensen and Anna Sundberg and Sabrina Wielsøe and Jan Christensen and Carsten P. Hansen and Stefan K. Burgdorf and Charlotte Suetta and Pieter {de Heer} and Lars B. Svendsen and Michael P. Achiam and Jesper F. Christensen},
keywords = {Sarcopenia, Muscle mass, Cancer, Computed tomography, Dual-energy x-ray absorptiometry},
abstract = {Summary
Background & aims
Sarcopenia is associated with an increased risk of complications to treatment and lower survival rates in patients with cancer, but there is a lack of agreement on cut-off values and assessment methods. We aimed to investigate the prevalence of sarcopenia assessed by dual-energy x-ray absorptiometry (DXA) and computed tomography (CT) as well as the agreement between the methods for identification of sarcopenia.
Methods
This cross-sectional study pooled data from two studies including patients scheduled for surgery for gastrointestinal tumors. We assessed sarcopenia using two different cut-off values derived from healthy young adults for DXA and two for CT. Additionally, we used one of the most widely applied cut-off values for CT assessed sarcopenia derived from obese cancer patients. The agreement between DXA and CT was evaluated using Cohen's kappa. The mean difference and range of agreement between DXA and CT for estimating total and appendicular lean soft tissue were assessed using Bland–Altman plots.
Results
In total, 131 patients were included. With DXA the prevalence of sarcopenia was 11.5% and 19.1%. Using CT, the prevalence of sarcopenia was 3.8% and 26.7% using cut-off values from healthy young adults and 64.1% using the widely applied cut-off value. The agreement between DXA and CT in identifying sarcopenia was poor, with Cohen's kappa values ranging from 0.05 to 0.39. The mean difference for estimated total lean soft tissue was 1.4 kg, with 95% limits of agreement from −8.6 to 11.5 kg. For appendicular lean soft tissue, the ratio between DXA and CT was 1.15, with 95% limits of agreement from 0.92 to 1.44.
Conclusions
The prevalence of sarcopenia defined using DXA and CT varied substantially, and the agreement between the two modalities is poor.}
}
@article{KAUR2021520,
title = {A survey on energy efficient routing techniques in WSNs focusing IoT applications and enhancing fog computing paradigm},
journal = {Global Transitions Proceedings},
volume = {2},
number = {2},
pages = {520-529},
year = {2021},
note = {International Conference on Computing System and its Applications (ICCSA- 2021)},
issn = {2666-285X},
doi = {https://doi.org/10.1016/j.gltp.2021.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666285X21000273},
author = {Loveleen Kaur and Rajbir Kaur},
keywords = {Energy efficiency, Internet of Things (IoT), Wireless Sensor Networks (WSNs), Fog computing, Sensor networks, Routing},
abstract = {Standardization and technological advancements have contributed in the development of the IoT. The accessibility of ease IoT gadgets has likewise assumed a key job in facilitating IoT research, improvement, and deployment. IoT is worldview network that permits the virtual existence of physical objects throughout our life. The Internet of Things (IoT) is based on the idea of installing embedded devices in everyday objects. In the mean time, because of the low cost and high accessibility of sensor devices, wireless sensor networks (WSNs) have an extraordinary job in overspreading of IoT. To be clear, the function of such systems is completely unpredictable in terms of node heterogeneity and node failure. Continuous advancements in IoT systems have resulted in several of the new protocols designed specifically for sensor networks where energy saving is such a top priority. The routing protocols, on the other hand, have earned the most attention because they might change based on the application and network design. This study examines the most recent routing protocols for sensor networks and developing action plans for the various approaches pursued. One of the potential drawbacks in the IoT is the energy requirement. Furthermore, several directions to extend the network's life expectancy have attracted in an expanding level of attention. Recently, a number of a achievements have emerged. Designing routing protocols is one of the most encouraging of these mechanisms, as demonstrated by the significant amount of energy required for information transmission. This paper begins with a detailed description of the foundation and its associated works. In addition, this study introduces a new routing protocol to increase the energy efficiency of sensor devices in the Internet of Things.}
}
@article{FLORESQUIROZ2021116736,
title = {A distributed computing framework for multi-stage stochastic planning of renewable power systems with energy storage as flexibility option},
journal = {Applied Energy},
volume = {291},
pages = {116736},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116736},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921002506},
author = {Angela Flores-Quiroz and Kai Strunz},
keywords = {Power system planning, Stochastic optimization, Renewable energy, Energy storage, Operational flexibility, Distributed computing},
abstract = {An integrated generation, transmission, and energy storage planning model accounting for short-term constraints and long-term uncertainty is proposed. The model allows to accurately quantify the value of flexibility options in renewable power systems by representing short-term operation through the unit commitment constraints. Long-term uncertainty is represented through a scenario tree. The resulting model is a large-scale multi-stage stochastic mixed-integer programming problem. To overcome the computational burden, a distributed computing framework based on the novel Column Generation and Sharing algorithm is proposed. The performance improvement of the proposed approach is demonstrated through study cases applied to the NREL 118-bus power system. The results confirm the added value of modeling short-term constraints and long-term uncertainty simultaneously. The computational case studies show that the proposed solution approach clearly outperforms the state of the art in terms of computational performance and accuracy. The proposed planning framework is used to assess the value of energy storage systems in the transition to a low-carbon power system.}
}
@article{FENG2020102202,
title = {Energy-efficient user selection and resource allocation in mobile edge computing},
journal = {Ad Hoc Networks},
volume = {107},
pages = {102202},
year = {2020},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2020.102202},
url = {https://www.sciencedirect.com/science/article/pii/S1570870520301098},
author = {Hao Feng and Songtao Guo and Anqi Zhu and Quyuan Wang and Defang Liu},
keywords = {Mobile edge computing, Energy efficiency, User selection, Resource allocation, Convex optimization},
abstract = {Mobile edge computing (MEC) as a new type of computing model can expand the computing power of cloud computing to the edge of radio access network (RAN), which brings a large number of applications close for end user. Compared to traditional cloud computing, computation tasks being offloaded to edge clouds nearby to execute can reduce transmission delay and energy consumption. However, how to select the best edge cloud in a dense cell to execute tasks remains challenging. To address this challenge, in this paper we propose joint user selection and resource allocation algorithm in MEC to maximize the user’s energy efficiency, defined as the ratio of user throughput to its energy consumption. We formulate the energy efficiency maximization problem as a mixed integer fractional nonlinear optimization problem, which involves both users’ offloading selection and uplink transmission power. To solve this non-convex optimization problem, we transform it into an equivalent subtractive convex optimization problem by relaxation transformation method, and furthermore provide the corresponding optimal solution of user selection and power allocation. Numerical results show that compared with other selection schemes, the proposed optimal scheme has a significant improvement in energy efficiency.}
}
@article{MAYER2020164102,
title = {Efficient determination of HPGe γ-ray efficiencies at high energies with ready-to-use simulation software},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {972},
pages = {164102},
year = {2020},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2020.164102},
url = {https://www.sciencedirect.com/science/article/pii/S0168900220305076},
author = {Jan Mayer and Elena Hoemann and Markus Müllenmeister and Philipp Scholz and Andreas Zilges},
keywords = {Monte-Carlo simulations, , HPGe detectors, Full-energy-peak efficiency},
abstract = {The full-energy-peak efficiency of HPGe detectors at γ-ray energies around 10MeV is not easily accessible with experimental methods. Monte-Carlo simulations with Geant4 can provide these efficiencies. G4Horus is a ready-to-use Geant4 application for the HORUS HPGe-detector array. Users can configure the modular parts to match their experiment with minimal knowledge of the simulation software and limited time commitment. In our case, knowing and implementing the geometry with high precision is the biggest challenge. To implement the different target chambers, we transform the existing CAD models to Geant4 geometry with CADMesh. We also found a large discrepancy between experimental and simulated efficiency for some older HPGe detectors, which could be remedied by introducing a large dead region around the inner core. This project is open source and available from https://github.com/janmayer/G4Horus (Mayer 2020). We invite everyone to adapt the project or adopt parts of the code for other projects.}
}
@article{ZAIN20141121,
title = {Optimization of a greener method for removal phenol species by cloud point extraction and spectrophotometry},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {118},
pages = {1121-1128},
year = {2014},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2013.09.129},
url = {https://www.sciencedirect.com/science/article/pii/S1386142513011360},
author = {N.N.M. Zain and N.K. {Abu Bakar} and S. Mohamad and N.Md. Saleh},
keywords = {Cloud point extraction, UV–Vis spectrophotometer, Phenol, Greener method, Water sample},
abstract = {A greener method based on cloud point extraction was developed for removing phenol species including 2,4-dichlorophenol (2,4-DCP), 2,4,6-trichlorophenol (2,4,6-TCP) and 4-nitrophenol (4-NP) in water samples by using the UV–Vis spectrophotometric method. The non-ionic surfactant DC193C was chosen as an extraction solvent due to its low water content in a surfactant rich phase and it is well-known as an environmentally-friendly solvent. The parameters affecting the extraction efficiency such as pH, temperature and incubation time, concentration of surfactant and salt, amount of surfactant and water content were evaluated and optimized. The proposed method was successfully applied for removing phenol species in real water samples.}
}
@article{BRINDHABAN202195,
title = {Effect of X-ray beam energy and image reconstruction technique on computed tomography numbers of various tissue equivalent materials},
journal = {Radiography},
volume = {27},
number = {1},
pages = {95-100},
year = {2021},
issn = {1078-8174},
doi = {https://doi.org/10.1016/j.radi.2020.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S1078817420301218},
author = {A. Brindhaban and O. Jassim},
keywords = {Computed tomography, Iterative image reconstruction, Energy dependence, CT number accuracy, Tissue equivalent materials},
abstract = {Introduction
Computed tomography (CT) numbers are used in radiological diagnosis, attenuation correction and radiotherapy treatment planning. Modern CT scanners use iterative reconstruction methods instead of the traditional filtered back projection (FBP). Hence, the investigation of CT number accuracy with image reconstruction techniques and X-ray tube potential (kVp) used in CT is warranted. The aim of this study is to evaluate the effect of Sinogram Affirmed Iterative Reconstruction (SAFIRE) Technique and image acquisition at different tube potentials on CT numbers of different tissue equivalent materials.
Methods
Images of the Computerised Imaging Reference System Model 062M Electron Density Phantom were acquired at different tube potentials and reconstructed using FBP and different strengths of SAFIRE. Average CT numbers, in circular regions of interest, and their standard deviations were used to investigate any dependence of CT numbers on tube potentials and/or image reconstruction technique using non-parametric statistical tests with p-values set at 0.05.
Results
Statistically significant differences in CT numbers were not observed (p > 0.091) between the different image reconstruction techniques. CT number of bone equivalent materials increased significantly (p < 0.015), by up to 400 Hounsfield Units, when tube potential was decreased. Such extent of CT number change over the tube potentials range used in this study may influence diagnostic outcomes in lung nodule, contrast enhanced and calcium score studies. For all other tissue equivalent materials, the CT number did not change significantly for different tube potentials. Linear relationship was observed between CT numbers and electron densities.
Conclusion
The study concludes that the CT numbers of all tissues did not change significantly with image reconstruction methods. However, the CT numbers of bone equivalent materials increased with decreasing tube potentials, which may result in misrepresentation of clinical information obtained.
Implications for practice
When CT images are used to extract quantitative parameters such as calcium score, to characterise lung nodules and contrast enhanced structures, the kVp used for image acquisition should be carefully selected to avoid any misrepresentation of clinical information.}
}
@article{BAKER2021108,
title = {Enabling Technologies for Energy Cloud},
journal = {Journal of Parallel and Distributed Computing},
volume = {152},
pages = {108-110},
year = {2021},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521000411},
author = {Thar Baker and Zehua Guo and Ali Ismail Awad and Shangguang Wang and Benjamin C.M. Fung},
keywords = {Energy cloud, Machine learning, Artificial intelligence, Cybersecurity, Networks security, Cyber–physical systems},
abstract = {We are thrilled and delighted to present this special issue, which emphasizes on the novel area of Enabling Technologies for Energy Cloud. This guest editorial provides an overview of all articles accepted for publication in this special issue.}
}
@article{ALAVI2021360,
title = {Quantification of adipose tissues by Dual-Energy X-Ray Absorptiometry and Computed Tomography in colorectal cancer patients},
journal = {Clinical Nutrition ESPEN},
volume = {43},
pages = {360-368},
year = {2021},
issn = {2405-4577},
doi = {https://doi.org/10.1016/j.clnesp.2021.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S2405457721001248},
author = {Dena Helene Alavi and Hege Berg Henriksen and Peter Mæhre Lauritzen and Ane Sørlie Kværner and Tomas Sakinis and Torgrim Mikal Langleite and Christine Henriksen and Siv Kjølsrud Bøhn and Ingvild Paur and Gro Wiedswang and Sigbjørn Smeland and Rune Blomhoff},
keywords = {Visceral adipose tissue, Adipose tissue, Inter- and intramuscular adipose tissue, Dual-energy X-ray absorptiometry, Computed tomography, Colorectal cancer},
abstract = {Summary
Background & aims
Excess adipose tissue may affect colorectal cancer (CRC) patients' disease progression and treatment. In contrast to the commonly used anthropometric measurements, Dual-Energy X-Ray Absorptiometry (DXA) and Computed Tomography (CT) can differentiate adipose tissues. However, these modalities are rarely used in the clinic despite providing high-quality estimates. This study aimed to compare DXA's measurement of abdominal visceral adipose tissue (VAT) and fat mass (FM) against a corresponding volume by CT in a CRC population. Secondly, we aimed to identify the best single lumbar CT slice for abdominal VAT. Lastly, we investigated the associations between anthropometric measurements and VAT estimated by DXA and CT.
Methods
Non-metastatic CRC patients between 50-80 years from the ongoing randomized controlled trial CRC-NORDIET were included in this cross-sectional study. Corresponding abdominal volumes were acquired by Lunar iDXA and from clinically acquired CT examinations. Also, single CT slices at L2-, L3-and L4-level were obtained. Agreement between the methods was investigated using univariate linear regression and Bland–Altman plots.
Results
Sixty-six CRC patients were included. Abdominal volumetric VAT and FM measured by DXA explained up to 91% and 96% of the variance in VAT and FM by CT, respectively. Bland–Altman plots demonstrated an overestimation of VAT by DXA compared to CT (mean difference of 76 cm3) concurrent with an underestimation of FM (mean difference of −319 cm3). A higher overestimation of VAT (p = 0.015) and underestimation of FM (p = 0.036) were observed in obese relative to normal weight subjects. VAT in a single slice at L3-level showed the highest explained variance against CT volume (R2 = 0.97), but a combination of three slices (L2, L3, L4) explained a significantly higher variance than L3 alone (R2 = 0.98, p < 0.006). The anthropometric measurements explained between 31-65% of the variance of volumetric VAT measured by DXA and CT.
Conclusions
DXA and the combined use of three CT slices (L2-L4) are valid to predict abdominal volumetric VAT and FM in CRC patients when using volumetric CT as a reference method. Due to the poor performance of anthropometric measurements we recommend exploring the added value of advanced body composition by DXA and CT integrated into CRC care.}
}
@article{PEREZ2021891,
title = {Energy-conscious optimization of Edge Computing through Deep Reinforcement Learning and two-phase immersion cooling},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {891-907},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002934},
author = {Sergio Pérez and Patricia Arroba and José M. Moya},
keywords = {Energy-aware optimization, Deep Reinforcement Learning, Edge Computing, Two-phase immersion cooling, Advanced driver assistance systems},
abstract = {Until now, the reigning computing paradigm has been Cloud Computing, whose facilities concentrate in large and remote areas. Novel data-intensive services with critical latency and bandwidth constraints, such as autonomous driving and remote health, will suffer under an increasingly saturated network. On the contrary, Edge Computing brings computing facilities closer to end-users to offload workloads in Edge Data Centers (EDCs). Nevertheless, Edge Computing raises other concerns like EDC size, energy consumption, price, and user-centered design. This research addresses these challenges by optimizing Edge Computing scenarios in two ways, two-phase immersion cooling systems and smart resource allocation via Deep Reinforcement Learning. To this end, several Edge Computing scenarios have been modeled, simulated, and optimized with energy-aware strategies using real traces of user demand and hardware behavior. These scenarios include air-cooled and two-phase immersion-cooled EDCs devised using hardware prototypes and a resource allocation manager based on an Advantage Actor–Critic (A2C) agent. Our immersion-cooled EDC’s IT energy model achieved an NRMSD of 3.15% and an R2 of 97.97%. These EDCs yielded an average energy saving of 22.8% compared to air-cooled. Our DRL-based allocation manager further reduced energy consumption by up to 23.8% in comparison to the baseline.}
}
@article{MALLYA2022e77,
title = {A novel dual-energy cone beam computed tomography device for assessment of jaw bone density},
journal = {Oral Surgery, Oral Medicine, Oral Pathology and Oral Radiology},
volume = {134},
number = {3},
pages = {e77-e78},
year = {2022},
issn = {2212-4403},
doi = {https://doi.org/10.1016/j.oooo.2022.04.027},
url = {https://www.sciencedirect.com/science/article/pii/S2212440322009257},
author = {Dr. Sanjay Mallya and Dr. Tara Aghaloo and Dr. Sotirios Tetradis and Dr. Vidhya Venkateswaran and Dr. Reuben Kim},
abstract = {Objective
To evaluate a novel dual-energy cone beam computed tomography (DE-CBCT) device for jaw bone density assessment.
Study Design
An ongoing clinical trial has enrolled 19 subjects with a target enrollment of 24 subjects. All subjects were 21 years and older and required a maxillofacial CT scan for standard-of-care diagnosis. Each subject received 2 examinations: one with a 64-slice multi-detector CT (MDCT) scanner and the second with the experimental DE-CBCT device (RCT720, Ray Co. Seoul, Korea). Asynchronous scanning of a custom hydroxyapatite phantom (concentrations of 0.8, 1.0 and 1.2 g/cc) provided calibration for bone density measurement. DICOM data from both CT examinations were imported into a custom version of OnDemand software. Calibration curves were generated from the MDCT and DE-CBCT scans of the hydroxyapatite phantoms. The MDCT and DE-CBCT scans of study subjects were superimposed and custom tools allowed calculation of bone density within a specified region of interest (ROI). Concordance between the MDCT- and DE-CBCT-derived bone densities was examined.
Results
In our preliminary assessments of data from current subjects, the mean calculated bone density in all the ROIs measured was 0.51 ± 0.27 g/cc for MDCT imaging and 0.50 ± 0.32 for DE-CBCT imaging. The range of the calculated bone densities was 0.06 to 1.1 g/cc for MDCT and 0.04 to 1.3 g/cc for DE-CBCT. Overall, there was strong correlation between bone densities calculated from MDCT scans with those calculated from DE-CBCT scans (r = 0.96, p <0.0001). Bland-Altman analysis identified a bias of 0.01 ± 0.1, and 95% limits of agreement range was -0.19 to 0.21.
Conclusion
The RCT720, a newly-developed DE-CBCT device, produced measurement of bone density equivalent to MDCT and overcomes the inaccuracies of bone density assessment using conventional CBCT scans. Statement of Ethical Review Human/Animal subjects were used and this study was approved by an institutional ethics panel}
}
@article{DICKMANN2021237,
title = {Proof of concept image artifact reduction by energy-modulated proton computed tomography (EMpCT)},
journal = {Physica Medica},
volume = {81},
pages = {237-244},
year = {2021},
issn = {1120-1797},
doi = {https://doi.org/10.1016/j.ejmp.2020.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1120179720303276},
author = {Jannis Dickmann and Christina Sarosiek and Victor Rykalin and Mark Pankuch and George Coutrakon and Robert P. Johnson and Vladimir Bashkirov and Reinhard W. Schulte and Katia Parodi and Guillaume Landry and George Dedes},
keywords = {Proton computed tomography, Particle therapy, Artifact reduction, Image quality},
abstract = {Purpose
To reduce imaging artifacts and improve image quality of a specific proton computed tomography (pCT) prototype scanner by combining pCT data acquired at two different incident proton energies to avoid protons stopping in sub-optimal detector sections.
Methods
Image artifacts of a prototype pCT scanner are linked to protons stopping close to internal structures of the scanner’s multi-stage energy detector. We aimed at avoiding such protons by acquiring pCT data at two different incident energies and combining the data in post-processing from which artifact-reduced images of the relative stopping power (RSP) were calculated. Energy-modulated pCT (EMpCT) images were assessed visually and quantitatively and compared to the original mono-energetic images in terms of RSP accuracy and noise. Data were acquired for a homogeneous water phantom.
Results
RSP images reconstructed from the mono-energetic datasets displayed local image artifacts which were ring-shaped due to the homogeneity of the phantom. The merged EMpCT dataset achieved a superior visual image quality with reduced artifacts and only minor remaining rings. The inter-quartile range (25/75) of RSP values was reduced from 0.7% with the current standard acquisition to 0.2% with EMpCT due to the reduction of ring artifacts. In this study, dose was doubled compared to a standard scan, but we discuss strategies to reduce excess dose.
Conclusions
EMpCT allows to effectively avoid regions of the energy detector that cause image artifacts. Thereby, image quality is improved.}
}
@article{FAROOQ2021107682,
title = {Intelligent energy prediction techniques for fog computing networks},
journal = {Applied Soft Computing},
volume = {111},
pages = {107682},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107682},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621006037},
author = {Umar Farooq and Muhammad Wasif Shabir and Muhammad Awais Javed and Muhammad Imran},
keywords = {Fog computing, Artificial neural network, Energy prediction},
abstract = {Energy Efficiency is a key concern for future fog-enabled Internet of Things (IoT). Since Fog Nodes (FNs) are energy-constrained devices, task offloading techniques must consider the energy consumption of the FNs to maximize the performance of IoT applications. In this context, accurate energy prediction can enable the development of intelligent energy-aware task offloading techniques. In this paper, we present two energy prediction techniques, the first one is based on the Recursive Least Square (RLS) filter and the second one uses the Artificial Neural Network (ANN). Both techniques use inputs such as the number of tasks and size of the tasks to predict the energy consumption at different fog nodes. Simulation results show that both techniques have a root mean square error of less than 3%. However, the ANN-based technique shows up to 20% less root mean square error as compared to the RLS-based technique.}
}
@article{FANKHAUSER2021e76,
title = {Re: Paolo Dell’Oglio, Hielke M. de Vries, Elio Mazzone, et al. Hybrid Indocyanine Green–99mTc-nanocolloid for Single-photon Emission Computed Tomography and Combined Radio- and Fluorescence-guided Sentinel Node Biopsy in Penile Cancer: Results of 740 Inguinal Basins Assessed at a Single Institution. Eur Urol 2020;78:865–72},
journal = {European Urology},
volume = {79},
number = {3},
pages = {e76-e77},
year = {2021},
issn = {0302-2838},
doi = {https://doi.org/10.1016/j.eururo.2020.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S0302283820310204},
author = {Christian D. Fankhauser and Arie Parnham and Vijay Sangar}
}
@article{SIMONETTI2021109456,
title = {Dual energy computed tomography evaluation of skeletal traumas},
journal = {European Journal of Radiology},
volume = {134},
pages = {109456},
year = {2021},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2020.109456},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X2030646X},
author = {Igino Simonetti and Francesco Verde and Luigi Palumbo and Francesco {Di Pietto} and Marta Puglia and Mariano Scaglione and Alfonso Ragozzino and Stefania Romano},
keywords = {Dual-energy computed tomography (DECT), Computed tomography (CT), Trauma, Emergency radiology (ER), Musculoskeletal (MSK), Bone marrow edema (BME)},
abstract = {Skeletal traumas are among the most common routine challenges faced by Emergency Radiologists, in particular in case of radiographically occult nondisplaced fractures or in case of soft tissue injuries. With the development of Dual Energy Computed Tomography (DECT) technology, new post-processing applications have gained a useful diagnostic role in many fields of musculoskeletal imaging including acute skeletal trauma imaging. In addition to conventional CT images, DECT allows for the generation of virtual calcium-suppressed images subtracting calcium from unenhanced CT images based on the fact that material attenuation varies at different energy levels. In this way, virtual-non-calcium (VNC) images can precisely characterize traumatic bone marrow edema in both axial and appendicular skeleton, facilitating prompt clinical decision, especially when magnetic resonance method is contraindicated or unavailable. Other DECT emerging applications in the trauma setting include metal artifact reduction and collagen mapping for the evaluation of injuries affecting ligament, tendon, and intervertebral disk. This review focuses on the basic principles of DECT and related post-processing algorithms, highlighting the current advantages and limitations of these new imaging advances in the Emergency Department related to skeletal traumas.}
}
@article{OJO2019246,
title = {Toward green computing practices: A Malaysian study of green belief and attitude among Information Technology professionals},
journal = {Journal of Cleaner Production},
volume = {224},
pages = {246-255},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.03.237},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619309461},
author = {Adedapo Oluwaseyi Ojo and Murali Raman and Alan G. Downe},
keywords = {Green information technology, Green belief, Green attitude, Green computing practices, Pro-environmental behavior, Information technology professional},
abstract = {The adoption of Green Information Technology (GIT) is important to ensure organizations’ environmental performance through sustainable production, consumption, utilization and disposal of Information Technology (IT) devices. However, research on the adoption of GIT practices has mostly addressed organizational factors and outcomes, with limited emphasis on the cognitive and attitudinal factors associated with behavioral change. Based on the Belief-Action-Outcome (BAO) framework, this research examined the effects of individual, social and organizational factors on GIT attitude among a sample of IT professionals in ISO 14001 certified IT companies in Malaysia and investigated the mediating effects of their beliefs about GIT. Further, this research investigated the relationship between GIT attitudes and behavioral change, as indicated through self-reported engagement in green computing practices. Survey methods were used to collect data from 333 respondents. The results support the direct effects of GIT knowledge, social influence and green management culture on GIT attitude. However, hypothesized indirect effects through the mediation of GIT beliefs were supported for GIT knowledge and social influence, but not for green management culture. The relationship between GIT attitude and engagement in green computing practices was supported. The implications of these results are discussed, and future research directions are suggested.}
}
@article{GU2021244,
title = {Energy-efficient computation offloading for vehicular edge computing networks},
journal = {Computer Communications},
volume = {166},
pages = {244-253},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420320168},
author = {Xiaohui Gu and Guoan Zhang},
keywords = {Vehicular networks, Multi-access edge computing, Computation offloading, Resource allocation, Mobility},
abstract = {The demanding computing capacity of emerging vehicular applications has emerged as a challenge in Internet of vehicles (IoVs). Multi-access edge computing (MEC) can significantly enhance computing capability and prolong battery life of vehicles through offloading computation-intensive tasks for edge computing. Considering the impact of vehicles’ mobility on communication quality, this paper provides an energy-efficient computation offloading scheme for vehicular edge computing networks (VECN). An energy-efficiency cost (EEC) minimization problem is formulated to make a tradeoff between latency and energy consumption, for completing computational tasks in an effective manner. Since that multiple variables and time-varying channel conditions make the formulated problem difficult to solve, we transform the original non-convex problem into a two-level optimization problem and develop an iterative distributed algorithm to obtain an optimal solution. Numerical results verify the convergence and superiority of the proposed algorithm.}
}
@article{ODONNELL2019102905,
title = {LiDAR point-cloud mapping of building façades for building energy performance simulation},
journal = {Automation in Construction},
volume = {107},
pages = {102905},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102905},
url = {https://www.sciencedirect.com/science/article/pii/S092658051830565X},
author = {James O’Donnell and Linh Truong-Hong and Niamh Boyle and Edward Corry and Jun Cao and Debra F. Laefer},
keywords = {Light Detection And Ranging (LiDAR), Laser scanning, City-scale modelling, Building Energy Performance Simulation (BEPS), Retrofit, Semi-automated façades generation},
abstract = {Current processes that create Building Energy Performance Simulation (BEPS) models are time consuming and costly, primarily due to the extensive manual inputs required for model population. In particular, generation of geometric inputs for existing building models requires significant manual intervention due to the absence, or outdated nature of available data or digital measurements. Additionally, solutions based on Building Information Modelling (BIM) also require high quality and precise geometrically-based models, which are not typically available for existing buildings. As such, this work introduces a semi-automated BEPS input solution for existing building exteriors that can be integrated with other related technologies (such as BIM or CityGML) and deployed across an entire building stock. Within the overarching approach, a novel sub-process automatically transforms a point cloud obtained from a terrestrial laser scanner into a representation of a building's exterior façade geometry as input data for a BEPS engine. Semantic enrichment is performed manually. This novel solution extends two existing approaches: (1) an angle criterion in boundary detection and (2) a voxelisation representation to improve performance. The use of laser scanning data reduces temporal costs and improves input accuracy for BEPS model generation of existing buildings. The approach is tested herein on two example cases. Vertical and horizontal accuracies of 1% and 7% were generated, respectively, when compared against independently produced, measured drawings. The approach showed variation in accuracy of model generation, particularly for upper floors of the test case buildings. However, the energy impacts resulting from these variations represented less than 1% of the energy consumption for both cases.}
}
@article{RHEINLANDER2020122,
title = {Harvester-aware transient computing: Utilizing the mechanical inertia of kinetic energy harvesters for a proactive frequency-based power loss detection},
journal = {Integration},
volume = {75},
pages = {122-130},
year = {2020},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2020.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167926020302601},
author = {Carl C. Rheinländer and Norbert Wehn},
keywords = {Transient computing, Power-Neutral systems, Energy Harvesting, Transiently-powered embedded devices},
abstract = {Power-neutral system design avoids energy buffers by directly powering the load by the energy harvester. In case of a power loss, checkpointing methods ensure forward progress by preserving the volatile system state using non-volatile memories. The timely detection of upcoming power losses is essential for a reliable checkpointing process. Moreover, various applications require early detections to, e.g., ensure the finalization of atomic operations. However, common voltage threshold-based methods only allow short-term detections. In this paper we propose a new methodology that allows early detections by exploiting physical characteristics of the harvester. To this end, small-scale kinetic energy harvesters are considered that employ rotatably mounted mechanical masses to drive electromagnetic generators. Due to the inertia of these masses, the power output does not stop abruptly, but gradually decays after the excitation of the harvester is over. We investigate the relationship between the initial excitation intensity as it is reflected in the output frequency, the load current and the remaining period of power availability. Our results indicate that this relationship allows to predict the power duration based on the output frequency of the harvester. We show that power losses can be detected up to one order of magnitude earlier with our frequency-based method than with state-of-the-art voltage-based methods.}
}
@article{YE2020107577,
title = {Offspeeding: Optimal energy-efficient flight speed scheduling for UAV-assisted edge computing},
journal = {Computer Networks},
volume = {183},
pages = {107577},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107577},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620312196},
author = {Weidu Ye and Junzhou Luo and Feng Shan and Wenjia Wu and Ming Yang},
keywords = {UAV, Edge computing, Data collection, Energy consumption, Flight speed scheduling, Optimal algorithm},
abstract = {Millions of Internet of Thing (IoT) devices have been widely deployed to support applications such as smart city, industrial Internet, and smart transportation. These IoT devices periodically upload their collected data and reconfigure themselves to adapt to the dynamic environment. Both operations are resource consuming for low-end IoT devices. An edge computing enabled unmanned aerial vehicle (UAV) is proposed to fly over to collect data and complete reconfiguration computing tasks from IoT devices. Distinct from most existing work, this paper focuses on flight speed scheduling that allocates proper flight speed to minimize the energy consumption of the UAV with a practical energy model, under the constraints of individual task execution deadlines and communication ranges. We formulate the Energy-Efficient flight Speed Scheduling (EESS) problem, and devise a novel diagram to visualize and analyze this problem. An optimal energy-efficient flight speed scheduling (Offspeeding) algorithm is then proposed to solve the offline version of the EESS problem. Utilizing Offspeeding and the optimal properties obtained from the theoretical analysis, an online heuristic speed scheduling algorithm is developed for more realistic scenarios, where information from IoT devices keeps unknown until the UAV flies close. Finally, simulation results demonstrate our online heuristic is near optimal. This research sheds light on a new research direction, e.g., deadline driven UAV speed scheduling for edge computing with a practical propulsion energy model.}
}
@article{GADDAM2021418,
title = {Principles and Applications of Dual Energy Computed Tomography in Neuroradiology},
journal = {Seminars in Ultrasound, CT and MRI},
volume = {42},
number = {5},
pages = {418-433},
year = {2021},
note = {Advances in Neuroradiology I},
issn = {0887-2171},
doi = {https://doi.org/10.1053/j.sult.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0887217121000792},
author = {Durga Sivacharan Gaddam and Matthew Dattwyler and Thorsten R Fleiter and Uttam K Bodanapally},
abstract = {Dual-energy computed tomography (DE CT) is a promising tool with many current and evolving applications. Available DE CT scanners usually consist of one or two tubes, or use layered detectors for spectral separation. Most DE CT scanners can be used in single energy or dual-energy mode, except for the layered detector scanners that always acquire data in dual-energy mode. However, the layered detector scanners can retrospectively integrate the data from two layers to obtain conventional single energy images. DE CT mode enables generation of virtual monochromatic images, blended images, iodine quantification, improving conspicuity of iodinated contrast enhancement, and material decomposition maps or more sophisticated quantitative analysis not possible with conventional SE CT acquisition with an acceptable or even lower dose than the SE CT. This article reviews the basic principles of dual-energy CT and highlights many of its clinical applications in the evaluation of neurological conditions.}
}
@article{TIWARI2022e82,
title = {In vitro assessment of a novel dual-energy cone beam computed tomography device},
journal = {Oral Surgery, Oral Medicine, Oral Pathology and Oral Radiology},
volume = {134},
number = {3},
pages = {e82},
year = {2022},
issn = {2212-4403},
doi = {https://doi.org/10.1016/j.oooo.2022.04.040},
url = {https://www.sciencedirect.com/science/article/pii/S2212440322009385},
author = {Dr. Ritu Tiwari and Dr. Sanjay Mallya},
abstract = {Objective
CT attenuation depends on atomic number and physical density of the object. Dual energy systems use two energy spectra and attenuation data from two different exposure conditions to estimate the effect of photoelectric absorption and Compton scatter in generating CT numbers. Based on this principle, a dual energy cone beam CT device (DE-CBCT, RCT-270, Ray Co. Ltd., Korea) was developed to assess jaw bone density and overcome limitations of gray level inhomogeneity in CBCT.
Study Design
We used a human skull phantom and varying concentrations of dipotassium phosphate (K2HPO4) to simulate trabecular and cortical bone. 0.2 ml polypropylene tubes with K2HPO4 (600mg/ml and 800mg/ml) were placed in empty sockets of the phantom at 3 discrete mandibular sites and scans were acquired. Next, to simulate temporal changes, we used varying concentrations of K2HPO4 (400 mg/ml-1200 mg/ml) in the extraction socket of a mandibular second premolar. All scans were made using the Rayscan DE-CBCT unit and repeated thrice. After each scan, the QRM-DE-CBCT phantom was also scanned with the same exposure factors for calibration. Scan calibration and bone density assessments were performed using the vendor's software.
Results
Calculated density of the same K2HPO4 placed at different locations (891± 144, 768 ± 29, 541 ± 126) was not statistically different. A similar result was obtained with K2HPO4 of higher density (1299 ± 58, 1203 ± 74, 998 ± 125). While simulating changing bone density, increasing concentrations of K2HPO4 yielded higher calculated densities, with a linear relationship between K2HPO4 concentrations and DE-CBCT calculated density (r2 = 0.88).
Conclusion
This study provides an initial assessment of DE-CBCT as a method to better estimate object density which can impact monitoring osseous healing, evaluation of potential implant sites, and augmentation threshold for image segmentation. In ongoing studies, we are characterizing image quality parameters and further assessing the impact of anatomic location. Statement of Ethical Review Ethical Review or exemption was not warranted for this study}
}
@article{WANG2021106551,
title = {Early diagnosis and prediction of intracranial hemorrhage using dual-energy computed tomography after mechanical thrombectomy in patients with acute ischemic stroke},
journal = {Clinical Neurology and Neurosurgery},
volume = {203},
pages = {106551},
year = {2021},
issn = {0303-8467},
doi = {https://doi.org/10.1016/j.clineuro.2021.106551},
url = {https://www.sciencedirect.com/science/article/pii/S0303846721000780},
author = {Zhenshan Wang and Wanqi Chen and Haitao Lin and Shiwei Luo and Yuan Liu and Yang Lin and Ying Tao and Weipeng Huang},
keywords = {Acute ischemic stroke, Mechanical thrombectomy, Contrast extravasation, Intracranial hemorrhage, Dual-energy CT (DECT)},
abstract = {Purpose
This study assesses the clinical value of dual-energy computed tomography (DECT) in the early diagnosis of intracranial hemorrhage and evaluates the risk of hemorrhagic transformation in patients with acute ischemic stroke (AIS) after mechanical thrombectomy.
Methods
Patients with AIS who have undergone thrombectomy with Solitaire stent and DECT within one hour after surgery were prospectively enrolled. Linear mixed energy images, virtual non-contrast (VNC) image, and iodine overlay map (IOM) were obtained. Routine CT scan was performed 24 h postoperatively. The sensitivity, specificity, positive and negative predictive values, and accuracy of DECT in the early diagnosis of intracranial hemorrhage was evaluated. The iodine concentration of intracranial lesions was measured by IOM with the follow-up results taken as reference. Receiver operating characteristic (ROC) analysis was performed to obtain the threshold of hemorrhagic transformation and increased bleeding.
Results
Among the 44 patients enrolled in this study, 25 (56.8 %) were diagnosed with simple extravasation of iodinated contrast agent, and 19 (43.2 %) showed intracranial hemorrhage in DECT. Compared with the follow-up CT 24 h after surgery, early diagnosis of postoperative intracranial hemorrhage using DECT demonstrated a sensitivity of 90.5 %, specificity of 100 %, positive predictive rate of 100 %, negative predictive rate of 92.0 %, and accuracy of 95.5 %. Among the 86 intracranial lesions that underwent iodine concentration measurement, 19 were diagnosed with hemorrhagic transformation or increased bleeding, and 67 were diagnosed without the aforementioned conditions. The sensitivity and specificity for differentiating the two groups were 73.7 % and 92.5 %, respectively, with a cut-off value of 2.7 mg/mL.
Conclusion
DECT is clinically valuable in early diagnosis and prediction of intracranial hemorrhage after mechanical thrombectomy in AIS patients.}
}
@article{AHMAD2020679,
title = {Daylight availability assessment and the application of energy simulation software – A literature review},
journal = {Materials Science for Energy Technologies},
volume = {3},
pages = {679-689},
year = {2020},
issn = {2589-2991},
doi = {https://doi.org/10.1016/j.mset.2020.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589299120300410},
author = {Asim Ahmad and Anil Kumar and Om Prakash and Ankish Aman},
keywords = {Day-lighting, Simulation, Modeling, Radiance, Energy-efficient building},
abstract = {The present study shows the application of various software performance analysis of daylighting and its energy savings potential in Energy-efficient building topologies. It can easily predict the daylight illuminance and its consequences in any energy-efficient building irrespective of any construction expense with the application of simulation software such as MatLab, Energy Plus, Velux, Relux, DIALux, in the energy analysis process. There are various computer-based tools accessible to the precise appraisal of the building's daylighting and thermal execution. This simulation software performs energy calculation for choosing effective retrofits of energy-efficient building. The study critically reviews and compares software used for energy simulation and daylighting in the building. The survey from this review found that dimming controlled daylighting committed 13% more saving of energy potential in factory building than on/off control system. The importance of daylight software in building energy saving is represented through the case study. The simulated and measured indoor illuminances with and without blinds are represented. The calculated MBE and RMSE (error) by Radiance and Radlink were satisfying each other. The modern state art of review would be helpful for the academician, scientist, and researcher working in the field daylighting.}
}
@article{LIN2021101601,
title = {Trainingless multi-objective evolutionary computing-based nonintrusive load monitoring: Part of smart-home energy management for demand-side management},
journal = {Journal of Building Engineering},
volume = {33},
pages = {101601},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2020.101601},
url = {https://www.sciencedirect.com/science/article/pii/S2352710219329079},
author = {Yu-Hsiu Lin},
keywords = {Artificial intelligence, Home energy management system, Internet of things, Multi-objective combinatorial optimization, Nonintrusive load monitoring},
abstract = {Electricity is the most widely used form of energy in modern society. One method of satisfying the continuously increasing industrial, commercial, and residential electrical-energy demands of consumers in smart grids is to use an Internet-of-things (IoT) service-oriented electrical-energy management system (EMS) to intrusively monitor and manage electrical loads, which can effectively react to demand-response schemes for demand-side management (DSM). Nonintrusive load monitoring (NILM), a viable cost-effective load disaggregation technique, has recently gained considerable attention as a nonintrusive alternative to EMS in the research field of smart grids. This paper presents a smart IoT-oriented home EMS founded on trainingless multi-objective evolutionary computing-based NILM for DSM in a smart grid. Evolutionary computing-based NILM is considered and addressed as a multi-objective combinatorial optimization problem. The proposed NILM technique can determine the electrical appliances based on their individual electrical characteristics extracted from composite electrical-load consumption with no intrusive deployment of smart plugs or power meters. A fully nonintrusive NILM alternative is considered and proposed. In addition, this alternative is different from conventional NILM because conventional NILM considers artificial intelligence including artificial neural networks (NNs) and deep NN as load classifiers of NILM where training and retraining stages and a hyperparameter tuning procedure are required. The proposed smart IoT-oriented home EMS was experimentally investigated with the trainingless multi-objective evolutionary computing-based NILM in a real house environment. The experimental results confirm that the proposed methodology is feasible.}
}
@article{PANDA2020102128,
title = {Energy efficient routing and lightpath management in software defined networking based inter-DC elastic optical networks},
journal = {Optical Fiber Technology},
volume = {55},
pages = {102128},
year = {2020},
issn = {1068-5200},
doi = {https://doi.org/10.1016/j.yofte.2019.102128},
url = {https://www.sciencedirect.com/science/article/pii/S1068520019306467},
author = {Satyasen Panda},
keywords = {Elastic optical networks, Software defined networking, Ant colony optimization, Deep echo state network, Traffic prediction, Energy efficiency, Lightpath management},
abstract = {To improve flexibility and robustness of high speed optical communication in inter-data center (DC) networks, Elastic optical networks (EONs) are progressively used. With continuous increase in the inter-DC traffic, it has become necessary to address the issue of rising power consumption for EONs. In this paper, we elaborately discussed the power consumption model of inter-DC EONs and the traffic prediction methods with central software defined networking (SDN) controller. The whole power consumption of the EON is controlled by avoiding the unnecessary tearing down and reactivating the lightpaths through the proposed Adaptive ant colony optimization based Deep echo state network (AACO-DESN) algorithm. The simulation results confirm the superiority of the proposed AACO-DESN algorithm in comparison to other existing algorithms in terms of energy efficiency and spectral efficiency for large EONs.}
}
@article{BUSACCA2021108330,
title = {Designing a multi-layer edge-computing platform for energy-efficient and delay-aware offloading in vehicular networks},
journal = {Computer Networks},
volume = {198},
pages = {108330},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108330},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621003315},
author = {Fabio Busacca and Giuseppe Faraci and Christian Grasso and Sergio Palazzo and Giovanni Schembra},
keywords = {5G, Edge Computing, Vehicular Networks, Reinforcement Learning, Markov Models},
abstract = {Vehicular networks are expected to support many time-critical services requiring huge amounts of computation resources with very low delay. However, such requirements may not be fully met by vehicle on-board devices due to their limited processing and storage capabilities. The solution provided by 5G is the application of the Multi-Access Edge Computing (MEC) paradigm, which represents a low-latency alternative to remote clouds. Accordingly, we envision a multi-layer job-offloading scheme based on three levels, i.e., the Vehicular Domain, the MEC Domain and Backhaul Network Domain. In such a view, jobs can be offloaded from the Vehicular Domain to the MEC Domain, and even further offloaded between MEC Servers for load balancing purposes. We also propose a framework based on a Markov Decision Process (MDP) to model the interactions among stakeholders working at the three different layers. Such a MDP model allows a Reinforcement Learning (RL) algorithm to take optimal decisions on both the number of jobs to offload between MEC Servers, and on the amount of computing power to allocate to each job. An extensive numerical analysis is presented to demonstrate the effectiveness of our algorithm in comparison with static policies not applying RL.}
}
@article{SHARMA2020176,
title = {Integration of life cycle assessment with energy simulation software for polymer exchange membrane (PEM) electrolysis},
journal = {Procedia CIRP},
volume = {90},
pages = {176-181},
year = {2020},
note = {27th CIRP Life Cycle Engineering Conference (LCE2020) Advancing Life Cycle Engineering : from technological eco-efficiency to technology that supports a world that meets the development goals and the absolute sustainability},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.139},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120304340},
author = {Hemant Sharma and Guillaume Mandil and Peggy Zwolinski and Emmanuelle Cor and Hugo Mugnier and Elise Monnier},
keywords = {Life cycle assessment, LCA, Electrolysis, PEM, Polymer exchange membrane},
abstract = {In the assessment and planning of energy systems, use of simulations is a crucial step. They allow the quantification of techno-economic potential to subsequently aid decision-making amongst various technological or design choices. In these software, environmental analysis is either too simplified or neglected completely since a conventional life cycle assessment (LCA) study might not be feasible to account for different variabilities related to scale, scope, energy carriers, etc. In this paper, we integrate techno-economic analysis of hydrogen production from polymer exchange membrane (PEM) electrolysis with life cycle assessment. This step-by-step guideline will be useful especially for professionals working in energy system design to develop a LCA model for PEM in their respective software. Consequently, this will enable them to take environmental indicators into account while planning facilities.}
}
@article{GARNETT2020160,
title = {A comprehensive review of dual-energy and multi-spectral computed tomography},
journal = {Clinical Imaging},
volume = {67},
pages = {160-169},
year = {2020},
issn = {0899-7071},
doi = {https://doi.org/10.1016/j.clinimag.2020.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0899707120302928},
author = {Richard Garnett},
keywords = {Dual-energy, Multi-spectral, Colour CT, Photon counting detector, Spectral CT},
abstract = {This review will provide a brief introduction to the development of the first Computed Tomography (CT) scan, from the beginnings of x-ray imaging to the first functional CT system introduced by Godfrey Houndsfield. The principles behind photon interactions and the methods by which they can be leveraged to generate dual-energy or multi-spectral CT images are discussed. The clinical applications of these methodologies are investigated, showing the immense potential for dual-energy or multi-spectral CT to change the fields of in-vivo and non-destructive imaging for quantitative analysis of tissues and materials. Lastly the current trends of research for dual-energy and multi-spectral CT are covered, showing that the majority of instrument development is focused on photon counting detectors for mutli-spectral CT and that clinical research is dominated by validation studies for the implementation of dual-energy and multi-spectral CT.}
}
@article{BAHRAM2011891,
title = {Development of cloud point extraction using pH-sensitive hydrogel for preconcentration and determination of malachite green},
journal = {Talanta},
volume = {85},
number = {2},
pages = {891-896},
year = {2011},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2011.04.074},
url = {https://www.sciencedirect.com/science/article/pii/S0039914011004036},
author = {Morteza Bahram and Foroogh Keshvari and Peyman Najafi-Moghaddam},
keywords = {Hydrogel, Preconcentration, Central composite design, Malachite green},
abstract = {A novel and sensitive cloud point extraction procedure using pH-sensitive hydrogel was developed for preconcentration and spectrophotometric determination of trace amounts of malachite green (MG). In this extraction method, appropriate amounts of poly(styrene-alt-maleic acid), as a pH-sensitive hydrogel, and HCl were added respectively into the aqueous sample so a cloudy solution was formed. The cloudy phase consists of hydrogel particles distributed entirely into the aqueous phase. Organic or inorganic compounds having the potential to interact with polymer particles (chemical interaction or physical adsorption) could be extracted to cloudy phase. After centrifuging, these particles of hydrogel were sedimented in the bottom of sample tube. The sedimented hydrogel-rich phase was diluted with acetonitrile and its absorbance was measured at 617nm (λmax of malachite green in hydrogel). Central composite design and response surface method were applied to design the experiments and optimize the experimental parameters such as, concentration of hydrogel and HCl, extraction time and salting out effect. Under the optimum conditions, the linear range was 1×10−8–5×10−7molL−1 malachite green with a correlation coefficient of 0.992. The limit of detection (S/N=3) was 4.1×10−9molL−1. Relative standard deviation (RSD) for 7 replicate determinations of 10−7molL−1 malachite green was 3.03%. In this work, the concentration factor of 20 was reached. Also the improvement factor of the proposed method was 23. The advantages of this method are simplicity of operation, rapidity and low cost.}
}
@article{FENG2021101086,
title = {Protocol for reliable energy data collection based on mobile fog computing},
journal = {Sustainable Energy Technologies and Assessments},
volume = {44},
pages = {101086},
year = {2021},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2021.101086},
url = {https://www.sciencedirect.com/science/article/pii/S2213138821000965},
author = {Zhenqiang Feng},
keywords = {Energy Reliable Data Collection, Fog computing, Wireless sensor network, Lo Ra WAN, Data acquisition protocol},
abstract = {Fog computing can quickly calculate and store user needs, and cooperate with remote cloud computing to provide more powerful computing capabilities. In this paper, the passive cache is optimized, and the Dynamic Cooperate Proactive Cache (DCPC) algorithm based on user mobility is proposed, which uses user movement information and request records collected by the fog server node to cache the content on the fog server node in advance. The algorithm combines the autoregressive differential moving average model to actively predict the requested content, which greatly improves the utilization of cache space. This paper proposes a wireless sensor network based on Lo Ra WAN to collect energy impact factors. In order to greatly extend the survival time of sensor networks, this paper proposes a data collection method based on the division of similar attribute regions. By selecting representative nodes and turning off redundant nodes, the sensor can be greatly improved while ensuring high accuracy of collected data. After system testing and simulation experiments, it is verified that the wireless sensor network developed in this paper can meet the collection requirements of energy impact factors, and the data collection method proposed can greatly extend the survival time of the sensor network.}
}
@article{DELLOGLIO2021e74,
title = {Reply to Christian Daniel Fankhauser, Arie Parnham, Vijay Sangar’s Letter to the Editor re: Paolo Dell’Oglio, Hielke M. de Vries, Elio Mazzone, et al. Hybrid Indocyanine Green–99mTc-nanocolloid for Single-photon Emission Computed Tomography and Combined Radio- and Fluorescence-guided Sentinel Node Biopsy in Penile Cancer: Results of 740 Inguinal Basins Assessed at a Single Institution. Eur Urol 2020;78:865–72},
journal = {European Urology},
volume = {79},
number = {3},
pages = {e74-e75},
year = {2021},
issn = {0302-2838},
doi = {https://doi.org/10.1016/j.eururo.2020.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S0302283820310216},
author = {Paolo Dell’Oglio and Hielke M. {de Vries} and Elio Mazzone and Gijs H. KleinJan and Maarten L. Donswijk and Henk G. {van der Poel} and Simon Horenblas and Fijs W.B. {van Leeuwen} and Oscar R. Brouwer}
}
@article{DIFELICE2021132,
title = {Indeterminate pulmonary nodule in lung allograft characterized using dual-energy computed tomography},
journal = {Radiology Case Reports},
volume = {16},
number = {1},
pages = {132-135},
year = {2021},
issn = {1930-0433},
doi = {https://doi.org/10.1016/j.radcr.2020.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1930043320305872},
author = {Christopher {Di Felice} and Elias George Kikano and Benjamin Young and Amit Gupta},
keywords = {Pulmonary Nodule, Lung Transplant, Dual-Energy CT},
abstract = {Pulmonary nodules (PNs) arising in the lung transplant recipient pose a diagnostic challenge for providers. Conventional computed tomography (CT) has improved our ability to detect PNs in this population, but establishing a confident diagnosis with imaging alone remains difficult. Dual-energy spectral detector CT is a novel, emerging technology that provides insight into the radiographic behavior of PNs, and has potential in differentiating benign from malignant morphologies. Herein, we report a case of a PN in a lung transplant recipient whose initial diagnostic work-up was inconclusive, but then had the diagnosis rendered using a spectral detector CT.}
}
@article{SAMPAIO2021107246,
title = {Autonomic energy management with Fog Computing},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107246},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107246},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002330},
author = {Hugo Vaz Sampaio and Carlos Becker Westphall and Fernando Koch and Ricardo {do Nascimento Boing} and René Nolio {Santa Cruz}},
keywords = {Internet of Things, Fog Computing, Energy Management System, Autonomic System, Smart Environments, Smart homes},
abstract = {We introduce an Autonomic System to perform management of energy consumption in Internet of Things (IoT) devices and Fog Computing, including an advanced orchestration mechanisms to manage dynamic duty cycles for extra energy savings. The solution works by adjusting Home (H) and Away (A) cycles based on contextual information, like environmental conditions, user behavior, behavior variation, regulations on energy and network resources utilization, among others. Performance analysis through a proof-of-concept implementation presents average energy savings of up to 61.51% when augmenting with a scheduling system and variable long sleep cycles (LS), and potential for 75.9% savings in specific conditions. We also concluded that there is no linear relation between increasing LS time and additional savings. The significance of this research is to promote autonomic management as a solution to develop more energy efficient buildings and smarter cities, towards sustainable goals.}
}
@article{TARIQ20201,
title = {Energy and memory-aware software pipelining streaming applications on NoC-based MPSoCs},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {1-16},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19330493},
author = {Umair Ullah Tariq and Hui Wu and Suhaimi Abd Ishak},
keywords = {Conditional task graph (CTG), Retiming, MPSoC, Energy optimization},
abstract = {In this article, we explore the problem of energy-aware scheduling of real-time applications modelled by conditional task graphs on NoC based MPSoC such that the total energy consumption is minimized. We propose a novel energy and memory-aware retiming conditional task graph (EMRCTG) approach that integrates task-level coarse-grained software pipelining with Dynamic Voltage and Frequency Scaling (DVFS). Our approach not only optimizes energy consumption but ensures that memory capacity constraints are satisfied. EMRCTG has two phases. In the first phase, we map tasks to processors, transform intra-period data dependencies into inter-period and generate a schedule by a Non-Linear Programming (NLP)-based algorithm assuming infinite memory capacity. The NLP-based algorithm assigns a continuous frequency and voltage to each task and each communication and uses a polynomial-time heuristic to transform the continuous frequencies and voltages to discrete frequencies and voltages. We analyse the memory consumption of the generated schedule and initiate schedule repair phase 2 if the memory capacity constraints violate. The schedule repair phase finds a set of nodes such that by reducing their retiming values the memory capacity constraints satisfy. We compare our approach against two existing approaches GeneS and JCCTS. GeneS is a genetic algorithm that first transforms the dependent task set into an independent task set and then collectively performs task mapping, ordering and voltage scaling. JCCTS is a mixed integer linear programming based approach that optimally removes inter-processor communication overhead. Our experimental result show that compared to the approach GeneS our approach can obtain an improvement in range of 1.6 to 18 percent and an average improvement of 11 percent. Compared to the approach JCCTS our approach can achieve an improvement in range of 9 to 42 percent and an average improvement of 26 percent.}
}
@article{MANOJPRABU2020266,
title = {Improved energy efficient design in software defined wireless electroencephalography sensor networks (WESN) using distributed architecture to remove artifact},
journal = {Computer Communications},
volume = {152},
pages = {266-271},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.12.056},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419309508},
author = {M. Manojprabu and V.R. {Sarma Dhulipala}},
keywords = {WBANs, WESNs, ANNT, HFCT, MW2F, Centralized topology},
abstract = {Software Defined Networking (SDN) has focused enormous attractiveness in changing conventional network by means of offering flexible and dynamic network management. It has drawn important concentration of the researchers from together academia and industries. Mainly, integrating SDN in Wireless Body Area Network (WBAN) applications specifies capable results in terms of handling with the issues like traffic management, security, energy efficiency etc. Recent improvements in miniaturization and energy efficient physiological sensor designs in SDN based Wireless Body Area Networks (WBANs) paved the way for health monitoring systems for collection and processing the real-time physiological data. The collection of signals from different sensor allows reliable diagnosis in heterogeneous than in homogeneous WBANs. Inspired by the evolutions of heterogeneous WBANs, a study on Wireless Electroencephalography Sensor Networks (WESNs) is carried out under distributed signal processing. The distributed WESNs are designed under two different hierarchy i.e. Hierarchical Fully-Connected Topology (HFCT) and Ad-Hoc Nearest-Neighbor Topology (ANNT) to improve the energy-efficiency using distributed Multi-channel Weighted Weiner Filter design (MW2F). Here, each module transmits linear combination of local channels with other modules. The power efficiency is improved in MW2F signal processing algorithm by avoiding centralization of EEG data. A case study is carried out to test the reduced energy consumption after the removal of eye blink artifacts and it is tested with centralized counterparts. The MW2F is evaluated in both topologies against centralized environments and significant reduction of eye blink artifacts improves the energy efficiency in HFCT than other topologies.}
}
@article{GASSENMAIER2021109845,
title = {Quantification of liver and muscular fat using contrast-enhanced Dual Source Dual Energy Computed Tomography compared to an established multi-echo Dixon MRI sequence},
journal = {European Journal of Radiology},
volume = {142},
pages = {109845},
year = {2021},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2021.109845},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X21003260},
author = {Sebastian Gassenmaier and Karin Kähm and Sven S. Walter and Jürgen Machann and Konstantin Nikolaou and Malte N. Bongers},
keywords = {Magnetic resonance imaging, Liver, Multidetector computed tomography, DECT},
abstract = {Purpose
To investigate the feasibility of liver fat quantification in contrast-enhanced dual source dual energy computed tomography (DECT) using multi-echo Dixon magnetic resonance imaging (MRI) as reference standard.
Method
Patients who underwent MRI of the liver including a multi-echo Dixon sequence for estimation of proton density fat fraction in 2017 as well as contrast-enhanced DECT imaging of the abdomen were included in this retrospective, monocentric IRB approved study. Furthermore, patients with a hepatic fat amount >5% who were examined in 2018 with MRI and DECT were included. The final study group consisted of 81 patients with 90 pairs of examinations. Analysis of parameter maps was performed manually using congruent regions of interest which were placed in the liver parenchyma, in the erector spinae muscles, and psoas major muscles.
Results
Mean patient age was 61 ± 13 years. Median time between MRI and DECT was 48 days. MRI liver fat quantification resulted in a median of 3.8% (IQR: 2.2–8.2%) compared to 1.8% (IQR: 0–6.3%) in DECT (p < 0.001), with a Spearman correlation of 0.73. Bland-Altman analysis resulted in a systematic underestimation of liver fat in DECT, with a mean difference of −1.7%. Fat quantification in the erector spinae muscles (p = 0.257) and the psoas major muscles (p = 0.208) was not significantly different in DECT compared to MRI.
Conclusions
Liver and muscular fat quantification in portal-venous phase DECT is feasible with good to excellent correlation compared to a multi-echo Dixon MRI sequence analysis. While there is an underestimation of the liver fat content in DECT, there are no significant differences between DECT and MRI fat quantification of the erector spinae and psoas major muscles.}
}
@article{SIMMONS2021138967,
title = {Using collocation and solutions for a sum-of-product potential to compute vibrational energy levels for general potentials},
journal = {Chemical Physics Letters},
volume = {781},
pages = {138967},
year = {2021},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2021.138967},
url = {https://www.sciencedirect.com/science/article/pii/S0009261421006503},
author = {Jesse Simmons and Tucker {Carrington Jr.}},
keywords = {Vibrational spectroscopy, Computational method, Collocation},
abstract = {It is easier to compute a vibrational spectrum when the potential energy surface (PES) is a sum-of-products (SOP). Many popular computational methods work only if the PES is a SOP. However, the most accurate PESs are not SOPs. We propose using collocation and solutions of the Schrödinger equation with a SOP PES to compute solutions on a corresponding general PES. This makes it possible to account for coupling and anharmonicity omitted from the SOP PES. We find that correcting energy levels computed on a SOP PES with collocation reduces differences with exact energy levels by about two orders of magnitude.}
}
@article{ALIRAHMI2021121412,
title = {Soft computing analysis of a compressed air energy storage and SOFC system via different artificial neural network architecture and tri-objective grey wolf optimization},
journal = {Energy},
volume = {236},
pages = {121412},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121412},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221016601},
author = {Seyed Mojtaba Alirahmi and Seyedeh Fateme Mousavi and Pouria Ahmadi and Ahmad Arabkoohsar},
keywords = {Solid oxide fuel cell, Compressed air energy storage, Grey wolf optimizer, Artificial neural network},
abstract = {In the present study, a novel combined system consisting of solid oxide fuel cell (SOFC), organic Rankine cycle (ORC), and compressed air energy storage (CAES) is proposed, investigated, and optimized. The SOFC and CAES models are validated individually to ensure the accuracy of the results. Here, the grey wolf multi-objective optimization (MOGWO) approach is applied to find the optimal system design and performance. For this, a trained neural network is provided to the MOGWO algorithm as a fitted function, and multi-objective optimization is carried out on it. The most significant benefit of the suggested method is time-saving. The proposed system's thermodynamic performance is investigated from the energy, exergy, economic, and environmental (4E) points of view at three periods, including full-time, charging, and discharging periods. The results indicate that the Levenberg-Marquardt training algorithm has the best performance among all of the algorithms. The value of exergetic round trip efficiency (ERTE), total cost rate, and CO2 emission at the best optimum point are obtained as 45.7%, 34.2 $/h, and 0.22 kg/kWh, respectively.}
}
@article{LIU2021109635,
title = {Unsupervised deep learning based image outpainting for dual-source, dual-energy computed tomography},
journal = {Radiation Physics and Chemistry},
volume = {188},
pages = {109635},
year = {2021},
issn = {0969-806X},
doi = {https://doi.org/10.1016/j.radphyschem.2021.109635},
url = {https://www.sciencedirect.com/science/article/pii/S0969806X21002851},
author = {Chi-Kuang Liu and Hsuan-Ming Huang},
keywords = {Deep learning, Outpainting, Dual-source dual-energy computed tomography},
abstract = {Due to a physical constraint, dual-energy computed tomography (DECT) performed on a dual-source CT scanner has a restricted field of view (FOV) in one imaging chain (i.e. tube B, 140 kV with tin filter). This indicates that dual-energy analysis cannot be performed outside the limited FOV. As a result, the dual-source DECT scanner may not be beneficial for larger patients. To address this issue, we study the feasibility of using an unsupervised deep learning (DL) based method to outpaint the missing image outside the limited FOV. Brain DECT images acquired on the dual-source DECT scanner were used to simulate a restricted FOV scan. First, the whole brain of DECT images was shifted to the corner of the image. Then, the restricted 140-kV CT image was generated by multiplying the shifted 140-kV CT image with a small circular mask. As a result, we can evaluate the proposed DL-based method without scanning the patient twice. Moreover, the non-truncated 140-kV CT images can be considered as ground truth for comparison. Our results show that the proposed DL-based method can reconstruct missing data and produce 140-kV CT images that appear similar to the true 140-kV CT images. Moreover, the mean CT number differences between the true and DL-based 140-kV CT images for brain, muscle, fat and bone were less than 3 HU. We also observed that virtual monoenergetic images obtained from true and DL-based DECT images were visually similar. Our preliminary study shows the feasibility of using an unsupervised DL-based method to yield the out-of-field imaging data in dual-source DECT.}
}
@article{RAMIREZGIL2021100481,
title = {Parallel computing for the topology optimization method: Performance metrics and energy consumption analysis in multiphysics problems},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100481},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100481},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920302043},
author = {Francisco Javier Ramírez-Gil and Claudia Marcela Pérez-Madrid and Emílio Carlos Nelli Silva and Wilfredo Montealegre-Rubio},
keywords = {Multiphysics topology optimization, Heterogeneous computing, MATLAB Parallel Computing Toolbox, Energy-aware algorithms},
abstract = {The topology optimization method (TOM) is a valuable tool to obtain conceptual designs in many scientific fields. However, small-scale problems have traditionally been considered due to the high computational resources this method demands. For example, hundreds of costly optimization iterations are needed, in which millions of design variables are used and where simulation of complex multiphysics phenomena could be required. To address this difficulty, the computing capacity can be increased, or efficient code implementations can be used, or a combination of both. Herein, the computing capacity and efficiency are increased simultaneously by programming parallel codes for running on the central processing unit (CPU) and on the graphics processing unit (GPU). A multiphysics problem is used as the optimization application. Specifically, electro-thermo-mechanical (ETM) microactuators are designed by TOM. To achieve this goal, three computer code versions are developed: one optimized sequential code, another using the parallelism offered by the CPU, and a third one using parallel computing on the GPU. Typical code performance metrics such as the execution time and their acceleration are measured. Additionally, an energy consumption analysis is performed for the first time in the context of parallel computing for topology optimization, which is an important topic from large-scale supercomputers to laptops that seek energy-aware methods. The results show that topologies are obtained up to 25 times faster with up to 93% less power consumption when parallel computing is used. This time reduction in TOM allows increasing the topology resolution, the inclusion of multiple physics, and significant energy savings.}
}
@article{DEROSE2021108065,
title = {STT-MTJ Based Smart Implication for Energy-Efficient Logic-in-Memory Computing},
journal = {Solid-State Electronics},
volume = {184},
pages = {108065},
year = {2021},
issn = {0038-1101},
doi = {https://doi.org/10.1016/j.sse.2021.108065},
url = {https://www.sciencedirect.com/science/article/pii/S0038110121001106},
author = {Raffaele {De Rose} and Tommaso Zanotti and Francesco Maria Puglisi and Felice Crupi and Paolo Pavan and Marco Lanuzza},
keywords = {STT-MTJ, Logic-in-memory, Material implication, Compact modeling, SIMPLY},
abstract = {Spin-transfer torque magnetic tunnel junction (STT-MTJ) technology is an attractive solution for designing non-volatile Logic-in-Memory (LIM) architectures. This work explores a smart material implication (SIMPLY) LIM scheme based on nanoscale STT-MTJs. The SIMPLY architecture is benchmarked against the conventional material implication (IMPLY) logic. Obtained results prove that for similar performance the STT-MTJ based SIMPLY scheme ensures more reliable operation (i.e., lower error rate by more than three orders of magnitude) and an energy saving of −70% than its IMPLY counterpart, at the only cost of minimal area overhead.}
}
@article{KAMEDA2021109775,
title = {The extracellular volume fraction of the pancreas measured by dual-energy computed tomography: The association with impaired glucose tolerance},
journal = {European Journal of Radiology},
volume = {141},
pages = {109775},
year = {2021},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2021.109775},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X21002564},
author = {Fumi Kameda and Masahiro Tanabe and Mayumi Higashi and Shoko Ariyoshi and Kenichiro Ihara and Etsushi Iida and Matakazu Furukawa and Munemasa Okada and Katsuyoshi Ito},
keywords = {Dual-energy computed tomography, Pancreas, Extracellular volume fraction, Impaired glucose tolerance, Liver cirrhosis},
abstract = {Purpose
To investigate the clinical value of measuring the ECV fraction of the pancreas by DECT in association with an impaired glucose tolerance (IGT) estimated by the hemoglobin A1C (HbA1C) value in patients with or without cirrhosis.
Materials and Methods
This retrospective study included patients who underwent contrast-enhanced dynamic CT with dual-energy mode between March 2018 and February 2019. The ECV fraction of the pancreas was calculated from iodine map images created from equilibrium-phase contrast-enhanced DECT images. The cross-sectional areas of the pancreas were also measured.
Results
In total, 51 patients were analyzed (median age, 69 years old; 22 women). The ECV fraction of the pancreas showed a significant negative correlation with the HbA1c value in the cirrhotic group (ρ=-0.346, p = 0.048), while there was no significant correlation in the non-cirrhotic group (ρ=-0.086, p = 0.734). In the elevated HbA1C group, the ECV fraction of the pancreas in the cirrhotic patients (median, 0.247; interquartile range [IQR], 0.098) was significantly lower than that in the non-cirrhotic patients (0.332, IQR 0.113) (p = 0.024). In the elevated HbA1C group, the cross-sectional area of the pancreas was significantly larger in the cirrhotic patients than that in the non-cirrhotic patients (median [IQR]; 2945 [904] vs. 1885 [909] mm2, p = 0.019).
Conclusion
A reduction in the ECV fraction of the pancreas measured by DECT as well as the enlargement of the pancreatic parenchyma was observed in cirrhotic patients with IGT. These findings suggest that the measurement of the pancreatic ECV fraction by DECT may help clarify the pathophysiology of IGT in patients with cirrhosis.}
}
@article{CHIANG2014118,
title = {A green cloud-assisted health monitoring service on wireless body area networks},
journal = {Information Sciences},
volume = {284},
pages = {118-129},
year = {2014},
note = {Special issue on Cloud-assisted Wireless Body Area Networks},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2014.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0020025514007038},
author = {Hua-Pei Chiang and Chin-Feng Lai and Yueh-Min Huang},
keywords = {Cloud-assisted service, Wireless body area network, Green healthcare},
abstract = {As cloud computing and wireless body sensor network (WBAN) technologies mature, relevant applications have grown more and more popular in recent years. The healthcare field is one of the popular applications for this technology that adopts sensor devices to sense signals of negative physiological events, and to notify users. The development and implementation of long-term healthcare monitoring that can prevent or quickly respond to the occurrence of disease and accidents present an interesting challenge for computing power and energy limits. This study proposed a green cloud-assisted healthcare service on WBAN, and considered the sensing frequency of the physiological signals of various body parts, as well as the data transmission among the sensor nodes of WBAN. The cloud-assisted healthcare service regulates the sensing frequency of nodes by considering the overall WBAN environment and the sensing variations of body parts. The experimental results show that the proposed service can effectively transmit the sensing data and prolong the overall lifetime of WBAN.}
}
@article{1998221,
title = {High energy high resolution monochromatic x-ray computed tomography using synchrotron radiation: Nagata, Y.; Yamaji, H.; Hayashi, K.; Kawashima, K.; Hydodo, K.; Kawata, H.; Ando, M. Nondestructive Characterization of Materials V, Karuizawa (Japan), 27–30 May 1991. pp. 299–307. Edited by T. Kishi, T. Saito, C. Ruud and R. Green. Iketani Science and Technology Foundation (1993)},
journal = {NDT & E International},
volume = {31},
number = {3},
pages = {221},
year = {1998},
issn = {0963-8695},
doi = {https://doi.org/10.1016/S0963-8695(98)90977-X},
url = {https://www.sciencedirect.com/science/article/pii/S096386959890977X}
}
@article{MALZ2020766,
title = {Computing the power profiles for an Airborne Wind Energy system based on large-scale wind data},
journal = {Renewable Energy},
volume = {162},
pages = {766-778},
year = {2020},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2020.06.056},
url = {https://www.sciencedirect.com/science/article/pii/S0960148120309666},
author = {E.C. Malz and V. Verendel and S. Gros},
keywords = {Airborne wind energy, Optimal control, Big data, Homotopy path strategy, Machine learning},
abstract = {Airborne Wind Energy (AWE) is a new power technology that harvests wind energy at high altitudes using tethered wings. Studying the power potential of the system at a given location requires evaluating the local power production profile of the AWE system. As the optimal operational AWE system altitude depends on complex trade-offs, a commonly used technique is to formulate the power production computation as an Optimal Control Problem (OCP). In order to obtain an annual power production profile, this OCP has to be solved sequentially for the wind data for each time point. This can be computationally costly due to the highly nonlinear and complex AWE system model. This paper proposes a method how to reduce the computational effort when using an OCP for power computations of large-scale wind data. The method is based on homotopy-path-following strategies, which make use of the similarities between successively solved OCPs. Additionally, different machine learning regression models are evaluated to accurately predict the power production in the case of very large data sets. The methods are illustrated by computing a three-month power profile for an AWE drag-mode system. A significant reduction in computation time is observed, while maintaining good accuracy.}
}
@article{XU2021107022,
title = {Deep reinforcement learning assisted edge-terminal collaborative offloading algorithm of blockchain computing tasks for energy Internet},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {131},
pages = {107022},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107022},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521002623},
author = {Siya Xu and Boxian Liao and Chao Yang and Shaoyong Guo and Bo Hu and Jinghong Zhao and Lei Jin},
keywords = {Blockchain, Edge-terminal collaboration, Task offloading, Deep reinforcement learning},
abstract = {In the regional distribution network, microgrid is often used to build local energy system to realize regional autonomy in the process of power generation, transmission, and consumption. Applying blockchain technology in microgrid can meet the needs of security and privacy in energy transactions, and can conduct secure point-to-point transactions between anonymous entities. However, blockchain nodes will generate numerous computing-intensive tasks in the process of mining, and cause high delay in energy transaction. Therefore, we take advantage of mobile edge computing (MEC) technology and propose an edge-terminal collaborative mining task processing framework to increase the computing ability of the blockchain system. This framework includes three working modes: local computing, user collaboration and edge node collaboration. Particularly, the trust value of collaborative user nodes is considered to avoid security threats caused by malicious nodes. Furthermore, we establish a delay-and-throughput-based blockchain computing task offloading model, and use asynchronous advantage actor-critic (A3C) algorithm to jointly optimize offloading decision, transmission power allocation, block interval and size configuration. Simulation results show that, compared with Only-MEC and Fixed-BlockSize algorithms, the proposed algorithm can reduce the average delay by 1.7% and 2.5%, and improve the average transaction throughput by 12.1% and 28.5% respectively.}
}
@article{SOBCZYNSKA2021102561,
title = {Corrigendum to “An analysis method for data taken by Imaging Air Cherenkov Telescopes at very high energies under the presence of clouds” [Astroparticle Physics 120 (2020) 102450]},
journal = {Astroparticle Physics},
volume = {128},
pages = {102561},
year = {2021},
issn = {0927-6505},
doi = {https://doi.org/10.1016/j.astropartphys.2021.102561},
url = {https://www.sciencedirect.com/science/article/pii/S0927650521000050},
author = {Dorota Sobczyńska and Katarzyna Adamczyk and Julian Sitarek and Michał Szanecki}
}
@article{BARDAZZI2021105201,
title = {Critical reflections on Water-Energy-Food Nexus in Computable General Equilibrium models: A systematic literature review},
journal = {Environmental Modelling & Software},
volume = {145},
pages = {105201},
year = {2021},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2021.105201},
url = {https://www.sciencedirect.com/science/article/pii/S1364815221002437},
author = {Elisa Bardazzi and Francesco Bosello},
keywords = {Water-energy-food nexus, Computable general equilibrium model, Economic modelling},
abstract = {The paper analyses how the Water-Energy-Food Nexus is treated in Computable General Equilibrium (CGE) models, discussing their design, importance and possible ways of improvement. The analysis of their structure is critical for evaluating their potential efficiency in understanding the Nexus, which will be particularly effective for gauging the importance of the topic, the reciprocal dependency of its elements and the expected macroeconomic, demographic and climatic pressures that will act on its components. General equilibrium models can be useful devices to this end, as they are specifically built to track interdependencies and transmission effects across sectors and countries. Nevertheless, the review showed that most CGEs in the literature struggle to represent the competing water uses across sectors and, in particular, those concerning the energy sector. Therefore, it highlights the need to resolve this issue as a necessary step toward improving future research.}
}
@article{GUPTA2021102768,
title = {Energy-efficient dynamic homomorphic security scheme for fog computing in IoT networks},
journal = {Journal of Information Security and Applications},
volume = {58},
pages = {102768},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102768},
url = {https://www.sciencedirect.com/science/article/pii/S221421262100017X},
author = {Sejal Gupta and Ritu Garg and Nitin Gupta and Waleed S. Alnumay and Uttam Ghosh and Pradip Kumar Sharma},
keywords = {Fog computing, Security and privacy, Elliptic-Curve cryptography (ECC), Homomorphic encryption, MQTT, Dynamic key change, Man-in-the-middle (MITM) attack, Internet of Things (IoT)},
abstract = {Recently, there is an exponential increase in the multimedia and other data over the Internet of Things (IoT). This data is generally send to the cloud for processing and storage. The fog layer in-between readily bridges communication among the IoT devices and the cloud. It delivers services efficiently by computing and analyzing various multimedia information generated by the IoT devices residing on the sensors. However, provision of effective security and energy are critical challenges. The purpose of this work is to enhance the secure transfer of information like multimedia. This scheme uses Message Queue Telemetry Transport (MQTT) protocol over SSL/TLS. Since MQTT is vulnerable to eavesdropping, the Elliptic curve-ElGamal cryptography algorithm is introduced which lends a homomorphic factor thereby mitigating man-in-the-middle attack. The dynamic key change and proportional offloading of data as proposed in the current research work helps to preserve node energy by selectively transferring data to the cloud and the fog according to the data topic. The results depict that the system security and lifetime can be improved in comparison to the existing protocols.}
}
@article{HAO2021259,
title = {Energy-aware scheduling in edge computing with a clustering method},
journal = {Future Generation Computer Systems},
volume = {117},
pages = {259-272},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.11.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20330624},
author = {Yongsheng Hao and Jie Cao and Qi Wang and Jinglin Du},
keywords = {Energy-aware, Cluster, Edge computing, Energy consumption},
abstract = {With the development of Cloud and 5G technology, edge devices have been widely used in various areas. However, the limited battery energy and processing ability of edge devices hinder the usage scope of those devices. Prior studies have typically managed to immigrate virtual machines (VMs) or offload tasks to reduce energy consumption and shorten execution time. In our work, we consider devices that can obtain energy from a green energy source (such as wind energy, or solar energy). First, we use a clustering method to divide nodes into some clusters, each with some edge nodes to ensure the clusters have a minimum distance (defined by energy transferring attenuation ratios between nodes) between nodes in the cluster; the cluster center is a node with a VM. Then, based on the clustering method, a scheduling heuristic is proposed to transfer energy, immigrate VM, and allocate tasks. The simulation result shows that our proposed method reduces both the total energy consumption and the energy consumption from outside of the system (ECFO).}
}
@article{CHEN2021110692,
title = {A simple method for computing the formation free energies of metal oxides},
journal = {Computational Materials Science},
volume = {198},
pages = {110692},
year = {2021},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2021.110692},
url = {https://www.sciencedirect.com/science/article/pii/S0927025621004195},
author = {Hantong Chen and Qijun Hong and Sergey Ushakov and Alexandra Navrotsky and Axel {van de Walle}},
keywords = {Condensed phases, Ab initio, Reference state},
abstract = {This paper purposes a simple procedure for obtaining the formation free energy of metal oxides in their solid states by introducing the concept of virtual solid oxygen free energy. This scheme is meant to address the fact that standard Density functional Theory (DFT) methods tend to exhibit poor accuracy for energy differences between condensed and gas phases. The idea is to define an oxygen reference state entirely in terms of solid oxide phases, so that formation free energies can be obtained without having to compute molecular energies. The proposed reference free energy can be easily obtained from both computations and experiments, thus enabling the direct integration of DFT and experimental data. Using the molybdenum system to construct this reference, we compute the Ellingham diagram for oxides of Mo, Cu, Mg, Al and Zn, which all show good agreement with experimental values. This procedure is independent of specific total energy method used, and is applicable to formation free energy calculations of any metal oxides.}
}
@article{MASHHADI2020107527,
title = {Optimal auction for delay and energy constrained task offloading in mobile edge computing},
journal = {Computer Networks},
volume = {183},
pages = {107527},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107527},
url = {https://www.sciencedirect.com/science/article/pii/S1389128620311841},
author = {Farshad Mashhadi and Sergio A. Salinas Monroy and Arash Bozorgchenani and Daniele Tarchi},
keywords = {Mobile edge computing, Deep learning, Auction, Delay and energy sensitive tasks},
abstract = {Mobile edge computing has emerged as a promising paradigm to complement the computing and energy resources of mobile devices. In this computing paradigm, mobile devices offload their computing tasks to nearby edge servers, which can potentially reduce their energy consumption and task completion delay. In exchange for processing the computing tasks, edge servers expect to receive a payment that covers their operating costs and allows them to make a profit. Unfortunately, existing works either ignore the payments to the edge servers, or ignore the task processing delay and energy consumption of the mobile devices. To bridge this gap, we propose an auction to allocate edge servers to mobile devices that is executed by a pair of deep neural networks. Our proposed auction maximizes the profit of the edge servers, and satisfies the task processing delay and energy consumption constraints of the mobile devices. The proposed deep neural networks also guarantee that the mobile devices are unable to unfairly affect the results of the auctions. Our extensive simulations show that our proposed auction mechanism increases the profit of the edge servers by at least 50% compared to randomized auctions, and satisfies the task processing delay and energy consumption constraints of mobile devices.}
}
@article{MAH2021168131,
title = {Domain wall dynamics in (Co/Ni)n nanowire with anisotropy energy gradient for neuromorphic computing applications},
journal = {Journal of Magnetism and Magnetic Materials},
volume = {537},
pages = {168131},
year = {2021},
issn = {0304-8853},
doi = {https://doi.org/10.1016/j.jmmm.2021.168131},
url = {https://www.sciencedirect.com/science/article/pii/S0304885321004078},
author = {Wai Lum William Mah and Durgesh Kumar and Tianli Jin and S.N. Piramanayagam},
abstract = {Artificial Intelligence (AI) has been gaining traction recently. However, they are executed on devices with the von Neumann architecture, requiring high power input. Consequently, brain-inspired neuromorphic computing (NC) has been gaining attention because it is expected to be more power efficient and more suitable for AI. Designing of NC circuits involves development of artificial neurons and synapses. More studies have hitherto been focused on artificial synapses instead of neurons because the latter should demonstrate leaky integrate-and-fire (LIF) properties, which is a challenge to replicate artificially. In this work, we propose a domain wall (DW) based device made from perpendicularly magnetized (Co/Ni)n nanowire (NW) with graded magnetic anisotropy and saturation magnetization. The DW is current-driven via spin-transfer-torque. Micromagnetic simulations demonstrated that the DWs in NWs with anisotropy field gradients can automatically return towards the initial position when electrical current is absent, indicative of the leakage process. The underlying physics of DW motion in such structure was studied in detail. To replicate the crystallinity of (Co/Ni)n structures, granular NWs were also defined. Depending on the grain structure of the NW, it was found that LIF properties were achieved under the conditions of steep anisotropy field gradients. Therefore, the proposed design has potential applications in neuron devices.}
}
@article{LOW20211754,
title = {Cost-Effective Analysis of Dual-Energy Computed Tomography for the Diagnosis of Occult Hip Fractures Among Older Adults},
journal = {Value in Health},
volume = {24},
number = {12},
pages = {1754-1762},
year = {2021},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2021.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S109830152101593X},
author = {Ying Liang Low and Eric Finkelstein},
keywords = {dual-energy computed tomography, magnetic resonance imaging, occult hip fracture},
abstract = {Objective
Early and accurate diagnosis of hip fractures minimizes morbidity and mortality. Although current guidelines favor magnetic resonance imaging (MRI) for the diagnosis of occult hip fractures, a new technology called dual-energy computed tomography (DECT) seems an effective alternative. This article investigates a potentially cost-effective strategy for the diagnosis of occult hip fractures in older adults in Singapore.
Methods
A decision tree model was developed to compare costs from a payer’s perspective and outcomes in terms of quality-adjusted life-years (QALYs) of different imaging strategies for diagnosing occult hip fracture, comparing MRI with DECT supplementing single-energy computed tomography (SECT) and SECT alone. Model inputs were obtained from local sources where available. Sensitivity analyses are performed to test the robustness of the results.
Results
The MRI strategy was dominated by the DECT strategy, whereas DECT supplementing SECT provided 0.30 more QALYs at an incremental cost of SGD106.41 with an incremental cost-effectiveness ratio of SGD352.52 per QALY relative to SECT alone. DECT seemed a cost-effective strategy at a willingness-to-pay threshold of SGD50 000 per QALY.
Conclusion
DECT supplementing SECT is a cost-effective imaging strategy to diagnose occult hip fractures among older adults in Singapore and should be included in clinical pathways to expedite timely treatment and considered for reimbursement schemes.}
}
@article{RAHMAN2022196,
title = {EnTruVe: ENergy and TRUst-aware Virtual Machine allocation in VEhicle fog computing for catering applications in 5G},
journal = {Future Generation Computer Systems},
volume = {126},
pages = {196-210},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.07.036},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21002983},
author = {Fatin Hamadah Rahman and S.H. Shah Newaz and Thien-Wan Au and Wida Susanty Suhaili and M.A. Parvez Mahmud and Gyu Myoung Lee},
keywords = {Energy efficiency, Trust, Vehicular fog, VM migration},
abstract = {It is undoubted that fog computing contributes in catering the latency-stringent applications of 5G, and one of the enabling technologies that fundamentally ensures the success of fog computing is virtualization as it offers isolation and platform independence. Although the emergence of vehicle-based fog (referred to as v-fog) facilities can certainly benefit from these desirable features of virtualization, there are several challenges that need to be addressed in order to realize the full potential that v-fogs can offer. One of the challenges of virtualization in v-fog is Virtual Machine (VM) migration. There are several factors that trigger a VM migration in a v-fog such as vehicle resource depletion. VM migrations would not only lead to nonessential usage of valuable resources (e.g. energy, bandwidth, memory) in the v-fogs, but also incur various overheads and performance degradation throughout the whole network. Thus, minimizing VM migrations is necessary. Furthermore, to ensure the seamless VM migrations between v-fogs, trust of v-fogs is required. While there exists studies of trust in the virtualization of cloud, they are irrelevant to v-fogs as v-fogs are different in nature (i.e. heterogeneous, mobile) from the cloud. Additionally, trust is not included in the decision making mechanisms of VM allocation for vehicular environments in the existing works. Moreover, as vehicle resources are constrained, their energy has to be utilized efficiently. In this paper, we propose EnTruVe, an ENergy and TRUst-aware VM allocation in VEhicle fog computing solution that aims to minimize the number of VM migrations while reducing VM processing associated energy consumption as much as possible. The VM allocation algorithm in EnTruVe provides a larger selection pool of v-fogs that meets the VMs requirements (e.g. trust, latency), thereby ensuring higher chances of success of VM allocation. Using Analytic Hierarchy Process (AHP), the proposed EnTruVe solution evaluates the v-fogs based on a set of metrics (e.g. energy consumption and end-to-end latency) to select the optimal v-fog for a VM allocation. Results obtained demonstrate that EnTruVe has the least number of VM migrations and it is the most energy efficient solution. Additionally, it shows that EnTruVe provides the highest utilization of v-fogs of up to 57.6% in comparison to other solutions as the number of incoming requests increases.}
}
@article{BESAGNI2020118674,
title = {MOIRAE – bottom-up MOdel to compute the energy consumption of the Italian REsidential sector: Model design, validation and evaluation of electrification pathways},
journal = {Energy},
volume = {211},
pages = {118674},
year = {2020},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2020.118674},
url = {https://www.sciencedirect.com/science/article/pii/S0360544220317825},
author = {Giorgio Besagni and Marco Borgarello and Lidia {Premoli Vilà} and Behzad Najafi and Fabio Rinaldi},
keywords = {Electrification pathways, Energy demand modeling, Residential sector, Bottom-up modeling},
abstract = {This paper presents a novel bottom-up modeling approach (MOIRAE) to compute the thermal and electrical energy consumptions of the residential sector, which is later applied to the Italian case. First, the structure of the model has been designed and the energy balance models for the electrical and thermal appliances have been described. The energy balance models have been applied to the micro-data offered by the Italian Institute of Statistics (ISTAT), consisting of a statistical-nationwide-representative sample and providing a comprehensive description of all the Italian households. Subsequently, MOIRAE performance has been assessed considering both global and local validations. On one hand, from a global point of view, MOIRAE has been validated by comparing the numerical outcomes (thermal and electrical energy consumptions) with historical national aggregated data. On the other hand, from a local point of view, MOIRAE has been validated by comparing the numerical outcomes with available micro-data, describing energy expenditure of the different households. Finally, MOIRAE has been employed to elucidate the electrification pathways for the Italian residential sector, aiming at replacing the natural gas, LPG, diesel and fuel oil energy carriers with electrical energy.}
}
@article{MACUROVA2020219,
title = {Determinig the Energy Equivalent Speed by Using Software Based on the Finite Element Method},
journal = {Transportation Research Procedia},
volume = {44},
pages = {219-225},
year = {2020},
note = {LOGI 2019 - Horizons of Autonomous Mobility in Europe},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.02.050},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520301009},
author = {Ľudmila Macurová and Pavol Kohút and Marek Čopiak and Ladislav Imrich and Miroslav Rédl},
keywords = {Energy equivalent speed, finite element method, PC-Crash},
abstract = {Paper is focused on the current possibilities of determining the value of energy equivalent speed using the PC Crash software based on the finite element method. Determining the energy equivalent speed of vehicles is one of the key tasks for the expert who analyzes the accident. The paper contains a definition of energy equivalent speed and theoretical basis of its calculation in PC-Crash software. Despite significant scientific progress, there are a few limitations of current methods for determining the energy equivalent speed in terms of accuracy and usability. The results of the simulation programs are highly dependent on the input parameters entered and therefore need to be within a technically acceptable range. The results obtained in simulation programs should also be verified using another method of determination of energy equivalent speed.}
}
@article{GASCHO201821,
title = {Farewell, “Green Sophie” – Hidden features of the 50 Swiss Franc banknote detected by computed tomography},
journal = {Journal of Forensic Radiology and Imaging},
volume = {14},
pages = {21-23},
year = {2018},
issn = {2212-4780},
doi = {https://doi.org/10.1016/j.jofri.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212478018300674},
author = {Dominic Gascho and Michael J. Thali and Stephan A. Bolliger}
}
@article{LAMBELL2021111061,
title = {Marked losses of computed tomography–derived skeletal muscle area and density over the first month of a critical illness are not associated with energy and protein delivery},
journal = {Nutrition},
volume = {82},
pages = {111061},
year = {2021},
issn = {0899-9007},
doi = {https://doi.org/10.1016/j.nut.2020.111061},
url = {https://www.sciencedirect.com/science/article/pii/S0899900720303440},
author = {Kate J. Lambell and Gerard S. Goh and Audrey C. Tierney and Adrienne Forsyth and Vinodh Nanjayya and Ibolya Nyulasi and Susannah J. King},
keywords = {Body composition, Computed tomography, Skeletal muscle mass, Critical illness, Nutrition support},
abstract = {Objectives
Changes in muscularity during different phases of critical illness are not well described. This retrospective study aimed to describe changes in computed tomography (CT)–derived skeletal muscle area (SMA) and density (SMD) across different weeks of critical illness and investigate associations between changes in these parameters and energy and protein delivery.
Methods
Thirty-two adults admitted to the intensive care unit (ICU) who had ≥2 CT scans at the third lumbar area performed ≥7 d apart were included in the study. CT-derived SMA (cm2) and SMD (Hounsfield units) were determined using specialized software. A range of clinical and nutrition variables were collected for each day between comparator scans. Associations were assessed by Pearson or Spearman correlations.
Results
There was a significant decrease in SMA between the two comparator scans where the first CT scan was performed in ICU wk 1 (n = 20; P < .001), wk 2 (n = 11; P < .007), and wk 3 to 4 (n = 7; P = .012). There was no significant change in SMA beyond ICU wk 5 to 7 (P = .943). A significant decline in SMD was observed across the first 3 wk of ICU admission (P < .001). Overall, patients received a mean 24 ± 6 kcal energy/kg and 1.1 ± 0.4 g protein/kg per study day and 83% of energy and protein requirements according to dietitian estimates. No association between SMA or SMD changes and nutrition delivery were found.
Conclusions
Critically ill patients experience marked losses of SMA over the first month of critical illness, attenuated after wk 5 to 7. Energy and protein delivery were not associated with degree of muscle loss.}
}
@article{HUSSAIN2020106617,
title = {Clonal selection algorithm for energy minimization in software defined networks},
journal = {Applied Soft Computing},
volume = {96},
pages = {106617},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106617},
url = {https://www.sciencedirect.com/science/article/pii/S156849462030555X},
author = {M.W. Hussain and B. Pradhan and X.Z. Gao and K.H.K. Reddy and D.S. Roy},
keywords = {Software-defined networking (SDN), Clonal Selection Algorithm (CSA), Load balancing, Energy optimization},
abstract = {With the advancements of Information and Communication Technologies (ICT), large scale distributed computing and massive data center infrastructures are becoming more common these days. Such trends have drastically put a lot of load on the volumes of data transferred over networks, thus necessitating close to capacity link utilizations flexible forwarding decision-making. Software-defined networking (SDN), with its inherent segregation of control and data planes, provides flexible decision making that can leverage the global network information available to the SDN controller for dynamic and accurate solutions. However, contemporary researchers have focused on the flexibility and security aspects of SDN, widely ignoring energy consumption strategies in next-generation IP networks, which otherwise is a crucial driver in any research field. The scanty existing energy minimization strategies are mostly based on aggregate traffic, which leads to imbalanced utilization of links and affects the quality of service (QoS) adversely. In this paper, we leverage SDN’s key benefits for reducing energy network consumption while realizing dynamic load balance with a few QoS constraints. To this end, a multiobjective optimization problem (MOOP) is formulated that attempts to minimize power consumption and link utilization. With different capacities of switches and links, finding optimal configurations and deciding best paths, even for relatively small networks, become computationally challenging and is, in fact, an NP-hard problem. In this paper, we propose to employ the Clonal Selection Algorithm (CSA), a discrete, metaheuristic solution to find out optimal solutions for this MOOP, namely a Clonal Selection based Energy Minimization (CSEM). Simulations have been carried out for testing the efficacy of the proposed CSEM using real-life network topologies and link-traffic data. The results obtained by the proposed CSEM prove to be efficacious, and the same have also been validated with three different benchmark functions to test the suitability of SDN for CSA.}
}
@article{OPREA2021107293,
title = {Edge and fog computing using IoT for direct load optimization and control with flexibility services for citizen energy communities},
journal = {Knowledge-Based Systems},
volume = {228},
pages = {107293},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107293},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121005554},
author = {Simona-Vasilica Oprea and Adela Bâra},
keywords = {IoT, DR, Optimization, Flexibilities, Algorithm, Fog and edge computing},
abstract = {The grid congestion and imbalance problems are nowadays approached through a new perspective of the energy flexibility services that are usually acquired through a bidding process by aggregators on behalf of grid operators and electricity retailers and traded via Local Flexibility Markets (LFM). They can be also acquired at a fixed price under specific conditions of interruption and operation. The flexibility services are offered by electricity consumers and prosumers that own generation and storage facilities and controllable appliances, adding value to the emerging Information and Communications Technology (ICT) and smart metering technologies. In this paper, we propose an adaptive direct load optimization and control with an Internet of Things (IoT) architecture and compare it with a classic approach of Direct Load Control (DLC). The optimization process consists in a day-ahead scheduling that aims to minimize the electricity expense using programmable appliances (shift) and their operational constraints, whereas the adaptive DLC comes on top of it with additional load control (shed) using flexibility to cope with real-time uncertainties including the temperature comfort of the consumers. As the consumption is recorded using smart meters and appliances that continuously generate large volumes of data at different time resolution and formats requiring fast analytical and decisional processes, the challenge consists in correlating and integrating the optimization requirements with IoT and appliance control within an edge and fog computing architecture to overcome grid congestion, power capacity scarcity and forecast errors. The simulations are performed using real open datasets consisting in 114 single-family houses that form a small community with modern and flexible appliances, providing that the electricity bill reduction is up to 22.62%. Furthermore, to validate, several indicators for consumers and aggregator are proposed: total daily used flexibility decreased on average by 21.05%, number of interruptions also decreased on average by 20.51%, maximum number of interruptions per appliance decreased by 58.33%, while Peak to Average Ratio (PAR) improved by 32% when implementing the proposed DLC architecture.}
}
@article{FERVERS2021109502,
title = {Virtual calcium-suppression in dual energy computed tomography predicts metabolic activity of focal MM lesions as determined by fluorodeoxyglucose positron-emission-tomography},
journal = {European Journal of Radiology},
volume = {135},
pages = {109502},
year = {2021},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2020.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X20306926},
author = {Philipp Fervers and Andreas Glauner and Roman Gertz and Philipp Täger and Jonathan Kottlors and David Maintz and Jan Borggrefe},
keywords = {Multiple myeloma, Osteolytic bone lesion, Virtual non-calcium, Dual energy CT, PET/CT metabolism},
abstract = {Purpose
Recent studies showed that dual energy CT (DECT) allows for detection of bone marrow infiltration in multiple myeloma (MM) by obtaining virtual non-calcium (VNCa) images. This feasibility study investigated, if VNCa imaging might discriminate metabolically active, focal lesions in MM against avital lesions in MM patients, considering fluorodeoxyglucose positron-emission-tomography CT (FDG PET/CT) as the standard of reference.
Method
The study included 60 osteolytic lesions in 10 consecutive low-dose whole body CT scans of patients with MM, who underwent both FDG PET/CT and DECT at a tertiary care university hospital. Circular ROI measurements were performed in predefined lesions on the monoenergetic CT (MECT) and VNCa images by three blinded radiologists. Each lesion was rated vital or avital by a blinded specialist of nuclear medicine, based on their FDG metabolism.
Results
Each of the three readers could separate FDG PET/CT negative and positive MM lesions when analyzing the VNCa images, while MECT did not show a significant difference. Best results were yielded by high calcium suppression with excellent inter-rater reliability (average sensitivity 0.91, specificity 0.88, cutoff -46.9 HU), followed by medium and low calcium suppression.
Conclusions
In contrast to MECT imaging, VNCa imaging in DECT appears to be feasible to assess metabolic activity of focal MM lesions as defined by the standard of reference, FDG PET/CT. Considering the higher cost and radiation exposure of FDG PET/CT, DECT VNCa imaging might develop to be the modality of choice to assess metabolic activity of focal MM lesions.}
}
@article{ALRAJEH2021100468,
title = {Using Virtual Machine live migration in trace-driven energy-aware simulation of high-throughput computing systems},
journal = {Sustainable Computing: Informatics and Systems},
volume = {29},
pages = {100468},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100468},
url = {https://www.sciencedirect.com/science/article/pii/S221053792030192X},
author = {Osama Alrajeh and Matthew Forshaw and Nigel Thomas},
keywords = {Live migration, HTC, Energy-efficiency, Fault-tolerance},
abstract = {Virtual Machine (VM) live migration is one of the strategic approaches that can be employed to reduce energy consumption and increase the utilisation of a large computing infrastructure. In this paper, we implement the pre-copy live migration algorithm to provide a test environment for live job migration in a high throughput computing system (HTC). We present details of our extension into the HTC-Sim simulation framework. Furthermore, we demonstrate the performance and energy impact of our responsive live migration approach in high throughput computing (HTC) systems with various migration policies. The proposed policies focus on which computer to select for migration from an HTC resource pool. We evaluate our migration policies through the use of our HTC-Sim simulation framework. Moreover, we compare the results between the policies as well as the system where migration is not considered. We demonstrate that our responsive migration could save approximately 75% of the system wasted energy due to job evictions by user interruptions where migration is not employed as a fault-tolerance mechanism.}
}
@incollection{BHASKERNAIR20221177,
title = {A Software Framework for Optimal Multiperiod Carbon-Constrained Energy Planning},
editor = {Yoshiyuki Yamashita and Manabu Kano},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {49},
pages = {1177-1182},
year = {2022},
booktitle = {14th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-85159-6.50196-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851596501962},
author = {Purusothmn Nair S. {Bhasker Nair} and Dominic C.Y. Foo and Raymond R. Tan and Michael Short},
keywords = {Multiperiod Energy Planning, Negative Emission Technologies, Process Integration, Policymaking, Decarbonisation Software},
abstract = {The delay in action to mitigate climate change has resulted in a greater dependence on carbon dioxide removal (CDR) to achieve net-zero carbon targets by 2050. This work reports a newly developed optimal decarbonisation software framework that is based on a superstructure targeting approach to energy planning. The novel mathematical optimisation tool, which is formulated as a mixed-integer linear program (MILP), determines the optimum deployment of renewable energy sources, negative emission technologies (NETs), and CO2 capture and storage (CCS) for long-term regional energy planning, subject to budget and emissions constraints. The software can be used by policymakers to determine long-term energy decarbonisation strategy including when to decommission which plants, what technologies to employ when, and which fuels could be replaced by lower-carbon alternatives. The application of the software framework is demonstrated with a case study containing seven power plants. In this multiperiod work, CCS deployment is favoured for coal-based power plants due to their high CO2 intensity, while energy-producing NETs is deployed for all periods.}
}
@article{WANG2021295,
title = {Economic benefits of Northeast Asia energy interconnection: A quantitative analysis based on a computable general equilibrium model},
journal = {Global Energy Interconnection},
volume = {4},
number = {3},
pages = {295-303},
year = {2021},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2096511721000633},
author = {Jian Wang and Shenghao Feng and Junyong Xiang},
keywords = {Global Energy Interconnection, CGE, Economic impacts, Northeast Asia},
abstract = {There has been an intense discussion on the energy infrastructure cooperation in Northeast Asia. Most studies have focused on the technical feasibility of grid interconnection, deployment of renewable energy, and have ignored the quantitative analysis of social and economic benefits of these proposals. This study uses a computable general equilibrium model to evaluate the effects of energy interconnection in Northeast Asia. Key model development tasks include 1) constructing a new nesting structure, 2) econometrically estimating the constant elasticities of substitution (CES) between fossil- and non-fossil-power generation bundles, 3) developing a new base-case scenario, and 4) developing the policy scenario. We found that while Northeast Asia will benefit from energy interconnection development with higher GDP than in the base-case; there will be a trade-off between higher investment and lower consumption. Sector results and environmental implications in this region are also discussed.}
}
@article{OPREA2022108812,
title = {Corrigendum to “A signaling game-optimization algorithm for residential energy communities implemented at the edge-computing side” [Comput. Ind. Eng. 169 (2022) 108272]},
journal = {Computers & Industrial Engineering},
volume = {174},
pages = {108812},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108812},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222008002},
author = {Simona-Vasilica Oprea and Adela Bâra}
}
@article{ZUO2021109681,
title = {Detection of bone marrow edema in osteonecrosis of the femoral head using virtual noncalcium dual-energy computed tomography},
journal = {European Journal of Radiology},
volume = {139},
pages = {109681},
year = {2021},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2021.109681},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X21001613},
author = {Tianzi Zuo and Yingmin Chen and Hongming Zheng and Xiuchuan Jia and Yunfeng Bao and Yuhang Wang and Ling Li and Xiaoying Huang},
keywords = {Bone marrow edema, Virtual noncalcium, Dual-energy CT, Magnetic resonance, Osteonecrosis of the femoral head},
abstract = {Purpose
To determine the diagnostic performance of virtual noncalcium (VNCa) dual-energy computed tomography (DECT) in the detection of bone marrow edema (BME) in participants with osteonecrosis of the femoral head (ONFH).
Methods
In this prospective study, 24 consecutive participants (15 men, 9 women; mean age, 44 years, range, 21–72 years) diagnosed with ONFH who underwent DECT and magnetic resonance imaging (MRI) between September 2019 and January 2020 were involved. Two independent readers visually evaluated color-coded VNCa images using a binary classification (0 = normal bone marrow, 1 = BME). MRI served as the reference standard for the presence of BME. Interobserver agreement for the visual evaluation of VNCa DECT images was calculated with κ statistics. We determined computed tomography (CT) numbers on VNCa images and weighted-average CT sets using region-of-interest-based quantitative analysis. The t-test was used to compare the differences of CT values between BME areas and normal bone marrow areas. Receiver operating characteristic (ROC) curve was used to select an optimal CT values of VNCa images for detecting BME. A p value of <0.05 was considered as statistically significant.
Results
The sensitivity, specificity, and accuracy of Reader 1 and Reader 2, respectively, in the identification of BME at DECT were 95 % and 89 % (18 and 17 of 19), 96 % and 96 % (25 and 25 of 26), and 93 % (43 and 42 of 45). Interobserver agreement was excellent (κ = 0.86). The VNCa CT numbers of the BME area and the normal bone marrow area were −28.6 (−17.9–−39.4) HU and −97.9 (−91.3–−104.4) HU, respectively, with statistical significance (t = −10.6, p < 0.001). The weighted-average CT numbers of the BME area and the normal bone marrow area were 152.4(122.2–182.7) HU and 121.1(103.6–183.6) HU, respectively, with no statistical significance (t = −2.0, p > 0.05). The area under the receiver operating characteristic curve was 0.99 in differentiation of the BME from normal bone marrow. A cut-off value of −57.2 HU yielded overall sensitivity, specificity, and accuracy, respectively, of 95 % (18 of 19), 100 % (26 of 26), and 98 % (44 of 45) detection of BME in participants with ONFH.
Conclusion
Visual and quantitative analyses of VNCa images shows excellent diagnostic performance for assessing BME in participants with ONFH.}
}