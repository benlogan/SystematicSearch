@article{OMAR2022110365,
title = {Al2O3:C and LiF: Mg, Ti characterisations at 100–150 kV energy range for computed tomography dose measurement},
journal = {Radiation Physics and Chemistry},
volume = {199},
pages = {110365},
year = {2022},
issn = {0969-806X},
doi = {https://doi.org/10.1016/j.radphyschem.2022.110365},
url = {https://www.sciencedirect.com/science/article/pii/S0969806X2200408X},
author = {R.S. Omar and S. Hashim and D.A. Bradley and M.K.A. Karim and I. Kobayashi and A.B.A. Kadir and A. Hashim},
keywords = {Characterisation, CT, OSLD, RQT},
abstract = {Performance characterisation has been carried out of nanoDot™ OSLDs and TLD-100™, validating their use for diagnostic x-ray dose profile assessments. Investigations include optical annealing, signal depletion, signal fading, dose-response, sensitivity, and energy dependence. Both dosimeter types were exposed using the Constant Potential Industrial X-ray (Model Philips MG 165) in Standard Radiation Qualities (RQT) procedure located in Nuclear Malaysia, Bangi. OSLD annealing using a 14 W compact fluorescent lamp showed an average signal loss of ∼93% as a result of 60 min of illumination. For dosimetric signal depletion, screened nanoDot™ OSLDs gave rise to the most favorable performance, with a mean signal loss of 1.0% per reading. In respect of signal fading, similar favorable performance was found for the screened nanoDot™ OSLDs, after a stabilisation period of 11–12 days post-irradiation, the average reading decreased by ∼1% over a further 17 days. For doses up to 500 mGy the TLD-100™ and screened nanoDot™ OSLDs both provide a highly linear response, with a regression coefficient of 0.999 in both cases. Linear energy dependence was found for RQT spectra from 100 to 150 kV.}
}
@article{XIE2023238,
title = {UWPEE: Using UAV and wavelet packet energy entropy to predict traffic-based attacks under limited communication, computing and caching for 6G wireless systems},
journal = {Future Generation Computer Systems},
volume = {140},
pages = {238-252},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22003260},
author = {Zichao Xie and Zeyuan Li and Jinsong Gui and Anfeng Liu and Neal N. Xiong and Shaobo Zhang},
keywords = {Traffic-based attack, Wavelet packet energy entropy, Sensor-cloud systems, 6G, IoT communication, Trust evaluation},
abstract = {The development of 6G has enhanced the communication capabilities of Internet of Things (IoT) devices. However, in wireless system, due to cost constraints, only a few devices have the communication capability of 6G, and the rest can only communicate with the Internet through self-organized networks. Due to simple hardware, these IoT devices have low capacities of communication, computing and caching. And they are vulnerable to all kinds of attacks. One of harmful attacks is traffic-based attack such as on–off attack, Denial of Service (DoS) attack which consumes the limited energy of IoT devices and wreaks havoc on data-based applications. However, there is no effective way to obtain the truth traffic of IoT devices, which makes it difficult for the cloud to secure the communication of IoT devices and manage the state of network. To ensure reliable communication, a novel approach to detect traffic-based attack by Unmanned Aerial Vehicle (UAV) and Wavelet Packet Energy Entropy (UWPEE) is proposed. In UWPEE scheme, UAV is sent to collect the truth traffic from IoT devices. Then wavelet packet energy entropy is innovatively adopted to detect attacks. Finally, the trust of IoT devices is determined according to their entropy. The experimental results show that UWPEE scheme can effectively identify traffic-based attacks with an accuracy rate of 84.47% and an average recognition efficiency of 4.89 for malicious nodes. Meanwhile, compared with the greedy algorithm, the flight path of the UAVs is reduced by 15.44%.}
}
@article{WANG2020119245,
title = {Modelling green multimodal transport route performance with witness simulation software},
journal = {Journal of Cleaner Production},
volume = {248},
pages = {119245},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.119245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619341150},
author = {Qing-Zhou Wang and Jia-Meng Chen and Ming-Lang Tseng and Hai-Min Luan and Mohd Helmi Ali},
keywords = {Green multimodal transport, Road network, Sensitivity analysis, Energy conservation, Emission reduction},
abstract = {This study proposes an approach for dynamically calculating multimodal transport line parameters based on a “Witness” software simulation. The approach is designed to reduce the carbon emissions of multimodal transport, as green transportation has recently received increased attention. A multi-objective multimodal transport efficiency evaluation framework is combined with an order relation analysis and an entropy-weight fuzzy analytical hierarchy process method to assess route performance under various multimodal transport schemes. This study validated the proposed approach using the China-Europe railway network from Wuhan, China to Berlin, Germany. The results are as follows: (1) the energy conservation and emission reduction values of a multimodal transport scheme fluctuate significantly, and it is scientifically appropriate to increase the energy conservation and emission reduction values as the indexes for scheme planning; (2) a multi-objective and multi-scenario efficiency evaluation method can provide a solution for green multi-modal transport route planning, and can reduce energy usage and emissions by 24% as compared to an optimal routing considering a single objective; and 3) the China-Europe railway express has unique advantages in a multi-modal transport network, as it can save more than 60% of the transport time and reduce the carbon emissions by 40% as compared with a traditional water transport. This study presents a green multi-modal transport organization approach.}
}
@article{KAUR2022101732,
title = {Cloud-Fog Assisted Energy Efficient Architectural Paradigm for Disaster Evacuation},
journal = {Information Systems},
volume = {107},
pages = {101732},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101732},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000089},
author = {Amandeep Kaur and  Sahil and Sandeep Kumar Sood},
keywords = {Fog computing, Panic attacks, Energy conservation, Fuzzy K-Nearest Neighbour (FK-NN), Principal Component Analysis (PCA), Holt Winters method},
abstract = {The disasters often lead to panic among people resulting into life threatening crowd movements. An efficient and systematic evacuation can mitigate the impact of devastation on human life. The evacuation, on the wake of extreme events, requires delay-sensitive computation and energy conserving mechanisms for power constrained data collection devices. In this paper, an IoT based Cloud-Fog assisted layered evacuation system is proposed for mass evacuation during emergency situation. The system utilizes fog computing paradigm to evaluate the panic health state in real-time using fuzzy K-nearest neighbour (FK-NN) method. The energy conserving mechanisms at fog layer performs data selection and data reduction to reduce the amount of data transmission to upper layer. Spatio-temporal Data Selection layer chooses the data for further transmission on the basis of its spatio-temporal analysis. Data reduction component reduces the dimensions of data using Principal Component Analysis (PCA). The cloud layer examines the future temporal health state of evacuees by employing Holt-Winters method. Geographic Analysis layer at cloud determines the priority evacuation status of person on the basis of current and future temporal panic state. Moreover, it specifies the priority evacuation status of region using the count of currently panicked and predicted health status. Experimentation proves the efficiency of proposed system at various levels.}
}
@article{WU2021102974,
title = {Cloud-to-edge based state of health estimation method for Lithium-ion battery in distributed energy storage system},
journal = {Journal of Energy Storage},
volume = {41},
pages = {102974},
year = {2021},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2021.102974},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X21006885},
author = {Ji Wu and Xingtao Liu and Jinhao Meng and Mingqiang Lin},
keywords = {Cloud-to-edge, State of health, Random forest, Measurement noise, Feature selection},
abstract = {Of the key parameters in the battery management system, the state of health is the most vital one concerning the distributed energy storage system's safety. Due to the limitation of the computing power of the battery management system in the actual application, a cloud-to-edge based state of health estimation method is proposed in this paper, where the battery management system is used to measure and pre-process the voltage and current data of the battery in the edge side. The cloud platform is utilized to estimate the state of health with an advanced data-driven algorithm on the cloud side. To reduce the complexity of the estimator, save the network traffic and decrease the impacts from measurement noises, a 3-round feature selection approach is developed to extract the measured battery data from the charging process. Afterward, a random forest regressor is applied to build the battery degradation model with the selected features and then estimate the state of health. Experimental results show that using the selected features may have a sufficient estimating accuracy while costing fewer traffic data and calculations. The proposed method also has a solid ability to reduce the effects of the noises. Moreover, experiments with different lithium-ion batteries are conducted to demonstrate the universality of the proposed method.}
}
@article{ZHANG2020299,
title = {Resource scheduling of green communication network for large sports events based on edge computing},
journal = {Computer Communications},
volume = {159},
pages = {299-309},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420302899},
author = {Bin Zhang and Dexu Chen},
keywords = {Edge computing, Sports events, Network systems, Resource scheduling, Green communications},
abstract = {The operation of large-scale sports events requires strong support from the network foundation. At the same time, network resource scheduling in the era of big data not only determines the operating efficiency of communication network systems, but also plays an important role in effectively reducing greenhouse gas emissions. In order to improve the green energy-saving effect of large-scale sports events network operation, this study analyzes the operation process of its network system and uses the time slot method to perform task routing decision processing. Moreover, this research uses Lyapunov’s optimization technology for algorithm design and algorithm construction for service chain cache and routing problems. In addition, this study combines large-scale sports event communication networks to conduct controlled trial design and performs algorithm performance analysis through simulation analysis. The research shows that the algorithm proposed in this paper can obtain a solution with better performance and can provide theoretical references for subsequent related research.}
}
@article{LIU2022,
title = {Adaptive delay-energy balanced partial offloading strategy in Mobile Edge Computing networks},
journal = {Digital Communications and Networks},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001225},
author = {Shumei Liu and Yao Yu and Lei Guo and Phee Lep Yeoh and Branka Vucetic and Yonghui Li},
keywords = {Mobile edge computing (MEC), Delay, Energy consumption, Dynamic balance, Partial computation offloading},
abstract = {Mobile Edge Computing (MEC)-based computation offloading is a promising application paradigm for serving large numbers of users with various delay and energy requirements. In this paper, we propose a flexible MEC-based requirement-adaptive partial offloading model to accommodate each user's specific preference regarding delay and energy consumption. To address the dimensional differences between time and energy, we introduce two normalized parameters and then derive the computational overhead of processing tasks. Different from existing works, this paper considers practical variations in the user request patterns, and exploits a flexible partial offloading mode to minimize computation overheads subject to tolerable delay, task workload and power constraints. Since the resulting problem is non-convex, we decouple it into two convex subproblems and present an iterative algorithm to obtain a feasible offloading solution. Numerical experiments show that our proposed scheme achieves a significant improvement in computation overheads compared with existing schemes.}
}
@article{ARABAS2021102221,
title = {Modeling and simulation of hierarchical task allocation system for energy-aware HPC clouds},
journal = {Simulation Modelling Practice and Theory},
volume = {107},
pages = {102221},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102221},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X20301568},
author = {Piotr Arabas},
keywords = {HPC, Task allocation, Modeling and simulation, Power-saving techniques, Hierarchical coordination},
abstract = {The paper presents a hierarchical approach to the problem of energy-aware task allocation in a distributed HPC system consisting of several computing clusters. The solution involves the control of clusters and the use of network traffic engineering methods to minimize overall energy consumption. The advantages of the hierarchical approach are twofold: first entities controlling parts of computing infrastructure may decide how much of their responsibility is delegated to the coordinator, next the computational complexity of related mathematical programming problems may be reduced, and their solution may be done in parallel. The mathematical programming problem of task allocation is defined in the centralized form first. After discussing its computation complexity and related organizational difficulties, it is decomposed to form a hierarchy. Finally, heuristics for allocation are proposed and verified via numerical simulation.}
}
@article{WANG2022107515,
title = {Design of integrated energy market cloud service platform based on blockchain smart contract},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {135},
pages = {107515},
year = {2022},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.107515},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521007547},
author = {Lei Wang and Yichao Ma and Liuzhu Zhu and Xuli Wang and Hao Cong and Tiancheng Shi},
keywords = {Integrated energy market, Cloud service, Blockchain smart contract, Digital currency electronic Payment},
abstract = {Traditional integrated energy market has problems such as single transaction type, opaque dispatch subsidies, and lack of institutions that can provide professional and fair services in the integrated energy market. Therefore, based on the smart contract of blockchain, this paper designs the cloud service platform for integrated energy market. The participants in the integrated energy market are divided into integrated energy suppliers and integrated energy users. The two can use the Digital Currency Electronic Payment (DCEP) to trade energy (carbon) in the blockchain smart contract provided by cloud service platform, as well as implement decentralized intelligent dispatch by using blockchain Internet of things technology. The smart contract designed by Antchain is used to simulate the energy trading and dispatch process of cloud service platform, and the verifiable simulation results are given. It is proved that the integrated energy market cloud service platform based on the blockchain smart contract designed in this paper can effectively realize not only the security and convenience of energy trading, but also the efficiency and intelligence of energy dispatch.}
}
@article{NAHA2022109240,
title = {Multiple linear regression-based energy-aware resource allocation in the Fog computing environment},
journal = {Computer Networks},
volume = {216},
pages = {109240},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109240},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003164},
author = {Ranesh Naha and Saurabh Garg and Sudheer Kumar Battula and Muhammad Bilal Amin and Dimitrios Georgakopoulos},
keywords = {Fog computing, Time sensitive application, Energy-awareness, Resource allocation, Internet of Things},
abstract = {Fog computing is a promising computing paradigm for processing time-sensitive Internet of Things (IoT) applications. It helps process application requests close to the users to deliver faster processing outcomes than the Cloud by minimising overall response time. The Fog computing computation environment is highly dynamic regarding resource availability and communication. Furthermore, most Fog devices are battery-powered; hence, the chances of the failure of the application processing is high, leading to delaying the application outcome. If we process the application requests on other available devices after the failure occurs, it might cause delay and may not comply with time-sensitive requirements. To avoid application processing failure due to power unavailability, we can run applications in an energy-aware manner. This is a challenging task due to the dynamic nature of the Fog computing environment. It is required to allocate resources for application requests so that the application processing should not fail due to the unavailability of power. In this paper, we propose a multiple linear regression-based resource allocation mechanism to run applications with energy-awareness in the Fog computing environment. This approach minimises failures due to power constraints of the devices. Prior works lack energy-aware application execution considering the dynamism of the Fog computing environment. Hence, we propose multiple linear regression-based approaches to achieve energy-awareness objectives. We present a sustainable energy-aware framework and algorithm that executes applications in a Fog environment in an energy-aware manner that minimises application execution failures. The trade-off between energy-efficient allocation and application execution time has been investigated and shown to have a minimum negative impact on the system while employing energy-aware allocation. The evaluation of the proposed method is carried out in a controlled simulation environment by extending CloudSim toolkit. We compared our proposed method with existing approaches. Our proposed approach minimises the delay and processing time by 20%, and 17% compared with the existing one. Furthermore, SLA violations decreased by 57% for the proposed energy-aware allocation.}
}
@article{TANG2022268,
title = {Heterogeneous UAVs assisted mobile edge computing for energy consumption minimization of the edge side},
journal = {Computer Communications},
volume = {194},
pages = {268-279},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.023},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002705},
author = {Qiang Tang and Linjiang Li and Caiyan Jin and Lixin Liu and Jin Wang and Zhuofan Liao and Yuansheng Luo},
keywords = {Mobile edge computing, Trajectory optimization, Heterogeneous UAV},
abstract = {The research on the Internet of Things (IoT) and edge computing, especially Unmanned Aerial Vehicles assisted Mobile Edge Computing (UAV-assisted MEC) attracts more and more interests of researchers. Nowadays, MEC systems with multiple UAVs have great research value, especially for emergency communication scenarios. In this paper, a heterogeneous UAVs assisted MEC system was proposed, and with the goal of minimization the edge side energy consumption, a parallel processing was involved to jointly optimize the communication scheduling, forwarding power, offloading data size, computing frequency and the trajectory of the UAV. The problem is formulated as a Mixed Integer Non-Linear Programming (MINLP), hard to solve. Therefore, we divided the MINLP into two sub-problems by applying the Block Coordinate Descent (BCD) method, and solved it using the Lagrangian Duality (LD) method and Successive Convex Optimization (SCO). The UAV’s trajectory was initialized as a circular, and then it was optimized by using the standardized convex solver design algorithm. In addition, a heuristic algorithm was put forward to obtain the optimization solution, while we set other benchmarks and the Monte Carlo (MC) algorithm to justify its validity and rationality. The simulation results showed that our strategy has better performance at minimizing energy consumption compared with other algorithms.}
}
@article{MADHAV2023S94,
title = {Feasibility Of 1-beat Ultra-fast Switching Dual Energy Coronary Computed Tomography Angiography},
journal = {Journal of Cardiovascular Computed Tomography},
volume = {17},
number = {4, Supplement },
pages = {S94},
year = {2023},
issn = {1934-5925},
doi = {https://doi.org/10.1016/j.jcct.2023.05.232},
url = {https://www.sciencedirect.com/science/article/pii/S1934592523003556},
author = {P. Madhav and D. Okerlund and P. McNicol and H. Ma and P. Deak and R. Büchel and A. Giannopoulos and A. Pazhenkottil and A. Zabel}
}
@article{MORE2019103306,
title = {A novel, green cloud point extraction and separation of phenols and flavonoids from pomegranate peel: An optimization study using RCCD},
journal = {Journal of Environmental Chemical Engineering},
volume = {7},
number = {5},
pages = {103306},
year = {2019},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2019.103306},
url = {https://www.sciencedirect.com/science/article/pii/S2213343719304294},
author = {Pavankumar R. More and Shalini S. Arya},
keywords = {Cloud point extraction, Bioactives, Pomegranate peel, Green extraction, RCCD, Partition coefficient},
abstract = {Cloud point extraction (CPE) is one of the novel; environment-friendly; energy; time and cost-effective extraction technique. Therefore, in the present work preconcentration and separation of bioactive compounds from pomegranate peel, a major waste of the pomegranate processing industry was carried out using CPE. CPE works on the principle of entrapping the hydrophobic bioactive compound in the micelle. Surfactant concentration (5–11%), pH (4–8), temperature (30–60 °C), and salt concentration (4–12%) optimization was carried out to separate total phenols and flavonoids with maximum recovery (%R), partition coefficient (Ks/a), fraction concentration (fc), and minimum loss (%L) using the rotatable central composite design (RCCD). The optimum levels were 8.22% surfactant (Triton X-114), 4% salt (NaCl) at temperature of 36.80 °C and pH 4 which resulted in 95% recovery, 14.59 Ks/a, 0.95 fc and 5.27% loss of total phenols with the maximum desirability about 0.846. While in the case of total flavonoids at 8.27% Triton X-114 at pH 5.07/34.30 °C and 4.06% NaCl resulted, 98% recovery, 9.71 Ks/a, 1.01 fc, and 2% loss with 0.842 desirability. The total yield obtained for total phenols was 205.2 mg of GAE/g while for total flavonoids it was 60.05 mg of QE/g of pomegranate peel powder. Therefore, it can be concluded that this method can be successfully applied in the extraction of bioactive from any food systems for clean and green extractives label.}
}
@article{WU2022111481,
title = {A new method for computing the anisotropic free energy of the crystal-melt interface},
journal = {Computational Materials Science},
volume = {210},
pages = {111481},
year = {2022},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2022.111481},
url = {https://www.sciencedirect.com/science/article/pii/S0927025622002439},
author = {Lingkang Wu and Baoqin Fu and Li Wang and Lin Liu and Guichao Hu and Ben Xu and Youliang Zhang and Jin Liu},
keywords = {Critical nucleus method (CNM), Modified critical nucleus method (MCNM), Interface stiffness, Interface free energy, Anisotropy parameters},
abstract = {Based on the classical nucleation theory and the crystal-melt interface kinetic equation, a new method was proposed to compute the anisotropic free energy of the crystal-melt interface. The method stems from the fact that the interface stiffness, instead of the interface free energy, controls the morphologies of nuclei when they are in equilibrium with the surrounding melts. In addition, the interface stiffness has an anisotropy which is an order of magnitude higher than that of the interface free energy, and the anisotropies of these two quantities are related to each other. Mapping out the relation between the curvature radius on the local interface of the nucleus and the equilibrium undercooling temperature during molecular dynamics simulations, we are able to determine the interface stiffness, the interface free energy, and their anisotropy parameters. The new method was used to compute the anisotropy of interface free energy for pure Cu as an example, and the results are in good agreement with the data from experiments and existing simulations.}
}
@article{DANTHULURI20233069,
title = {Energy and cost optimization mechanism for workflow scheduling in the cloud},
journal = {Materials Today: Proceedings},
volume = {80},
pages = {3069-3074},
year = {2023},
note = {SI:5 NANO 2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.07.168},
url = {https://www.sciencedirect.com/science/article/pii/S2214785321050276},
author = {Sudha Danthuluri and Sanjay Chitnis},
keywords = {Cloud computing, Scientific workflow, Energy consumption, Reliability, Fault tolerance},
abstract = {Cloud computing has become one of the most important platforms for various applications like artificial intelligence, big data, the Internet of Things, and others, especially demand for cloud computing has exploded in recent years. Because of the increased demand for cloud computing, it has become more complex, which can lead to software or hardware failure. Due to the advanced infrastructure, failure may not be detected and repaired at a given timeline and will end up costing higher. In addition, the advent of cloud computing has another advantage over the massive spread of scientific work. Scientific workflow refers to a series of computations that enable data analysis in a distributed and systematic way; because this work flow has a large number of works, energy consumption is a major problem. Besides these issues, it also suffers from system reliability; in the response to these issues, several researchers have designed their mechanism, however they failed to understand cloud complex environment. Therefore, here we have designed a mechanism that efficiently reduces energy consumption, improve the fault tolerance to achieve reliability, performs operations in very less time, and optimize the cost in the workflow model. We have also demonstrated efficient energy optimization techniques by reducing task loads.}
}
@article{SUN2022252,
title = {An energy efficient and runtime-aware framework for distributed stream computing systems},
journal = {Future Generation Computer Systems},
volume = {136},
pages = {252-269},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002187},
author = {Dawei Sun and Yijing Cui and Minghui Wu and Shang Gao and Rajkumar Buyya},
keywords = {Stream computing, Runtime-aware, Reliable migration, Energy consumption, High throughput, Low latency},
abstract = {Task scheduling in distributed stream computing systems is an NP-complete problem. Current scheduling schemes usually have a pause or slow start process due to the fluctuation of input data stream, which affects the performance stability, especially the high throughput and low latency goals. In addition, idle compute nodes at runtime may result in large idle load energy consumption. To address these problems, we propose an energy efficient and runtime-aware framework (Er-Stream). This paper thoroughly discusses the framework from the following aspects: (1) The communication between real-time data streaming tasks is investigated; stream application, resource and energy consumption are modeled to formalize the scheduling problem. (2) After an initial topology is submitted to the cluster, task pairs with high communication cost are processed on the same compute node through a lightweight task partitioning strategy, minimizing the communication cost between nodes and avoiding frequent triggering of runtime scheduling. (3) At runtime, reliable task migration is performed based on node communication and resource usage, which in turn helps the dynamic adjustment of the node energy consumption. (4) Metrics including latency, throughput, resource load and energy consumption are evaluated in a real distributed stream computing environment. With a comprehensive evaluation of variable-rate input scenarios, the proposed Er-Stream system provides promising improvements on throughput, latency and energy consumption compared to the existing Storm’s scheduling strategies.}
}
@article{HAO2022108426,
title = {Optimizing the GPU based method calculating energy deposition of beams coupling with discrete materials in dynamical and thermal simulations for higher computing efficiency},
journal = {Computer Physics Communications},
volume = {278},
pages = {108426},
year = {2022},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2022.108426},
url = {https://www.sciencedirect.com/science/article/pii/S001046552200145X},
author = {Changwei Hao and Yuan Tian and Ping Lin and Yunzhen Du and Lijuan Yang and Sheng Zhang and Lei Yang and Qingguo Zhou and Wenshan Duan},
keywords = {Beam-target coupling, Energy deposition simulations, Granular-flow targets, Algorithm optimizations, GPUs},
abstract = {In order to obtain more accurate energy deposition simulation results of beams coupling with discrete materials especially the granular materials in dynamical and thermal simulations, a discrete energy deposition calculation method was proposed previously to replace the equivalent homogenization method and accelerated with GPUs (Tian et al., 2021 [10]). However, it was found that the computing efficiency drops so severely with the increasing of the energy space size and the time required for simulations increases greatly unacceptably. In this work, for higher computing performance, based on the bottleneck analyses of CUDA (Compute Unified Device Architecture) kernels of the previous method, two improvements were made to the space cell marking phase and the energy deposition phase. In the space cell marking phase, a new proposed Lagrange-Euler Mapping Method searching the fixed space cells through flowing grains replaced the previous Euler Searching Method searching the flowing grains from fixed space cells. In the energy deposition phase, a warp aggregation method was used, assigning fewer threads to perform atomic operations. In the simulation of a granular-flow target bombarded by a 1 GeV proton beam setting a large energy computing space with a number of cells up to ∼108, the improved algorithm can effectively reduce the number of memory access instructions of CUDA warps. As a result, the computing performance was improved by 85% on A100 GPUs and 206% on K80 GPUs, making the dynamical and thermal simulations of granular-flow targets more efficient. Our method also could be beneficial for simulating the interactions between arbitrary source (or field) with a fixed spatial distribution and discrete materials.}
}
@article{RAZAQUE20221,
title = {Energy-efficient and secure mobile fog-based cloud for the Internet of Things},
journal = {Future Generation Computer Systems},
volume = {127},
pages = {1-13},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21003320},
author = {Abdul Razaque and Yaser Jararweh and Bandar Alotaibi and Munif Alotaibi and Salim Hariri and Muder Almiani},
keywords = {Mobile fog-based cloud, IoT, Energy, Hybrid energy-efficient consumption, Blockchain technology},
abstract = {The mobile fog-based cloud (MFBC) plays a major role for the Internet of Things (IoT) and is highly suitable for obtaining virtualized data without additional waiting time. However, the limited battery power of mobile devices limits access to abundant data. To address this problem, an energy-efficient and secure algorithm should be introduced to reduce energy consumption for the MFBC. In this paper, an energy-efficient and secure hybrid (EESH) algorithm is introduced for the MFBC to support the IoT. The EESH algorithm uses the voltage scaling factor to reduce energy consumption. The performance of the EESH algorithm is substantially better than the state-of-the-art contending algorithms when the number of IoT-tasks increases, and EESH consumes minimal energy. Furthermore, the identity of mobile cloud users is of paramount significance; therefore, the EESH is further secured by applying a malicious data detection (MDD) algorithm using blockchain technology. The capability of the processors on the server side of the MFBC is analyzed and improved for the IoT. The proposed EESH, MDD and underlying algorithms are programmed on the Java platform. Finally, the performance of the proposed algorithms on the MFBC is compared with that of known algorithms in terms of security, energy efficiency, throughput and latency.}
}
@article{ROSERO2021117770,
title = {Cloud and machine learning experiments applied to the energy management in a microgrid cluster},
journal = {Applied Energy},
volume = {304},
pages = {117770},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.117770},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921011090},
author = {D.G. Rosero and N.L. Díaz and C.L. Trujillo},
keywords = {Cloud computing, Machine learning, Energy management system, Prosumer, Microgrid clustering, Renewable energy},
abstract = {The way to organize the generation, storage, and management of renewable energy and energy consumption features has taken relevance in recent years due to demands that define the social welfare of this century. Like demand increases, other factors require grid infrastructure improvement, updates, and opening to other technologies that assuage the final customer needs. Precisely, the interest in renewable energy sources, the constant evolution of energy storage technologies, the continuous research involving microgrid management systems, and the evolution of cloud computing technologies and machine learning strategies motivate the development of this article. Tasks associated with a microgrid cluster like the integration of a considerable number of heterogeneous devices, real-time support, information processing, massive storage capabilities, security considerations, and advanced optimization techniques usage could take place in an autonomous and scalable energy management system architecture under a machine learning perspective running in real-time and using Cloud resources. This paper focuses on identifying the elements considered by different authors to define a cloud-based architecture and ensure the appropriately supervised learning functionality under a microgrids cluster environment. Namely, it was necessary to revise and run microgrid simulations, real-time simulation platforms usage, connection to a virtual server for microgrid control and set the energy management system using cloud computing and machine learning. Based on the review and considering the scenarios mentioned, this article presents a scalable and autonomous cloud-based architecture that allows power generation forecast, energy consumption prediction, a real-time energy management system using machine learning techniques.}
}
@article{WU2021125362,
title = {Risk assessment of renewable energy-based island microgrid using the HFLTS-cloud model method},
journal = {Journal of Cleaner Production},
volume = {284},
pages = {125362},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.125362},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620354081},
author = {Yunna Wu and Mengyao Hu and Mingjuan Liao and Fangtong Liu and Chuanbo Xu},
keywords = {Risk assessment, Island microgrid, BWM, Hesitant fuzzy linguistic term sets, Cloud model, Fuzzy synthetic evaluation},
abstract = {Difficulty in power supply has always been the main factor hindering the sustainable development of islands. It is an effective way to address the energy supply issue of off-grid islands by building island microgrid system with abundant renewable energy, such as wind energy and solar energy. However, island microgrids offer a mix of the high penetration rate of renewable energy and the complex island environment, being confronted with many challenges in the whole life cycle. To implement the project smoothly and make full use of the advantages of renewable energy-based island microgrids, a risk assessment must be incorporated. In this study, based on a three-dimensional model, we take a holistic view of identifying risks that includes four categories (technical, economic, environmental and social aspects). Later, a reasonable and applicable risk assessment framework is constructed, in which the fuzziness and randomness of information are taken into consideration. The hesitant fuzzy linguistic term set is used to endow the index evaluation information, and the cloud model is used to transform the evaluation information. The final results show that the overall risk level of China’s island microgrids is “Slightly High”, particularly in the technical aspect. Furthermore, corresponding mitigation strategies are put forward. This study conducts a comprehensive study on the risk assessment and risk response measures of island microgrids, which is conducive to deal with potential risks, thereby to minimize the loss and promote the island microgrid development.}
}
@article{TOUBAN2022592,
title = {Computed Tomography Measured Psoas Cross Sectional Area Is Associated With Bone Mineral Density Measured by Dual Energy X-Ray Absorptiometry},
journal = {Journal of Clinical Densitometry},
volume = {25},
number = {4},
pages = {592-598},
year = {2022},
issn = {1094-6950},
doi = {https://doi.org/10.1016/j.jocd.2022.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1094695022000439},
author = {Basel M. Touban and Michael J. Sayegh and Jesse Galina and Sonja Pavlesen and Tariq Radwan and Mark Anders},
keywords = {Bone mineral density, CT scan, DEXA, fragility fracture, osteoporosis, sarcopenia},
abstract = {Dual-energy X-ray absorptiometry (DEXA) is the gold standard for osteoporosis screening and diagnosis. However, abdominal conventional computed tomography (CT) scan is widely available and multiple studies validated its use as a screening tool for osteoporosis compared to DEXA. The aim of this study was to determine the reliability of measuring core muscle size at the L3–L4 intervertebral disk space and estimate the relationship between core muscle size and bone mineral density (BMD) measured by DEXA. Retrospective chart review was performed on patients who underwent a DEXA scan for osteoporosis and a conventional abdominal CT scan within one-year apart. Total cross-sectional area (CSA) and Hounsfield Unit (HU) density of core muscles (psoas, paraspinal, and abdominal wall muscles) were measured. The association between psoas, paraspinal, abdominal, and central muscle CSA and Bone Mineral density (BMD) at L3, L4, total Lumbar Spine (LS), and right (R) and left (L) hip was estimated in crude and adjusted for age and sex linear regression models. Sixty patients (37 females, 23 males) met the inclusion criteria. The average interval between DEXA and abdominal CT scans was 3.6 months (range 0.1–10.2). Psoas muscle density was significantly positively associated with R hip BMD in both crude and adjusted models (β = 20.2, p = 0.03; β = 18.5, p = 0.01). We found a significant positive linear association between psoas muscle CSA and HU density with BMD of LS, R, and L hip in both crude and adjusted models. The strongest significant positive linear association was observed between total abdominal CSA and R hip BMD in crude and age and sex adjusted (ß = 85.3, p = 0.01; ß = 63.9, p = 0.02, respectively). CT scans obtained for various clinical indications can provide valuable information regarding BMD. This is the first study investigating association between BMD with central muscle density and CSA, and it demonstrated their significant positive the association.}
}
@article{ZAHOORRAJA2022100504,
title = {Neuro-computing intelligent networks for entropy optimized MHD fully developed nanofluid flow with activation energy and slip effects},
journal = {Journal of the Indian Chemical Society},
volume = {99},
number = {7},
pages = {100504},
year = {2022},
issn = {0019-4522},
doi = {https://doi.org/10.1016/j.jics.2022.100504},
url = {https://www.sciencedirect.com/science/article/pii/S0019452222001662},
author = {M. Asif {Zahoor Raja} and M. Shoaib and Afkar Abbas and M. Ijaz Khan and C.G. Jagannatha and Chetana Gali and M.Y. Malik and Mamdooh Alwetaishi},
keywords = {Artificial backpropagated Levenberg-Marquardt neural network (ABP-LMNN), MHD nanofluid, Activation energy, Slip effects, Entropy generation},
abstract = {In this paper, the main focus of this research is to represent an intelligent computing model through an artificial backpropagated Levenberg-Marquardt neural network (ABP-LMNN) for entropy optimized magnetohydrodynamic fully developed nanofluid flow with slip and activation energy effects. In mathematical modeling, dimensionless non-linear ODEs represent the magnetohydrodynamic nanofluid flow model (MHD-NFM). A reference dataset of ABP-LMNN is constructed for diverse situations of MHD-NFM by discrepancy of parameters. The attained reference dataset (RD) is randomly utilized for validation, testing and training processes for ABP-LMNN are employed to examine the approximate solution of MHD-NFM is demonstrated by comparison of outcomes. The authentic performance of the ABP-LMNN is validated through accuracy in the phrase of error histogram, mean square error and regression learning. The thermal and solutal parameters upsurge both the thermal and the concentration gradients. Moreover, the velocity profiles are declined owing to an increase in the second-order slip parameter in the tangential direction of the flow.}
}
@article{ISMAIL2021328,
title = {Machine Learning-based Energy-Aware Offloading in Edge-Cloud Vehicular Networks},
journal = {Procedia Computer Science},
volume = {191},
pages = {328-336},
year = {2021},
note = {The 18th International Conference on Mobile Systems and Pervasive Computing (MobiSPC), The 16th International Conference on Future Networks and Communications (FNC), The 11th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921014411},
author = {Leila Ismail and Huned Materwala},
keywords = {Cloud computing, Computation offloading, Edge computing, Energy-efficiency, Machine learning, Queuing theory, Support Vector Machine (SVM), Vehicular network},
abstract = {A vehicular network underpinned by the 3-tier vehicle-edge-cloud infrastructure enables an efficient and safer travel experience. The compute-intensive vehicular applications are often offloaded to the edge and/or cloud servers to enhance the applications’ Quality of Services (QoS). The underlying edge-cloud servers consume a high among of energy. Consequently, it becomes crucial to optimizing energy consumption in the offloading process. Current energy-efficient offloading strategies in 2-tier vehicle-edge infrastructure, do not account for cloud computing energy consumption. In this paper, we address this void by proposing a machine learning-based energy-aware offloading algorithm, which optimizes the energy of the edge-cloud computing platform. The offloading strategy is enabled by the Support Vector Machine (SVM) regression model machine learning algorithm used for the edge-cloud power prediction. The experimental results show that the proposed algorithm is a promising approach in energy savings.}
}
@article{MCGUIRE2023,
title = {Reduction of beam hardening artifact in photon-counting computed tomography: Using low-energy threshold polyenergetic reconstruction},
journal = {Journal of Cardiovascular Computed Tomography},
year = {2023},
issn = {1934-5925},
doi = {https://doi.org/10.1016/j.jcct.2023.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1934592523004136},
author = {Aaron M. McGuire and Carter D. Smith and Jordan H. Chamberlin and Dhruw Maisuria and Adrienn Tóth and U. Joseph Schoepf and Jim O'Doherty and Reginald F. Munden and Jeremy Burt and Dhiraj Baruah and Ismail M. Kabakus}
}
@article{BANYSALAMEH2022100714,
title = {Energy-aware spectrum coordination with intelligent frequency-hopping for software defined networks},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100714},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100714},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000518},
author = {Haythem {Bany Salameh} and Esraa {Al Jarrah} and Moayad Aloqaily and Ali Eyadeh},
keywords = {IoT, Dynamic spectrum access, Rendezvous, Spectrum coordination, Massive connectivity},
abstract = {Software-defined Cognitive radio (CR) networking is considered a promising communication paradigm that can deliver huge spectrum opportunities for enabling massive deployment for wireless Internet-of-Things (IoT) applications and services. An important challenge in this domain is how to provide effective energy-efficient distributed spectrum coordination mechanisms (known as Spectrum Rendezvous), through which the CR IoT users can opportunistically coordinate their transmissions over the idle licensed frequency channels without relying on the existence of a dedicated common control channel (CCC). This paper proposes a novel frequency hopping (FH) distributed rendezvous mechanism for CR-capable IoT (CR-IoT) networks using grid-based quorum systems, referred to as adaptive grid-based quorum channel-hopping scheme. The proposed quorum-based scheme divides the time into equidistant cycle segments, referred to as quorum-based FH intervals. Each quorum interval consists of equal-duration beaconing intervals, each associated with a licensed channel. According to the proposed algorithm, each CR-IoT device randomly selects awake-sleep schedules based on the adopted quorum system structure. Any two devices can exchange their control information over the assigned channels of their common awake beaconing intervals. The proposed algorithm provides probabilistic guarantees on the successful rendezvous (RDV) between any two CR-IoT users within a single quorum-based FH interval (cycle) and dynamically adjusts the adopted QS structure based on the time-varying traffic load in the CR-IoT network such that the RDV probability and the expected average time to rendezvous improve with the least possible energy consumption. Compared to reference design schemes, the simulation results demonstrate that the proposed algorithm significantly increases the RDV probability, reduces the average rendezvous time, and reduces the energy consumption per successful RDV.}
}
@article{LEWIS2022100011,
title = {MADplots: A methodology for visualizing and characterizing energy-dependent attenuation of tissues in spectral computed tomography},
journal = {Research in Diagnostic and Interventional Imaging},
volume = {2},
pages = {100011},
year = {2022},
issn = {2772-6525},
doi = {https://doi.org/10.1016/j.redii.2022.100011},
url = {https://www.sciencedirect.com/science/article/pii/S2772652522000114},
author = {Matthew A. Lewis and Todd C. Soesbe and Xinhui Duan and Liran Goshen and Yoad Yagil and Shlomo Gotman and Robert E. Lenkinski},
keywords = {Tomography, X-ray computed / methods, Radiographic image interpretation, Computer-assisted / methods/, 2D-histograms, Retrospective studies},
abstract = {Rationale and objectives
A method for visualizing and analyzing the complete information contained in spectral CT scans using two-dimensional histograms (i.e. Material Attenuation Decomposition plots – MADplots) of the water-photoelectric attenuation versus water-scatter attenuation at the cohort (combination of multiple studies across patients), examination, series, slice, and organ/ROI levels is described.
Materials and methods
The appearance of a MADplot with several standard biological materials was predicted using ideal material properties available from NIST and the ICRU to generate a map for this non-spatial data space. Software tools were developed to generate MADplots as new DICOM series that facilitate spectral analysis. Illustrative examples were selected from an IRB-approved, retrospective cohort of Spectral Basis Images (SBIs) scanned using a pre-release, dual-layer detector spectral CT.
Results
By combining all of the voxels for contrast and non-contrast studies, the predicted appearance of the MADplot was confirmed. Locations of several kinds of tissue, the shape of the tissue distributions in normal lung, and the variations in the manner in which organ-specific MADplots change with pathology are demonstrated for the presence of fat in both the liver and pancreas highlighting the potential use for identifying pathologies on spectral CT images.
Conclusions
The examples of MADplots shown at cohort (combined studies), examination, series, slice, organ, and ROI levels illustrate their potential utility in analyzing and displaying spectral CT data. Future studies are directed at developing MADplot based organ segmentation and the automated detection and display of organ based pathologies.}
}
@article{MATTHAIOU20221531,
title = {Dual-Energy Computed Tomography as an Adjunct in the Evaluation of Peripheral Chronic Total Occlusions: A Feasibility Study},
journal = {Journal of Vascular and Interventional Radiology},
volume = {33},
number = {12},
pages = {1531-1535},
year = {2022},
issn = {1051-0443},
doi = {https://doi.org/10.1016/j.jvir.2022.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1051044322011915},
author = {Nikolas Matthaiou and Nikolaos Galanakis and Antonios E. Papadakis and Elias Kehagias and Nikolaos Kontopodis and Stavros Charalambous and Konstantinos Perisinakis and Thomas G. Maris and Christos V. Ioannou and Dimitrios Tsetis},
abstract = {This study investigated the role of dual-energy computed tomography (CT) for lesion characterization in patients with peripheral arterial disease manifesting with chronic total occlusions (CTOs). Forty-one symptomatic patients with CTOs underwent dual-energy CT angiography before endovascular treatment. The lesions were subsequently analyzed in a dedicated workstation, and 2 indexes—dual-energy index (DEI) and effective Z (Zeff)—were calculated, ranging from 0.0027 to 0.321 and from 6.89 to 13.02, respectively. Statistical analysis showed a significant correlation between the DEI and Zeff values (P < .001). The interobserver intraclass correlation coefficient was 0.91 for the mean Zeff values and 0.86 for the mean DEI values. This technique could potentially provide useful information regarding the composition of a CTO.}
}
@article{AMINI2023S51,
title = {Abstract #1403692: Discrepancy Between Dual-Energy X-ray Absorptiometry and Quantitative Computed Tomography in Analyzing Bone Mineral Density in a Patient with Primary Hyperparathyroidism, Fractures and Degenerative Joint Disease - A Case Report and Review of Literature},
journal = {Endocrine Practice},
volume = {29},
number = {5, Supplement },
pages = {S51},
year = {2023},
note = {AACE Annual Meeting 2023 Abstracts},
issn = {1530-891X},
doi = {https://doi.org/10.1016/j.eprac.2023.03.117},
url = {https://www.sciencedirect.com/science/article/pii/S1530891X23001805},
author = {Masoud Amini and Tatiana Baron and Michael Brian Lim and Isaac Sachmechi}
}
@article{QU2022e10970,
title = {Research on energy saving of computer rooms in Chinese colleges and universities based on IoT and edge computing technology},
journal = {Heliyon},
volume = {8},
number = {10},
pages = {e10970},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10970},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022022587},
author = {Jia Qu},
keywords = {IoT, Edge device, Edge computing, Data analysis, Energy consumption monitoring},
abstract = {To solve the problems of low overall service quality of the university computer room, unstable environment control of the computer room, low adaptive adjustment ability, and high energy consumption. This article takes Chinese universities as an example to analyze university computer room supervision status, use the Internet of Things (IoT) to remotely and automatically monitor the computer room environment and energy consumption, and analyze the amount of data generated by the rapid increase of IoT edge devices. The method and model of edge computing in the computer room energy consumption monitoring system are proposed through research. The monitoring methods of critical parameters such as the computer room's thermal environment and energy consumption are given. Corresponding solutions for computer room management, testing, use, and energy-saving services are given. It provides a brand-new idea for energy saving in colleges and universities and network room security.}
}
@article{FORD2022100661,
title = {A cost effective framework for analyzing cross-platform software energy efficiency},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100661},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100661},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000063},
author = {Blake W. Ford and Ziliang Zong},
keywords = {Software energy efficiency, Power profiling, Dynamic program analysis, Usability, Portability},
abstract = {In recent history, most desktop software has been built for x86-based CPUs and developers rarely needed to consider cross-platform development. With ARM emerging as a promising architecture for both high performance and low power, we have reached a landmark for multi-architecture development. The most notable example at present is the introduction of Apple silicon (ARM-based CPU) and Apple’s substantial efforts to migrate software from previously supported Intel CPUs to ARM CPUs. Porting software requires significant work and it is difficult for developers to predict the performance and energy efficiency of software early on without testing it on the target hardware. Privileged access to prototype hardware and limited options for simulation before release further complicate the problem. In this paper, we propose a cost effective framework that allows developers to estimate the energy consumption of their software on a new platform without modifying their code and without direct access to target hardware. Specifically, our framework includes three modules: (1) The instruction prediction module uses a machine learning approach to automatically generate code for a target platform. (2) The energy estimation module leverages portable energy scores to calculate the energy consumption of this generated code. (3) The instruction profiling module analyzes the cross-platform code in an efficient way and produces evaluation reports. Our experiments conducted on the CoreMark® and LINPACK benchmarks have shown encouraging results in terms of program correctness and estimated energy consumption when porting software from x86 CPUs to ARM CPUs.}
}
@article{LI2021116089,
title = {Double-layer energy management system based on energy sharing cloud for virtual residential microgrid},
journal = {Applied Energy},
volume = {282},
pages = {116089},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116089},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920315154},
author = {Shenglin Li and Jizhong Zhu and Ziyu Chen and Tengyan Luo},
keywords = {Renewable energy sources, Energy management system, Energy sharing, Residential microgrid, Smart household},
abstract = {The idea of energy sharing can contribute to achieving the goal of resource optimization by redistributing and sharing idle energy assets. How to design an appropriate energy management strategy in the energy sharing environment has been the focus of intensive research in energy sharing field. In this paper, a new effective double-layer energy management system (EMS) based on the energy sharing cloud (ESC) is developed for a virtual residential microgrid (VRMG). The proposed ESC can be regarded as an open energy sharing environment, where the cloud platform helps cloud users build their VRMGs by providing energy services including renewable energy sources (RESs) generation and energy storage. The mathematical model of the VRMG is formulated, and the energy service prices for RESs generation and energy storage are monthly set separately. Moreover, considering the changes in household load and RESs generation, an energy management strategy of double-layer EMS is designed. The upper-layer EMS helps VRMG obtain the monthly optimal capacity configuration of RESs and energy storage, and the lower-layer EMS realizes the daily electricity scheduling optimization for the VRMG whose objectives are to minimize the total operational cost and maximize the electrical comfort level. Simulation studies demonstrate that the proposed energy sharing mechanism can meet the changing energy needs of the cloud user, and numerical experiments also confirm the performance and effectiveness of the proposed double-layer EMS.}
}
@article{HUFLAGE2022690,
title = {Metal artefact reduction in low-dose computed tomography: Benefits of tin prefiltration versus postprocessing of dual-energy datasets over conventional CT imaging},
journal = {Radiography},
volume = {28},
number = {3},
pages = {690-696},
year = {2022},
issn = {1078-8174},
doi = {https://doi.org/10.1016/j.radi.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1078817422000694},
author = {H. Huflage and J.-P. Grunz and C. Hackenbroch and D. Halt and K.S. Luetkens and A.M. {Alfred Schmidt} and T.S. Patzer and S. Ergün and T.A. Bley and A.S. Kunz},
keywords = {Spectral shaping, Dual-energy, Metal artefact reduction, Virtual monoenergetic imaging, Tin prefiltration},
abstract = {Introduction
The purpose of this study was to determine the potential for metal artefact reduction in low-dose multidetector CT as these pose a frequent challenge in clinical routine. Investigations focused on whether spectral shaping via tin prefiltration, virtual monoenergetic imaging or virtual blend imaging (VBI) offers superior image quality in comparison with conventional CT imaging.
Methods
Using a third-generation dual-source CT scanner, two cadaveric specimens with different metal implants (dental, cervical spine, hip, knee) were examined with acquisition protocols matched for radiation dose with regards to tube voltage and current. In order to allow for precise comparison, and due to the relatively short scan lengths, automatic tube current modulation was disabled. Specifically, the following scan protocals were examined: conventional CT protocols (100/120 kVp), tin prefiltration (Sn 100/Sn 150 kVp), VBI and virtual monoenergetic imaging (VME 100/120/150 keV). Mean attenuation and image noise were measured in hyperdense and hypodense artefacts, in artefact-impaired and artefact-free soft tissue. Subjective image quality was rated independently by three radiologists.
Results
Objectively, Sn 150 kVp allowed for the best reduction of hyperdense streak artefacts (p < 0.001), while VME 150 keV and Sn 150 kVp protocols facilitated equally good reduction of hypodense artefacts (p = 0.173). Artefact-impaired soft tissue attenuation was lowest in Sn 150 kVp protocols (p ≤ 0.011), whereas all VME showed significantly less image noise compared to conventional or tin-filtered protocols (p ≤ 0.001). Subjective assessment favoured Sn 150 kVp regarding hyperdense streak artefacts and delineation of cortical bone (p ≤ 0.005). The intraclass correlation coefficient was 0.776 (95% confidence interval: 0.712–0.831; p < 0.001) indicating good interrater reliability.
Conclusion
In the presence of metal implants in our cadaveric study, tin prefiltration with 150 kVp offers superior artefact reduction for low-dose CT imaging of osseous tissue compared with virtual monoenergetic images of dual-energy datasets. The delineation of cortical boundaries seems to benefit particularly from spectral shaping.
Implications for practice
Low-dose CT imaging of osseous tissue in combination with tin prefiltration allows for superior metal artefact reduction when compared to virtual monoenergetic images of dual-energy datasets. Employing this technique ought to be considered in daily routine when metal implants are present within the scan volume as findings suggest it allows for radiation dose reduction and facilitates diagnosis relevant to further treatment.}
}
@article{TROJAN20221055,
title = {A new software program for monitoring the energy distribution in a thermal waste treatment plant system},
journal = {Renewable Energy},
volume = {184},
pages = {1055-1073},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0960148121017328},
author = {Marcin Trojan and Jan Taler and Krzysztof Smaza and Wojciech Wróbel and Piotr Dzierwa and Dawid Taler and Karol Kaczmarski},
keywords = {Thermal waste treatment plant, Online monitoring, Computer system, Calorific value of the waste, Mathematical modeling},
abstract = {The paper presents a novel system for monitoring the operation of a thermal waste treatment plant. The computer program is based on measurements of pressure, temperature, and mass flow rate of the working medium. The boiler efficiency is calculated using an indirect method. Thermal online calculations of the boiler combustion chamber make it possible to determine the heat flow rate to the evaporator. Based on the energy balance of the boiler evaporator, the superheated steam mass flow rate is determined, taking into account the mass flow rates of water injected into steam attemperators. By comparing the calculated and measured value of steam mass flow rate, the calorific value of the fuel fed is determined. The developed system enables monitoring of energy flow rates in individual elements of the power unit, taking into account losses in the boiler and turbine condenser. The following parameters are determined online, the efficiency of the boiler and the entire incinerator, heat losses in the boiler and the turbine condenser, as well as the heat flow rate, supplied to the municipal district heating system. By monitoring the plant operation, it is possible to adjust online the optimum excess air ratio.}
}
@article{BENBLIDIA2021102,
title = {A renewable energy-aware power allocation for cloud data centers: A game theory approach},
journal = {Computer Communications},
volume = {179},
pages = {102-111},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002954},
author = {Mohammed Anis Benblidia and Bouziane Brik and Moez Esseghir and Leila Merghem-Boulahia},
keywords = {Cloud data centers, Green networking, Game theory, Renewable energy, Smart grid, Power dispatching},
abstract = {With the rapid emerging of Internet of Things (IoT) devices and the proliferation of cloud-based applications, the cloud computing industry is becoming a vital element for ensuring our daily services. However, cloud computing uses large scale data centers equipped with energy-hungry servers and huge power facilities that massively consume power. This presents a real challenge which can negatively influence the power grid, while exposing the environment to global warming issues. Therefore, minimizing cloud data center power consumption is a challenging problem and has to be addressed. In this paper, we look at renewable energy in the context of a smart grid–cloud architecture and investigate the issue of grid power dispatching to cloud data centers. Since cloud data centers have a non-cooperative nature regarding power demand from the power stations, we model our power allocation problem as a non-cooperative game. Afterwards, we prove the existence and the uniqueness of Nash equilibrium. Moreover, we formulate the payoff function of our game as a non-linear optimization problem before resolving it using Lagrange multipliers and Karush–Kuhn–Tucker (KKT) conditions. Thus, we determine the assigned optimal quantity to each data center based on three main criteria : renewable energy usage, number of critical running applications and workload charge. Extensive simulations are performed by comparing our scheme with an existing work. Results show that our scheme outperforms the comparing approach with a percentage of 31.2% in terms of power load rate and significantly reduces emissions of carbon dioxide.}
}
@article{QIN2021120569,
title = {Comprehensive evaluation of regional energy internet using a fuzzy analytic hierarchy process based on cloud model: A case in China},
journal = {Energy},
volume = {228},
pages = {120569},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.120569},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221008185},
author = {Guangyu Qin and Meijuan Zhang and Qingyou Yan and Chuanbo Xu and Daniel M. Kammen},
keywords = {Regional energy internet, Comprehensive evaluation, Fuzzy analytic hierarchy process, Cloud theory},
abstract = {Regional energy internet is an important application of “Internet+” smart energy in China’s new urbanization process. The evaluation of regional energy internet is a significant way to measure the development level of regional energy internet. However, such work is a blank of current research. Therefore, the purpose of this study is to evaluate regional energy network from a comprehensive view. Firstly, this study establishes an evaluation criteria system for regional energy internet, which consists of four dimensions (technical, economic, social, and engineering) and associated 16 criteria. Secondly, considering the uncertainty involved the evaluation process, cloud model is utilized to depict the fuzziness and randomness, which are indispensable elements in the uncertainty. Thirdly, a fuzzy analytic hierarchy process, which is a combination of AHP and fuzzy set theory, is developed to determine the weights of the identified dimensions and criteria. Finally, the evaluation results in practical case in China show that the comprehensive evaluation grade of the demonstration project is good. And the technical dimension and economic dimension should be noted, mainly involving comprehensive efficiency and energy economy level. The results of the evaluation are exactly consistent with the actual situation, indicating that the evaluation method has strong practical value.}
}
@article{MANIKANDAN2022100784,
title = {Soft computing technique with maintenance and controlling for distributed energy using differential evolution based local power distribution system and fuzzy radial basis function neural network},
journal = {Sustainable Computing: Informatics and Systems},
volume = {36},
pages = {100784},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100784},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922001159},
author = {N. Manikandan and Prameeladevi Chillakuru and R. Suresh Kumar and Sachi Nandan Mohanty and Roobaea Alroobaea and Saeed Rubaiee and Abdulkader S. Hanbazazah},
keywords = {Soft computing, Energy conversion systems, Distributed energy system, Maintenance, Controlling},
abstract = {Learning-based modelling methods are now being used to construct a precise prediction method for renewable energy sources. CI (Computational Intelligence) methods are proven to be effective in developing and optimising renewable instruments. Difficulty of this type of energy is determined by its coverage of enormous amounts of data as well as factors that must be properly analyzed. Soft computing approaches are widely recognized as critical instruments for improving performance of spinning electrical devices in both controls as well as design. The progress of gentle computing methods employed in rotating electrical machines is crucial for a wide range of energy conversion devices, including generators, high-performance electric engines and electric cars.This research proposes a novel technique for maintaining and controlling distributed energy systems. Here the maintenance and controlling data have been evaluated by differential evolution based local power distribution system (DE-LPDS) for control system as well as fuzzy radial basis function neural network (FRBFNN) is used in Micro-grid energy management system. The suggested energy management system is modelled to coordinate among many flexible sources by establishing priority resources, direct demand control signals, and power prices. Experimental results show various maintenance and controlling datasets in terms of QoS of 82%, energy efficiency of 95%, power consumption of 59 %, computational time of 34 ms and training accuracy of 95 % by proposed technique.}
}
@article{GROSU2022731,
title = {Hepatobiliary Dual-Energy Computed Tomography},
journal = {Radiologic Clinics of North America},
volume = {60},
number = {5},
pages = {731-743},
year = {2022},
note = {Hepatobiliary Imaging},
issn = {0033-8389},
doi = {https://doi.org/10.1016/j.rcl.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0033838922000665},
author = {Sergio Grosu and Benjamin M. Yeh},
keywords = {Computed tomography, X-Ray, DECT, Humans, Abdomen, Liver, Gallstones, Bile duct}
}
@article{MOHAMMADHASANIZADE2021114915,
title = {SAEA: A security-aware and energy-aware task scheduling strategy by Parallel Squirrel Search Algorithm in cloud environment},
journal = {Expert Systems with Applications},
volume = {176},
pages = {114915},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114915},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421003560},
author = {Behnam {Mohammad Hasani Zade} and Najme Mansouri and Mohammad Masoud Javidi},
keywords = {Cloud, Task scheduling, Fuzzy system, Squirrel Search Algorithm, Makespan},
abstract = {The rapid growth of networking technologies resulted in the execution of an extensive data-centric task, which needs the critical quality of service by cloud data centers. The task scheduling problem is difficult to attain an optimal solution, so we use the Squirrel Search Algorithm to approximate the optimal solution. Traditional scheduling algorithms attempt to reduce execution time without taking into account the energetic cost and security issues. In this scheme, a fuzzy-based task scheduling (SAEA) algorithm is developed which closely combines energy cost, makespan, degree of imbalance, and security levels for multi-objective optimization scheduling problems. In addition, SAEA tries to find a high-quality knowledge base that accurately describes the fuzzy system by parallel squirrels search algorithm (PSSA). The automatic design of a fuzzy rule-based system is currently attracting the interest due to the inherently dynamic nature and the typical complex search spaces of cloud. Extensive experiments prove that SAEA algorithm obtains superior performances in energy cost around 45% compared with MGA and has a better result in terms of total execution time, makespan, degree of imbalance, and security value than other similar scheduling algorithms under high load condition.}
}
@article{HAN2022101123,
title = {A hybrid granular-evolutionary computing method for cooperative scheduling optimization on integrated energy system in steel industry},
journal = {Swarm and Evolutionary Computation},
volume = {73},
pages = {101123},
year = {2022},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2022.101123},
url = {https://www.sciencedirect.com/science/article/pii/S2210650222000931},
author = {Zhongyang Han and Xinyu Zhang and Hongqi Zhang and Jun Zhao and Wei Wang},
keywords = {Integrated energy system, Granular computing, CMOP, Cooperative scheduling optimization},
abstract = {Integrated Energy System (IES) in steel industry, typically involving gas, heat and electricity, exhibits distributed nature and complex coupling relationship, which demands effective scheduling optimization to cooperatively utilize the multiple energy media. For this purpose, a Granular-Evolutionary Hybrid Computing (GEHC) framework is proposed in this study to solve the scheduling of IES as a Constrained Multi-Objective Problem (CMOP). More specifically, a Collaborative Fuzzy C-Means (CFCM) clustering model is firstly established, where the horizontal and vertical structures are well constructed to obtain information granules regarding the cooperative relationship among the units in different energy subsystems. Then, a Probability-based Granular Computing (PGrC) approach is proposed for the initial optimization of the IES scheduling, which is guided by the expert knowledge compacted in the information granules. To determine the collaborative parameters in CFCM that essentially controls the cooperation among the multiple energy media, as well as satisfying the complex practical constraints, a Multi-Objective Differential Evolution (MODE)-based method is proposed for further scheduling optimization. To evaluate the performance on cooperativity, convergency and diversity, the proposed GEHC and the compared methods that either has partial collaborations or use other optimization frameworks are applied to a real-world optimization problem. The experimental results demonstrate that the proposed GEHC performs superior to the other methods considering both the objective functions and performance indicators, which can be potentially helpful to achieve well-performed unmanned IES scheduling in steel industry.}
}
@article{PETERS202234,
title = {In vivo assessment of tissue-specific radiological parameters with intra- and inter-patient variation using dual-energy computed tomography},
journal = {Radiotherapy and Oncology},
volume = {175},
pages = {34-41},
year = {2022},
issn = {0167-8140},
doi = {https://doi.org/10.1016/j.radonc.2022.07.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167814022042190},
author = {Nils Peters and Aaron Kieslich and Patrick Wohlfahrt and Christian Hofmann and Christian Richter},
keywords = {Proton therapy, Dual-energy computed tomography, DECT, Tissue parameters},
abstract = {Purpose/objective
Experimental in vivo determination of radiological tissue parameters of organs in the head and pelvis within a large patient cohort, expanding on the current standard human tissue database summarized in ICRU46.
Material/methods
Relative electron density (RED), effective atomic number (EAN) and stopping-power ratio (SPR) were obtained from clinical dual-energy CT scans using a clinically validated DirectSPR implementation and organ segmentations of 107 brain-tumor (brain, brainstem, spinal cord, chiasm, optical nerve, lens) and 120 pelvic cancer patients (prostate, kidney, liver, bladder). The impact of contamination by surrounding tissues on the tissue parameters was reduced with a dedicated contour adaption routine. Tissue parameters were characterized regarding the cohort mean value as well as the variation within each patient (2σintra) and between patients (2σinter). For the brain, age-dependent differences were determined.
Results
For 10 organs, including 4 structures not listed in ICRU46, the mean RED, EAN and SPR as well as their respective intra- and inter-patient variation were determined. SPR intra-patient variation was higher than 1.3% (1.3–4.6%) in all organs and always exceeded the inter-patient variation of the organ mean SPR (0.6–2.1%). For the brain, a significant SPR variation between pediatric and non-pediatric patients was determined.
Conclusion
Radiological tissue parameters in the head and pelvis were characterized in vivo for a large patient cohort using dual-energy CT. This reassesses parts of the current standard database defined in ICRU46, furthermore complementing the data described in literature by smaller substructures in the brain as well as by the quantification of organ-specific inter- and intra-patient variation.}
}
@article{JONES2021167,
title = {A multi-energy system optimisation software for advanced process control using hypernetworks and a micro-service architecture},
journal = {Energy Reports},
volume = {7},
pages = {167-175},
year = {2021},
note = {The 17th International Symposium on District Heating and Cooling},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.08.159},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721007629},
author = {Sean Jones and Richard Charlesworth and Kevin Naik and Thomas Charlesworth and Edward O’Dwyer and Anton Ianakiev and Jeff Johnson and Rabah Boukhanouf and Mark Gillott and Victor Sellwood and Joy Aloor},
keywords = {Multi-energy systems, Energy optimisation, Predictive algorithm, Hypernetwork theory, Microservice architecture},
abstract = {This paper describes a multi-energy system optimisation software, “Sustainable Energy Management System” (SEMS), developed as part of a Siemens, Greater London Authority and Royal Borough of Greenwich partnership in collaboration with the University of Nottingham, Nottingham Trent University and Imperial College London. The software was developed for application at a social housing estate in Greenwich, London, as part of the Borough’s efforts to retrofit the energy systems and building fabric of its housing stock. Its purpose is to balance energy across vectors and networks through day-ahead forecasting and optimisations that can be interpreted as control outputs for energy plant such as a water source heat pump, district heating pumps and values, power switchgear, gas boilers, a thermal store, electric vehicle chargers and a photovoltaic array. The optimisation objectives are to minimise greenhouse gas emissions and operational cost. The tool uses Hypernetwork Theory based orchestration coupled with a microservice architecture. The distributed nature of the design ensures flexibility and scalability. Currently, microservices have been programmed to forecast domestic heating demand, domestic electricity demand, electric vehicle demand, solar photovoltaic generation, ground temperature, and to run a day-ahead energy balance optimisation. This paper presents the results from both domestic heat and electricity demand forecasting, as well as the overall design and integration of the software with a physical system. The works build on that of O’Dwyer, et al. (2020) who developed a preliminary energy management software and digital twin. Their work acts as a foundation for this real-world commercialisation-ready program that integrates with physical assets.}
}
@article{PRASAD2021297,
title = {Investigation of particle density on dust cloud dynamics in a minimum ignition energy apparatus using digital in-line holography},
journal = {Powder Technology},
volume = {384},
pages = {297-303},
year = {2021},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2021.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0032591021001297},
author = {Shrey Prasad and Christian Schweizer and Pranav Bagaria and Ankit Saini and Waruna D. Kulatilaka and Chad V. Mashuga},
keywords = {Particle density, Dust cloud dynamics, Minimum ignition energy, Kühner MIKE3, Dust explosion},
abstract = {Solids industries have inherent dust explosion hazards. Process risk assessment parameters include overpressure, deflagration index, and minimum ignition energy. These parameters are influenced by dust dynamics, such as turbulence and concentration. Cloud dynamics are affected by particle morphology, density, size, and polydispersity, which affect explosion parameters. Influence of properties such as density during explosion parameter measurement is often overlooked. This study examines particle density influence on dust clouds in the Kühner MIKE3. Two clouds of particles with different density, but similar particle size and morphology were examined with high-speed digital in-line holography, providing velocity and concentration measurements prior to ignition. Results show velocity and concentration of denser dust is lower compared to lighter dust after ignition delay. The results highlight the density effect on cloud dynamics and make a case for particle density to be considered in setting ignition delay in order to get consistent cloud dynamics and test results.}
}
@article{DELLOGLIO2020865,
title = {Hybrid Indocyanine Green–99mTc-nanocolloid for Single-photon Emission Computed Tomography and Combined Radio- and Fluorescence-guided Sentinel Node Biopsy in Penile Cancer: Results of 740 Inguinal Basins Assessed at a Single Institution},
journal = {European Urology},
volume = {78},
number = {6},
pages = {865-872},
year = {2020},
issn = {0302-2838},
doi = {https://doi.org/10.1016/j.eururo.2020.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0302283820306989},
author = {Paolo Dell’Oglio and Hielke M. {de Vries} and Elio Mazzone and Gijs H. KleinJan and Maarten L. Donswijk and Henk G. {van der Poel} and Simon Horenblas and Fijs W.B. {van Leeuwen} and Oscar R. Brouwer},
keywords = {Sentinel node, Indocyanine green, Fluorescence, Blue dye, Penile cancer},
abstract = {Background
Sentinel node (SN) biopsy in penile cancer (PeCa) is typically performed using 99mTc-nanocolloid and blue dye. Recent reports suggested that the hybrid (radioactive and fluorescent) tracer indocyanine green (ICG)-99mTc-nanocolloid may improve intraoperative optical SN identification.
Objective
The current study aimed to confirm the reliability of ICG-99mTc-nanocolloid and to assess whether blue dye is still of added value.
Design, setting, and participants
A total of 400 ≥T1G2N0 PeCa patients were staged with SN biopsy at a single European centre. SNs were preoperatively identified with lymphoscintigraphy and single-photon emission computed tomography. Intraoperatively, SNs were detected via gamma tracing, blue staining, and fluorescence imaging.
Outcome measurements and statistical analysis
All patients (n=400, 740 groins) received ICG-99mTc-nanocolloid. Intraoperative SN identification rates were retrospectively evaluated. In those patients who received ICG-99mTc-nanocolloid and blue dye (n=266, 492 groins), SN visualisation rates were compared using the McNemar test.
Results and limitations
In total, 740 groins were assessed. No tracer-related (allergic) reactions were reported. All preoperatively defined SNs (n=1163) were localised intraoperatively. Of all excised SNs, 98% were detectable with gamma probe and 96% were visible with fluorescence imaging. In the analysis of the patients who received ICG-99mTc-nanocolloid and blue dye, fluorescence imaging yielded a 39% higher SN detection rate than blue dye (95% confidence interval 36–43%, p<0.001). Of the SNs that were tumour positive, 100% were intraoperatively visualised by fluorescence imaging, whereas merely 84% of the positive nodes stained blue.
Conclusions
This study confirms that ICG-99mTc-nanocolloid is a reliable SN tracer for PeCa that significantly improves optical SN detection over blue dye.
Patient summary
Hybrid indocyanine green (ICG)-99mTc-nanocolloid is a safe and reliable sentinel node (SN) tracer, as established in this large series of 400 penile cancer patients (740 groins). It enables accurate pre- and intraoperative SN identification and significantly improves SN detection rate compared with blue dye, without staining the surgical field or the need for an additional injection.}
}
@article{LI2021116977,
title = {Cloud-based health-conscious energy management of hybrid battery systems in electric vehicles with deep reinforcement learning},
journal = {Applied Energy},
volume = {293},
pages = {116977},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116977},
url = {https://www.sciencedirect.com/science/article/pii/S0306261921004499},
author = {Weihan Li and Han Cui and Thomas Nemeth and Jonathan Jansen and Cem Ünlübayir and Zhongbao Wei and Xuning Feng and Xuebing Han and Minggao Ouyang and Haifeng Dai and Xuezhe Wei and Dirk Uwe Sauer},
keywords = {Energy management, Vehicle-to-cloud, Reinforcement learning, Battery aging, Lithium-ion, Battery safety},
abstract = {In order to fulfill the energy and power demand of battery electric vehicles, a hybrid battery system with a high-energy and a high-power battery pack can be implemented as the energy source. This paper explores a cloud-based multi-objective energy management strategy for the hybrid architecture with a deep deterministic policy gradient, which increases the electrical and thermal safety, and meanwhile minimizes the system’s energy loss and aging cost. In order to simulate the electro-thermal dynamics and aging behaviors of the batteries, models are built for both high-energy and high-power cells based on the characterization and aging tests. A cloud-based training approach is proposed for energy management with real-world vehicle data collected from various road conditions. Results show the improvement of electrical and thermal safety, as well as the reduction of energy loss and aging cost of the whole system with the proposed strategy based on the collected real-world driving data. Furthermore, processor-in-the-loop tests verify that the proposed strategy can achieve a much higher convergence rate and a better performance in terms of the minimization of both energy loss and aging cost compared with state-of-the-art learning-based strategies.}
}
@article{CHULUUNBAATAR2022108397,
title = {KANTBP 3.1: A program for computing energy levels, reflection and transmission matrices, and corresponding wave functions in the coupled-channel and adiabatic approaches},
journal = {Computer Physics Communications},
volume = {278},
pages = {108397},
year = {2022},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2022.108397},
url = {https://www.sciencedirect.com/science/article/pii/S0010465522001163},
author = {O. Chuluunbaatar and A.A. Gusev and S.I. Vinitsky and A.G. Abrashkevich and P.W. Wen and C.J. Lin},
keywords = {Eigenvalue and multichannel scattering problems, Kantorovich method, Finite element method, Multichannel adiabatic approximation, Ordinary differential equations, High-order accuracy approximations},
abstract = {A FORTRAN program for calculating energy values, reflection and transmission matrices, and corresponding wave functions in a coupled-channel approximation of the adiabatic approach is presented. In this approach, a multidimensional Schrödinger equation is reduced to a system of the coupled second-order ordinary differential equations on a finite interval with the homogeneous boundary conditions of the third type at left- and right-boundary points for the discrete spectrum and scattering problems. The resulting system of such equations, containing potential matrix elements and first-derivative coupling terms is solved using high-order accuracy approximations of the finite element method. The scattering problem is solved with non-diagonal potential matrix elements in the left and/or right asymptotic regions and different left and right threshold values. Benchmark calculations for the fusion cross sections of 36S+48Ca, 64Ni+100Mo reactions are presented. As a test desk, the program is applied to the calculation of the reflection and transmission matrices and corresponding wave functions of the exact solvable wave-guide model, and also the fusion cross sections and mean angular momenta of the 16O+144Sm reaction.
Program summary
Program Title: KANTBP CPC Library link to program files: https://doi.org/10.17632/4vm9fhyvh3.1 Licensing provisions: CC BY NC 3.0 Programming language: FORTRAN Nature of problem: In the adiabatic approach [1], a multidimensional Schrödinger equation for quantum reflection [2], the photoionization and recombination of a hydrogen atom in a homogeneous magnetic field [3–6], the three-dimensional tunneling of a diatomic molecule incident upon a potential barrier [7], wave-guide models [8], the fusion model of the collision of heavy ions [9–11], and low-energy fusion reactions of light- and medium mass nuclei [12] is reduced by separating the longitudinal coordinate, labeled as z, from transversal variables to a system of second-order ordinary differential equations containing the potential matrix elements and first-derivative coupling terms. The purpose of this paper is to present a program based on the use of high-order accuracy approximations of the finite element method (FEM) for calculating energy levels, reflection and transmission matrices and wave functions for such systems of coupled-channel second order differential equations (CCSODEs) on finite intervals of the variable z∈[zmin,zmax] with homogeneous boundary conditions of the third-type at the left- and right-boundary points, which follow from the discrete spectrum and scattering problems. Solution method: The boundary-value problems for the system of CCSODEs are solved by the FEM using high-order accuracy approximations [13,14]. The generalized algebraic eigenvalue problem AF=EBF with respect to pair unknowns (E,F), arising after the replacement of the differential eigenvalue problem by the finite-element approximation, is solved by the subspace iteration method [14]. The generalized algebraic eigenvalue problem of a special form (A−EB)F=DF with respect to pair unknowns (D,F) arising after the corresponding replacement of the scattering boundary problem in open channels at fixed energy value, E, is solved by the L DLT factorization of the symmetric matrix and back-substitution methods [14]. Additional comments including restrictions and unusual features: The user must supply subroutine POTCAL for evaluating potential matrix elements. The user should also supply subroutines ASYMEV (when solving the eigenvalue problem) or ASYMSL and ASYMSR (when solving the scattering problem) which evaluate asymptotics of the wave functions at boundary points in the case of a boundary conditions of the third-type for the above problems.
References
[1]M. Born, Festschrift Goett. Nach. Math. Phys. K1 (1951) 1–6.[2]H. Friedrich, Theoretical Atomic Physics, 3rd ed., Springer, Berlin, 2006.[3]A. Alijah, J. Hinze, J.T. Broad, J. Phys. B 23 (1990) 45–60.[4]O. Chuluunbaatar, A.A. Gusev, V.L. Derbov, M.S. Kaschiev, L.A. Melnikov, V.V. Serov, and S.I. Vinitsky, J. Phys. A 40 (2007) 11485–11524.[5]O. Chuluunbaatar, A.A. Gusev, S.I. Vinitsky, V.L. Derbov, L.A. Melnikov, V.V. Serov, Phys. Rev. A 77 (2008) 034702.[6]O. Chuluunbaatar, A.A. Gusev, V.P. Gerdt, V.A. Rostovtsev, S.I. Vinitsky, A.G. Abrashkevich, M.S. Kaschiev, V.V. Serov, Comput. Phys. Commun. 178 (2008) 301–330.[7]G.L. Goodvin, M.R.A. Shegelski, Phys. Rev. A 72 (2005) 042713.[8]G. Chuluunbaatar, A.A. Gusev, O. Chuluunbaatar, S.I. Vinitsky, L.L. Hai, EPJ Web Conf. 226 (2020) 02008.[9]H.J. Krappe, K. Moehring, M.C. Nemes, H. Rossner, Z. Phys. A. 314 (1983) 23–31.[10]T. Ichikawa, K. Hagino, and A. Iwamoto, Phys. Rev. C 75 (2007) 064612.[11]C.L. Jiang, B.B. Back, K.E. Rehm, K. Hagino, G. Montagnoli, A.M. Stefanini, Eur. Phys. J. A 57 (2021) 235.[12]V.V. Sargsyan, G.G. Adamian, N.V. Antonenko, H. Lenske, Eur. Phys. J. A 56 (2020) 19.[13]O. Chuluunbaatar, A.A. Gusev, A.G. Abrashkevich, A. Amaya-Tapia, M.S. Kaschiev, S.Y. Larsen, S.I. Vinitsky, Comput. Phys. Commun. 177 (2007) 649–675.[14]K.J. Bathe, Finite Element Procedures in Engineering Analysis, Englewood Cliffs, Prentice Hall, New York, 1982.}
}
@article{OPREA2022108272,
title = {A signaling game-optimization algorithm for residential energy communities implemented at the edge-computing side},
journal = {Computers & Industrial Engineering},
volume = {169},
pages = {108272},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108272},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003394},
author = {Simona-Vasilica Oprea and Adela Bâra},
keywords = {Signaling game, Optimization, Smart metering system, Advanced tariff, Edge-computing},
abstract = {The ongoing tremendous progress of IT&C technologies, large-scale integration of smart metering systems, along with smart appliances and energy management systems allow the implementation of various advanced tariff scheme such as: Time-of-Use (ToU), critical peak pricing, real-time tariff (RTT), etc. These new features enhance suppliers to interact with consumers and provide incentives for peak shaving. Thus, the electricity consumers schedule the operation of their appliances to minimize the electricity cost. Game theory is applied to various economic processes, including electricity consumption optimization, leading to significant savings. Hence, in this paper, we propose a Signaling Game model for Optimization (SGO), that is performed at the Edge-computing node and involves multiple optimization steps. An advanced tariff signal is iteratively computed based on the consumption level. The game-optimizing process stops when there is no further improvement of the players’ reward. The simulations using advanced tariffs and a one-year dataset of a small residential community of several houses reveal excellent results in terms of savings, gains, Flat Index (FI) and Peak Average to Ratio (PAR). The gain of the community is about 11% and the savings are approx. 3,357 Euro per year. Compared to the results with a ToU tariff, the results with advanced tariffs (RTT) are significantly improved: FI increased from 0.68 to 0.76 and PAR decreased from 2.3 to 1.86.}
}
@article{GECK2020163,
title = {Computing Green functions in small characteristic},
journal = {Journal of Algebra},
volume = {561},
pages = {163-199},
year = {2020},
note = {Special issue in memory of Kay Magaard},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2019.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S0021869320300144},
author = {Meinolf Geck},
keywords = {Finite groups of Lie type, Green functions, Character sheaves},
abstract = {Let G(q) be a finite group of Lie type over a field with q elements, where q is a prime power. The Green functions of G(q), as defined by Deligne and Lusztig, are known in almost all cases by work of Beynon–Spaltenstein, Lusztig und Shoji. Open cases exist for groups of exceptional type E62, E7, E8 in small characteristics. We propose a general method for dealing with these cases, which proceeds by a reduction to the case where q is a prime and then uses computer algebra techniques. In this way, all open cases in type E62, E7 are solved, as well as at least one particular open case in type E8.}
}
@article{LIU202384,
title = {Intelligent energy-efficient scheduling with ant colony techniques for heterogeneous edge computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {172},
pages = {84-96},
year = {2023},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522002131},
author = {Jing Liu and Pei Yang and Cen Chen},
keywords = {Ant-colony, DVFS, Energy-efficient, Deadline, Heterogeneous edge computing},
abstract = {Energy efficiency is a significant issue in heterogeneous edge computing systems for a large number of latency-sensitive applications. This article presents an efficient technique to minimize energy overhead of time-constrained applications modeled by DAGs in heterogeneous edge computing. The technique is divided into three stages. First, we design a new method to compute task priority and propose the ant-colony based energy-aware scheduling algorithm to get a preliminary scheduling result. Second, taking the slack time between tasks and their deadlines into consideration, we propose the downward proportionally reclaiming slack algorithm to further cut down energy overhead by the DVFS technique. Third, taking the slack time between tasks into consideration, we propose the upward and downward proportionally reclaiming slack algorithm to cut down energy overhead by the DVFS technique again. Simulated results indicate that the presented technique is highly efficient in reducing energy overhead compared with state-of-the-art techniques using benchmarks of distinct characteristics.}
}
@article{MOGHADDAM2021142,
title = {Metrics for improving the management of Cloud environments — Load balancing using measures of Quality of Service, Service Level Agreement Violations and energy consumption},
journal = {Future Generation Computer Systems},
volume = {123},
pages = {142-155},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2100131X},
author = {Seyedhamid Mashhadi Moghaddam and Michael O’Sullivan and Charles Peter Unsworth and Sareh Fotuhi Piraghaj and Cameron Walker},
keywords = {Cloud computing, Cloud metrics, Cloud QoS, Energy consumption, Cloud SLAVs},
abstract = {Cloud service providers use load balancing algorithms in order to avoid Service Level Agreement Violations (SLAVs) and wasted energy consumption due to host over- and under-utilization, respectively. Load balancing algorithms migrate VMs between hosts in order to balance host loads. Any Virtual Machines (VMs) that are migrated experience performance degradation which results in lower Quality of Service (QoS) and can possibly result in SLAVs. Hence, an optimal load balancing method should reduce the number of over- and under-utilized hosts with a minimal number of VM migrations. One of the metrics used previously in the literature for evaluating load balancing stated that it equally considered SLAVs caused by both over-utilized hosts and migrations. However, in this paper, we show that, in fact, this metric values keeping the number of migrations low at the expense of an increased number of over-utilized hosts. This disparity is demonstrated by simulation of Google, PlanetLab and Azure data sets in CloudSim. This metric may suit public cloud providers which are focused on minimizing SLAVs and keeping energy costs low, but does not consider the QoS of customer VMs. We propose an alternative metric that considers QoS for the VMs. This alternative metric considers not only performance loss during migration, but also performance degradation due to host over-utilization. Private cloud providers, e.g., IT services within large organizations, often value the performance of their “customer” VMs, i.e., the QoS their organization receives, as well as traditional cloud provider costs, i.e., energy and SLAV costs. Hence, our alternative metric would be more appropriate in these scenarios. We compare and contrast load balancing methods using both the existing, biased metric and our new alternative metric.}
}
@article{MENALE20223179,
title = {Persistent respiratory failure after SARS-CoV-2 infection: The role of dual energy computed tomography. A case report},
journal = {Radiology Case Reports},
volume = {17},
number = {9},
pages = {3179-3184},
year = {2022},
issn = {1930-0433},
doi = {https://doi.org/10.1016/j.radcr.2022.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S1930043322003818},
author = {Silvia Menale and Valentina Scheggi and Jacopo Giovacchini and Niccolò Marchionni},
keywords = {Persistent respiratory failure, SARS-COV-2, Pulmonary microembolism, Dual energy CT (DECT), Case report},
abstract = {Background: COVID-19 disease is often complicated by respiratory failure, developing through multiple pathophysiological mechanisms, with pulmonary embolism (PE) and microvascular thrombosis as key and frequent components. Newer imaging modalities such as dual-energy computed tomography (DECT) can represent a turning point in the diagnosis and follow-up of suspected PE during COVID-19. Case presentation: A 78-year-old female presented to our internal medicine 3 weeks after initial hospitalization for COVID-19 disease, for recrudescent respiratory failure needing oxygen therapy. A computed tomography (CT) lungs scan showed a typical SARSCoV-2 pneumonia. Over the following 15 days, respiratory function gradually improved. Unexpectedly, after 21 days from symptom onset, the patient started complaining of breath shortening with remarkable desaturation requiring high-flow oxygen ventilation. CT pulmonary angiography and transthoracic echocardiography were negative for signs of PE. Thereby, Dual-energy CT angiography of the lungs (DECT) was performed and detected diffuse peripheral microembolism. After 2 weeks, a second DECT was performed, showing a good response to the anticoagulation regimen, with reduced extent of microembolism and some of the remaining emboli partially recanalized. Discussion: DECT is an emerging diagnostic technique providing both functional and anatomical information. DECT has been reported to produce a much sharper delineation of perfusion defects than pulmonary scintigraphy, using a significantly lower equivalent dose of mSv. We highlight that DECT is particularly useful in SARS-Cov-2 infection, in order to determine the predominant underlying pathophysiology, particularly when respiratory failure prolongs despite improved lung parenchymal radiological findings}
}
@article{SUDHA20213137,
title = {Energy-aware parameter tuning mechanism for workflow scheduling in the cloud environment},
journal = {Materials Today: Proceedings},
volume = {45},
pages = {3137-3142},
year = {2021},
note = {International Conference on Advances in Materials Research - 2019},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.12.219},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320399041},
author = {Danthuluri Sudha and Sanjay Chitnis},
keywords = {Workflow scheduling, Energy consumption, EAPT, Parameter tuning, Scientific workflow},
abstract = {Cloud Computing is one of the successful computing paradigms and it has contributed to the huge growth of the IT industry as well as individual usability of computing resource due to its ease of accessing resource properties. Moreover, cloud computing emergence has another advantage for deployment in a large scientific workflow. The scientific workflow defines a computation series which enables data analysis in a distributed and structured manner; since this workflow possesses a huge amount of task, energy consumption is a primary issue. In this research work, we have designed an efficient mechanism named EAPT (Energy-Aware Parameter tuning) to minimize energy consumption; in here two eminent parameter such as computation parameter, communication parameter and task size are tuned for optimizing these parameters resulting in minimization in energy. Further EAPT achieves the energy minimization through balancing the load dynamically, through tuning the above discussed parameter. EAPT tunes this in efficient way such that load are distributed based upon resources. EAPT is evaluated by considering the energy consumption as a parameter and varying the number of VM as 20, 40 and 60; moreover, a comparative analysis is carried out with the existing DVFS model. Comparative analysis shows that EAPT consumes marginally less energy than the existing model.}
}
@article{BERKANE20223136,
title = {Modelling elastic scaling of cloud with energy-efficiency: Application to smart-university},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {6, Part B},
pages = {3136-3150},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820305541},
author = {Mohamed Lamine Berkane and Mahmoud Boufaida and Nour El Houda Bouzerzour},
keywords = {Cloud computing, Elasticity, Energy-efficiency, Autonomic computing, Smart university},
abstract = {Cloud computing represents one of major innovations in Information Technology (IT). It is a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources. Elasticity is one of the most important characteristic in the cloud. Due to the enormous amounts of energy consumed by a cloud, it becomes difficult to build an elasticity system that satisfies all the requirements that might arise during its lifetime. Different approaches are proposed to address the energy-efficiency in the modelling of elasticity system at multiple layers of cloud services. Most of the existing approaches measure the overall hardware energy consumption, instead of the energy consumption of the software. Understanding how energy is consumed by cloud with elastic scaling mechanism is a key for managing better energy efficient software. This work proposes an architecture for modelling elasticity and energy-efficiency in the application layer. This architecture combines both the characteristics of adaptation (with Autonomic Computing) and variability (with Feature Model) into a single solution. The feature model considers the variability in the structural modelling and the behaviour one. We show the feasibility of the proposed approach by analysing the smart university applications associated with the Znn.com scenario.}
}
@article{DAS2020102043,
title = {Spatio-Fog: A green and timeliness-oriented fog computing model for geospatial query resolution},
journal = {Simulation Modelling Practice and Theory},
volume = {100},
pages = {102043},
year = {2020},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.102043},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301741},
author = {Jaydeep Das and Anwesha Mukherjee and Soumya K. Ghosh and Rajkumar Buyya},
keywords = {Geospatial query, Fog computing, Cloud computing, Delay-sensitive, Power-efficient},
abstract = {Geospatial data analysis is an emerging area of research today. Systems need to respond to user requests in a timely manner. In this paper we have proposed a fog computing framework namely Spatio-Fog, where the fog devices contain the geospatial data of their current region and process geospatial queries using resources in the proximity. The geospatial query resolution is performed by the fog device either itself or using cloud servers or fog device of other region depending on the geographical region related to the geospatial query. We have performed both empirical study and experimental analysis to demonstrate feasibility of our proposed approach. The empirical study illustrates that the proposed architecture Spatio-Fog reduces the power consumption and delay by approximately 43–47% and 47–83% respectively over the use of existing geospatial query resolution system. The experimental analysis demonstrates that the proposed framework reduces the power consumption and delay by 30–60% approximately than the existing geospatial query resolution system.}
}
@article{KAFKA2022111943,
title = {X-ray computed tomography analysis of pore deformation in IN718 made with directed energy deposition via in-situ tensile testing},
journal = {International Journal of Solids and Structures},
volume = {256},
pages = {111943},
year = {2022},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2022.111943},
url = {https://www.sciencedirect.com/science/article/pii/S0020768322003973},
author = {Orion L. Kafka and Cheng Yu and Puikei Cheng and Sarah J. Wolff and Jennifer L. Bennett and Edward J. Garboczi and Jian Cao and Xianghui Xiao and Wing Kam Liu},
keywords = {Additive manufacturing, Directed energy deposition, Tensile testing, Mechanical property variations, In-situ X-ray CT, Pore mechanics, Model verification},
abstract = {Directed energy deposition (DED) is a metal additive manufacturing technique often used for larger-scale components and part repair. It can result in material performance that differs from conventionally processed metal. This work studies spatial and orientation-based differences in tensile properties of nickel-based alloy IN718 using in-situ x-ray computed tomography to observe internal pore populations. Anisotropy and spatial variability in mechanical properties are shown while the evolution of pore shape during deformation is measured. Measured pore deformation is compared to predict deformations simulated using a computational crystal plasticity scheme, which provides insight, through inverse modeling, to the grain orientation in which the pore resides. The measurements provide a high fidelity method to compare experimental and computational approaches to pore deformation studies. Pore deformation measurements show that pores tend to grow and elongate in the direction of loading, consistent with ductile deformation and likely deforming with the material. Generally, the pore defects observed in this material (not from lack-of-fusion) do not cause so-called premature failure, and fully developed necking occurs prior to fracture.}
}
@article{JIA2022102743,
title = {Transient computing for energy harvesting systems: A survey},
journal = {Journal of Systems Architecture},
volume = {132},
pages = {102743},
year = {2022},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102743},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122002284},
author = {Min Jia and Edwin Hsing-Mean Sha and Qingfeng Zhuge and Shouzhen Gu},
keywords = {Transient computing, Energy harvesting systems, Checkpoint, Atomic task, Idempotency},
abstract = {Battery-powered, ultra-low-power embedded devices are often limited by the size and maintenance costs of batteries, giving rise to battery-less devices and the emergence of energy harvesting systems. Energy harvesters obtain enough energy from the environment in order to satisfy program execution. However, the difference in the harvesting source and the size of the energy storage makes the program not execute continuously due to frequent interruptions due to power failures. Frequent power failures make the program lose volatile state, inconsistent data, and non-termination, so the energy harvesting system has to preserve the storage of volatile logic, maintain data consistency, and avoid non-termination. In this paper, we show the transient computing techniques for energy harvesting systems. We hope that this research will provide researchers with insights into transient computing and help them address the remaining challenges.}
}
@article{FENG2021102048,
title = {A global-energy-aware virtual machine placement strategy for cloud data centers},
journal = {Journal of Systems Architecture},
volume = {116},
pages = {102048},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102048},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000448},
author = {Hao Feng and Yuhui Deng and Jie Li},
keywords = {Cloud computing, Cloud data centers, Energy efficiency, Virtual machine placement},
abstract = {Virtual machine (VM) placement is a key technique for energy optimization in cloud data centers. Previous work generally focus on how to place the VMs efficiently in servers to optimize the physical resources used (e.g., memory, bandwidth, CPU, etc.), network resources used or cooling energy consumption. These work can optimize the energy consumption of cloud data centers according to one or two aspects (e.g. server, network or cooling), however, these methods may cause increased energy consumption in other aspects. To address this problem, we propose a global-energy-aware VMP (virtual machine placement) strategy to reduce, from multiple aspects, the total energy consumption of data centers. A two-step SAG algorithm is designed to lower the energy consumption of cloud data centers where multiple VMs are deployed. We conduct extensive experiments to evaluate the effectiveness of SAG. Two workloads from real-world data centers are utilized to quantitatively measure and compare the performance of our SAG with other typical algorithms. Experimental results indicate that, compared to other algorithms, our global-energy-aware VMP strategy can reduce the total energy consumption of the cloud data center by 8%–24.9%.}
}
@article{KHAN2021100390,
title = {Energy, performance and cost efficient cloud datacentres: A survey},
journal = {Computer Science Review},
volume = {40},
pages = {100390},
year = {2021},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2021.100390},
url = {https://www.sciencedirect.com/science/article/pii/S1574013721000307},
author = {Ayaz Ali Khan and Muhammad Zakarya},
keywords = {Clouds, Datacentres, Resource management, Energy efficiency, Performance},
abstract = {In major Information Technology (IT) companies such as Google, Rackspace and Amazon Web Services (AWS), virtualization and containerization technologies are usually used to execute customers’ workloads and applications — as part of their cloud computing services offering. The computational resources are provided through large-scale datacentres, which consume substantial amount of energy and, consequently, affect our environment with global warming. Cloud datacentres have become a backbone for today’s business and economy, which are the fastest-growing electricity consumers, globally. Numerous studies suggest that ∼30% of the US datacentres are comatose and the others are grossly less-utilized, which make it possible to save energy through technologies like virtualization and containerization. These technologies provide support for allocation and consolidation of workloads on appropriate resources. However, consolidation comprises migrations of virtual machines (VMs), containers and/or applications, depending on the underlying virtualization method; that are expensive in terms of energy consumption, performance degradation, and therefore, costs which is mostly not accounted for in many existing models, and, possibly, it could be more energy and performance efficient not to consolidate. This paper describes energy consumption and performance, therefore, cost issues of large-scale datacentres. Besides, we cover various methods for energy and performance efficient distributed systems, clouds and datacentres. We elaborate energy efficiency methods at three different levels: hardware; resource management; and applications. Besides these, different performance management techniques are mapped onto taxonomies and described in details. In last, energy, performance and cost management techniques, at geographically distributed and multi-access edge computing platforms, are described along with critical discussion.}
}
@article{ZHOU2020360,
title = {Evaluating the permeability properties of green bed in iron ore sintering using high resolution X-ray computed tomography and orthogonal array tests},
journal = {Powder Technology},
volume = {375},
pages = {360-368},
year = {2020},
issn = {0032-5910},
doi = {https://doi.org/10.1016/j.powtec.2020.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S003259102030752X},
author = {Mingxi Zhou and Jianuo Xu and Hao Zhou},
keywords = {Green bed permeability, Iron ore sintering, Pore network, X-ray computed tomography, Orthogonal array tests},
abstract = {This study reconstructed porous structure and numerically evaluated permeability properties over several green sinter bed samples by applying X-ray computed tomography (XCT) and orthogonal array tests together. Contributions of three factors including moisture, hydrated lime and magnetite concentrate on experimental Japanese Permeability Units value and XCT-simulated absolute permeability coefficient were analyzed and compared. Results portray that pore network model is helpful to extract the pore distribution and topology in green sinter bed, the observed increase in green bed permeability is attributed to the increase of pore radius, throat radius and throat length, rather than tortuosity. The optimum combination for improving green bed permeability is 7.8% moisture, 4% hydrated lime and 0% concentrate. Moisture and hydrated lime factors are the more effective parameters on bed permeability while concentrate factor is insignificant, since those binders have different mechanisms in determining the outcome granule size distribution and strength, resulting in various pore properties.}
}
@article{JAISWAL2021155,
title = {Green computing in IoT: Time slotted simultaneous wireless information and power transfer},
journal = {Computer Communications},
volume = {168},
pages = {155-169},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.12.024},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421000220},
author = {Ankita Jaiswal and Sushil Kumar and Omprakash Kaiwartya and Mukesh Prasad and Neeraj Kumar and Houbing Song},
keywords = {Green computing, Energy optimization, Lagrangian dual decomposition, Internet of things},
abstract = {Simultaneous Wireless Information and Power Transfer (SWIPT) is an emerging field to transmit information and power in IoT network through the same RF signal. Time switching (TS) protocol is more favorable in free space communication than Power Splitting (PS) protocol when the transmitted RF signal is already weak. This is because, transmitted signal loses its power due to attenuation in free space, and using PS design receiver circuit (complex), the received weak signal is further split into two fraction for energy harvesting (EH) and information decoding (ID) simultaneously, that causes inadequacy in SWIPT system. Whereas, using TS design receiver circuit (simple) insert extra delay in the network as EH and ID operations are done in two different time domain one by one. Literature on SWIPT lacks towards cooperation between more energy harvesting in case of free space communication (TS) and critical information transmission in case of delay constraint communication (PS). In this context, this paper presents a time-slotted SWIPT (T-SWIPT) focusing on maximization of energy efficiency in the relay based sensors-enabled IoT network. It enables simultaneous energy harvesting at receiver and neighboring sensors without adding extra delay in the network. The PS ratio, transmission power allotment and energy broadcast time are jointly formulated as non-convex energy efficiency maximization problem. A solution to the problem is presented using Lagrangian dual decomposition and fractional programming. The performance evaluation shows that T-SWIPT attains optimum energy efficiency by trading off transmission power allotment, power-splitting ratio and sink broadcast time slot.}
}
@article{GAO2021127036,
title = {Site selection decision of waste-to-energy projects based on an extended cloud-TODIM method from the perspective of low-carbon},
journal = {Journal of Cleaner Production},
volume = {303},
pages = {127036},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.127036},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621012555},
author = {Jianwei Gao and Xiangzhen Li and Fengjia Guo and Xin Huang and Huijuan Men and Ming Li},
keywords = {Waste-to-energy project site selection, Low-carbon, Hesitant fuzzy linguistic term set with credibility, Combination weight, Cloud-TODIM method},
abstract = {The waste-to-energy (WTE) project has become a vital part for the development of renewable energy for its characteristic of harmlessness and resource utilization, and the concept of low-carbon has gradually become the universal consensus of all countries. How to select a satisfactory site for WTE project to make a win-win situation under a low-carbon perspective has become the concern of many researchers. Deficiencies in the current site selection process may lead to inaccurate results. To solve such problems, this paper proposes an extended cloud-TODIM (Tomada de Decisão Iterativa Multicritério) method. Firstly, we construct a criteria system covering five criteria and eighteen sub-criteria. Secondly, the evaluation information is described through the hesitant fuzzy linguistic term set (HFLTS) with credibility and then the hesitant fuzzy linguistic term set is converted to clouds, which can fully describe the fuzziness, uncertainty and randomness. Thirdly, the weights of criteria are calculated by the combination weight of Analytic Network Process (ANP) method and entropy method. Furthermore, an improved TODIM method is proposed to select the optional alternative. Finally, a case in Shaanxi province is provided to verify the feasibility of our method.}
}
@article{SINGHAL2022216,
title = {Special Issue on Smart Green Computing for Wireless Sensor Networks},
journal = {Computer Communications},
volume = {190},
pages = {216-218},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422001499},
author = {Chetna Singhal and Deepak Kumar Jain and Alberto Tarable and Anand Nayyar}
}
@article{LEE2022104612,
title = {Edge computing-enabled secure and energy-efficient smart parking: A review},
journal = {Microprocessors and Microsystems},
volume = {93},
pages = {104612},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104612},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122001545},
author = {Cheng Pin Lee and Fabian Tee Jee Leng and Riyaz Ahamed Ariyaluran Habeeb and Mohamed Ahzam Amanullah and Muhammad Habib ur Rehman},
keywords = {Smart parking, Edge computing, IoT security, Energy-efficient},
abstract = {With the increasing number of cars on road, it can be challenging trying to find a parking spot, especially during seasonal times. In the existing mechanism, the driver has to go in rounds trying to find a parking spot, this is an inefficient mechanism as it tends to make the driver waste fuel, increase carbon emission, which is not environmentally friendly, and make the driver feel stressed. Hence, this study investigates Smart Parking system, which integrates many Internet of Things (IoT) components alongside leveraging the use of cloud and edge computing could be the solution. Edge computing is a crucial technology that supports sensor level processing, which is a significant element for data privacy and security in Smart Parking applications. Smart Parking would be able to guide the driver directly to the parking spot, which will minimize or even eliminate the issues in the existing mechanism. It is noteworthy that it is necessary to ensure that the Smart Parking system is secure, because IoT devices are very much prone to security attacks. Further, introducing energy-efficient techniques in Smart Parking will contribute to a greener environment and an economically efficient model. Many studies have focused on Smart Parking with IoT, however, from our critical analysis, we have concluded that many studies have failed to address the security and energy efficiency aspect of smart parking. Hence, we have investigated the impact of security and energy efficiency in Smart Parking system, and have classified the existing literature based on the categories of devises such as sensors, communication protocols, security threats, energy efficiency, applications, and reviews of available resources based on the proposed taxonomy, the study has also tabulated few use cases and emphasized the relationship between IoT, security, and energy efficiency in smart parking, and identified and examined most significant research challenges.}
}
@article{WANG2022,
title = {Prospective observational study on the prognosis of ureteral lesions caused by impacted stones via dual-energy spectral computed tomography},
journal = {Asian Journal of Urology},
year = {2022},
issn = {2214-3882},
doi = {https://doi.org/10.1016/j.ajur.2022.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2214388222000911},
author = {Junjie Wang and Ximing Wang and Haozhou Zhong and Wengui Xie and Qilin Xi},
keywords = {Ureteral lesions, Impacted ureteral stone, Dual-energy spectral computed tomography, Ureteral stricture, Ureteroscopic lithotripsy},
abstract = {Objective
Ureteral lesions caused by impacted ureteral stones are likely to result in postoperative ureteral stricture. On this basis, the study aimed to investigate if dual-energy spectral computed tomography can predict ureteral hardening caused by impacted stones and to explore the relationship between different types of ureteral lesions and the risk of ureteral stricture.
Methods
This prospective study collected data of 93 patients with impacted stones from hospital automation system during January 2018 to October 2019. They underwent an abdominal scan on a dual-energy spectral computed tomography. During surgery, the operator used ureteroscopy to identify ureteral lesions, which were classified into four categories: edema, polyps, pallor, and hardening. Seven months later, 90 patients were reviewed for the degree of hydronephrosis.
Results
Endoscopic observations revealed 38 (41%) cases of ureteral edema, 20 (22%) cases of polyps, 13 (14%) cases of pallor, and 22 (24%) cases of hardening. There were significant differences in hydronephrosis, the period of impaction, the calcium concentration of the ureter, and the slope of the spectral Hounsfield unit curve between the four groups. After that, we evaluated the factors associated with ureteral hardening and found that the calcium concentration of the ureter and hydronephrosis remained independent predictors of ureteral hardening. Receiver operating characteristic curve analysis showed that 5.3 mg/cm³ calcium concentration of the ureter is an optimal cut-off value to predict ureteral hardening. The result of follow-up showed that 80 patients had complete remission of hydronephrosis, with a complete remission rate of 61.9% (13/21) in the hardening group and 97.1% (67/69) in the non-hardening group (p<0.001).
Conclusion
Calcium concentration of the ureter is an independent predictor of ureteral hardening. Patients with ureteral hardening have more severe hydronephrosis after ureteroscopic lithotripsy. When the calcium concentration of the ureter is less than 5.3 mg/cm³, ureteral lesions should be actively treated.}
}
@article{SHUKLA2023233,
title = {An energy-efficient single-cycle RV32I microprocessor for edge computing applications},
journal = {Integration},
volume = {88},
pages = {233-240},
year = {2023},
issn = {0167-9260},
doi = {https://doi.org/10.1016/j.vlsi.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167926022001195},
author = {Satyam Shukla and Punyesh Kumar Jha and Kailash Chandra Ray},
keywords = {Processor design, Single-cycle processor, RISC-V ISA, RV32I chip, FPGA, ASIC},
abstract = {With the advancement in the Internet of Things (IoT) and Wireless Sensor Node (WSN) applications computation on edge is preferred over computation on the cloud. However, due to limited resource availability in the edge environment, energy efficiency can be seen as the primary barrier in the growth of edge computing devices. In edge computing applications like wearable devices, forest monitoring systems, and other such applications, power is of utmost importance. The use of a complex, pipelined, general-purpose processor is redundant for such low-end applications and it leads to high power consumption. Hence, edge computing systems require low-power solutions while still maintaining a minimum performance level. In this work, an energy-efficient single-cycle RISC-V instruction set architecture (ISA) based RV32I processor is designed for low-end edge computing applications. The design incorporates base integer implementation of RISC-V ISA. The proposed computing system is designed using Verilog HDL and synthesized on Semi-conductor Laboratory (SCL) 180 nm CMOS process technology node. The synthesis tool reports power consumption of 14.7 mW at 50 MHz frequency with an area of 0.24 mm2. Though synthesis of this design reports area of 0.24 mm2, the design is taped out as a part of an available shuttle project where die size is fixed to 5 mm × 5 mm. The PNR (Place and Route) tool reports power consumption of 11.8 mW at an operating frequency of 40 MHz. A basic functional testing of proposed fabricated RV32I chip is carried out in laboratory experimental setup and results are presented. The comparison result shows that the proposed RV32I processor consumes 0.29 mW/MHz power which is less than state-of-the-art low-end processors. Hence, the proposed design is suitable for low-end edge computing applications.}
}
@article{KIMULI2022100909,
title = {Macroeconomic effects of a low carbon electrification of greater Kampala Metropolitan area energy policy: A computable general equilibrium analysis},
journal = {Energy Strategy Reviews},
volume = {43},
pages = {100909},
year = {2022},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2022.100909},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X22001031},
author = {Ismail Kimuli and Michael Lubwama and Adam Sebbit and John Baptist Kirabira},
keywords = {GKMA, Sustainability, Energy scenarios, Static CGE modeling, Low-carbon electrification},
abstract = {Greater Kampala Metropolitan Area (GKMA) is Uganda's capital facing increasing pressures to raise electricity generation and also mitigate CO2 emissions. A low-carbon electrification of the GKMA energy policy is proposed for sustainability. But the macroeconomic effects of this policy are unknown. The study uses a multi-sector, single region, static GKMA-CGE model to address the knowledge gap in 4 scenarios. BAU is the baseline against which comparisons with Kabejja (20% CO2 abatement); Carbon-Tax ($100/ton) and Lutta scenarios (95% CO2 abatement) are made. The results indicate GDP increased by Kabejja:0.7%; Carbon-Tax:1.3%; Lutta:1.56% with respect to BAU. Equivalent variation also increased compared to BAU. Energy & CO2 intensities of GDP decreased in all scenarios. The study recommends Lutta for a sustainable 2050.}
}
@article{SU2022100014,
title = {Sub-femto-Joule energy consumption memory device based on van der Waals heterostructure for in-memory computing},
journal = {Chip},
volume = {1},
number = {2},
pages = {100014},
year = {2022},
issn = {2709-4723},
doi = {https://doi.org/10.1016/j.chip.2022.100014},
url = {https://www.sciencedirect.com/science/article/pii/S2709472322000120},
author = {Zi-Jia Su and Zi-Hao Xuan and Jing Liu and Yi Kang and Chun-Sen Liu and Cheng-Jie Zuo},
keywords = {two-dimensional material heterostructures, nonvolatile memory, energy consumption, sub-femto-Joule},
abstract = {In-memory computing has carried out calculations in situ within each memory unit and its main power consumption comes from data writing and erasing. Further improvements in the energy efficiency of in-memory computing require memory devices with sub-femto-Joule energy consumption. Floating gate memory devices based on two-dimensional (2D) material heterostructures have outstanding characteristics such as non-volatility, multi-bit storage, and low operation energy, suitable for application in in-memory computing chips. Here, we report a floating gate memory device based on a WSe2/h-BN/Multilayer-graphene/h-BN heterostructure, the energy consumption of which is in sub-femto Joule (0.6 fJ) per operation for program/erase, and the read power consumption is in the tens of femto Watt (60 fW) range. We show a Hopfield neural network composed of WSe2/h-BN/Multilayer-graphene/h-BN heterostructure floating gate memory devices, which can recall the original patterns from incorrect patterns. These results shed light on the development of future compact and energy-efficient hardware for in-memory computing systems.}
}
@article{LEE2023102787,
title = {Scale-CIM: Precision-scalable computing-in-memory for energy-efficient quantized neural networks},
journal = {Journal of Systems Architecture},
volume = {134},
pages = {102787},
year = {2023},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2022.102787},
url = {https://www.sciencedirect.com/science/article/pii/S1383762122002727},
author = {Young Seo Lee and Young-Ho Gong and Sung Woo Chung},
keywords = {Digital-based computing-in-memory, Quantized neural networks, Precision-scalable computation},
abstract = {Quantized neural networks (QNNs), which perform multiply-accumulate (MAC) operations with low-precision weights or activations, have been widely exploited to reduce energy consumption. QNNs usually have a trade-off between energy consumption and accuracy depending on the quantized precision, so that it is necessary to select an appropriate precision for energy efficiency. Nevertheless, the conventional hardware accelerators such as Google TPU are typically designed and optimized for a specific precision (e.g., 8-bit), which may degrade energy efficiency for other precisions. Though an analog-based computing-in-memory (CIM) technology supporting variable precision has been proposed to improve energy efficiency, its implementation requires extremely large and power-consuming analog-to-digital converters (ADCs). In this paper, we propose Scale-CIM, a precision-scalable CIM architecture which supports MAC operations based on digital computations (not analog computations). Scale-CIM performs binary MAC operations with high parallelism, by executing digital-based multiplication operations in the CIM array and accumulation operations in the peripheral logic. In addition, Scale-CIM supports multi-bit MAC operations without ADCs, based on the binary MAC operations and shift operations depending on the precision. Since Scale-CIM fully utilizes the CIM array for various quantized precisions (not for a specific precision), it achieves high compute-throughput. Consequently, Scale-CIM enables precision-scalable CIM-based MAC operations with high parallelism. Our simulation results show that Scale-CIM achieves 1.5∼15.8 × speedup and reduces system energy consumption by 53.7∼95.7% across different quantized precisions, compared to the state-of-the-art precision-scalable accelerator.}
}
@article{CHEN2021104594,
title = {A theoretical model for the prediction of the minimum ignition energy of dust clouds},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {73},
pages = {104594},
year = {2021},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2021.104594},
url = {https://www.sciencedirect.com/science/article/pii/S0950423021002035},
author = {Tengfei Chen and Jan Berghmans and Jan Degrève and Filip Verplaetsen and Jo {Van Caneghem} and Maarten Vanierschot},
keywords = {Minimum ignition energy, Dust clouds, Theoretical model},
abstract = {In this study, a physical model of the dust cloud ignition process is developed for both cylindrical coordinates with a straight-line shaped ignition source and spherical coordinates with a point shaped ignition source. Using this model, a numerical algorithm for the calculation of the minimum ignition energy (MIE) is established and validated. This algorithm can evaluate MIEs of dusts and their mixtures with different dust concentrations and particle sizes. Although the average calculated cylindrical MIE (MIEcylindrical) of the studied dusts only amounts to 63.9% of the average experimental MIE value due to reasons including high idealization of the numerical model and possible energy losses in the experimental tests, the algorithm with cylindrical coordinates correctly predicts the experimental MIE variation trends against particle diameter and dust concentration. There is a power function relationship between the MIE and particle diameter of the type MIE ∝ dpk with k being approximately 2 for cylindrical coordinates and 3 for spherical coordinates. Moreover, as dust concentration increases MIE(conc) first drops because of the decreasing average distance between particles and, at fuel-lean concentrations the increasing dust cloud combustion heat; however, after the dust concentration rises beyond a certain value, MIE(conc) starts to increase as a result of the increasingly significant heat sink effect from the particles and, at fuel-rich concentrations the no longer increasing dust cloud combustion heat.}
}
@article{KUEH2023S245,
title = {Myocardial Characterisation Using Delayed Multi-Energy Cardiac Computed Tomography},
journal = {Heart, Lung and Circulation},
volume = {32},
pages = {S245-S246},
year = {2023},
note = {Abstracts for the 71st Annual Scientific Meeting of the Cardiac Society of Australia and New Zealand, 3-6 August 2023, Adelaide, Australia},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2023.06.279},
url = {https://www.sciencedirect.com/science/article/pii/S144395062303233X},
author = {S. Kueh and J. Benatar and R. Stewart}
}
@article{RAFIQ2022102492,
title = {Intelligent edge computing enabled reliable emergency data transmission and energy efficient offloading in 6TiSCH-based IIoT networks},
journal = {Sustainable Energy Technologies and Assessments},
volume = {53},
pages = {102492},
year = {2022},
issn = {2213-1388},
doi = {https://doi.org/10.1016/j.seta.2022.102492},
url = {https://www.sciencedirect.com/science/article/pii/S2213138822005422},
author = {Ahsan Rafiq and Mohammed Saleh {Ali Muthanna} and Ammar Muthanna and Reem Alkanhel and Wadhah Ahmed Muthanna Abdullah and Ahmed A. {Abd El-Latif}},
keywords = {IIoT, 6TiSCH, Authentication, DODAG, Task Scheduling, Edge offloading},
abstract = {The evolution of 6TiSCH networks is greatly increasing in the field of IIoT which supports reliable communication in IIoT environments. However, there is a lot of research gaps in the field of IIoT using 6TiSCH networks in terms of high latency, more energy consumption which degrades the overall performance of the IIoT environment. To address the above limitation in existing IIoT 6TiSCH approaches, an Edge-assisted 6TiSCH network for IIoT is proposed to overcome the above challenges such as Low latency and more energy consumption by efficient task scheduling and edge offloading considering risk in IIoT environment. The proposed work consists of three layers namely the 6TiSCH layer, Edge layer, and Cloud layer. In the 6TiSCH layer, initially, devices are registered by their credentials and authenticated by TA using Enhanced Advanced Encryption Standard (EAES) which reduces the unwanted energy consumption and latency by dropping the malicious devices. After authentication, only the verified devices are allowed for network construction using CORONA-based DODAG construction for selecting optimal parent which reduces the energy consumption. The optimal parent is selected by using Red Colobuses Monkey (RCM) optimization algorithm. Then two-level task scheduling is done namely Slot frame length optimization using Stochastic Gradient Descent (SGD) algorithm and adaptive partitioning using XG boost algorithm. In the Edge layer, effective offloading is done by using the Soft Actor-Critic (SAC) algorithm for reducing the energy consumption during offloading. Finally, all the processed data are sent to the cloud layer for access. The proposed work is experimented with using the Cooja simulation tool with Contiki 3 OS. The performance of the proposed work is compared to the existing works in terms of Latency (42 ms-20.4 ms) low, energy consumption (28.8 J-12 J) low in 6TiSCH layer, Latency (40 ms-20.4 ms) low, energy consumption (24.8 J-13.6 J) low in Edge layer, and overall comparison achieves throughput (508.8Kbps-322Kbps) high, packet delay ratio (8 %-4.6 %) high, end-to-end delay (6 s-9 s) low and efficiency (14.6 %-7.8 %) high. The experimental results show that our proposed work performs well than existing works.}
}
@article{PERDANA2022101006,
title = {European Economic impacts of cutting energy imports from Russia: A computable general equilibrium analysis},
journal = {Energy Strategy Reviews},
volume = {44},
pages = {101006},
year = {2022},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2022.101006},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X22002000},
author = {Sigit Perdana and Marc Vielle and Maxime Schenckery},
keywords = {European union, Russia, Computable general equilibrium model, Fit for 55 package, Imports ban},
abstract = {The recent economic sanctions against Russia can jeopardise the sustainability of the European Union’s (EU) energy supply. Despite the EU’s strong commitment to stringent abatement targets, fossil fuels still play a significant role in the EU energy policy. Furthermore, high dependency on Russian energy supplies underlines the vulnerability of the EU energy security. Using a global computable general equilibrium model, we prove that the current EU embargo on coal and oil imported from Russia will have adverse supply effects, substantially increasing energy prices and welfare costs for the EU resident. Although it reduces emissions, extending the embargo to include natural gas doubles this welfare cost. The use of coal is likely to increase, especially with respect to EU electricity generation, given the current constraints of additional import capacities from non-Russian producers. The impact on Russia once the EU extends the sanctions to natural gas is less substantial than on the EU. Russian welfare cost will increase less than 50%, indicating that extending the current restriction to boycott Russian gas is a costly policy option.}
}
@article{LI2022569,
title = {Quantitative dual-energy computed tomography texture analysis predicts the response of primary small hepatocellular carcinoma to radiofrequency ablation},
journal = {Hepatobiliary & Pancreatic Diseases International},
volume = {21},
number = {6},
pages = {569-576},
year = {2022},
issn = {1499-3872},
doi = {https://doi.org/10.1016/j.hbpd.2022.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1499387222001357},
author = {Jin-Ping Li and Sheng Zhao and Hui-Jie Jiang and Hao Jiang and Lin-Han Zhang and Zhong-Xing Shi and Ting-Ting Fan and Song Wang},
keywords = {Hepatocellular carcinoma, Dual-energy, Radiofrequency ablation, Tumor response, Texture analysis},
abstract = {Background
Radiofrequency ablation (RFA) is one of the effective therapeutic modalities in patients with hepatocellular carcinoma (HCC). However, there is no proper method to evaluate the HCC response to RFA. This study aimed to establish and validate a clinical prediction model based on dual-energy computed tomography (DECT) quantitative-imaging parameters, clinical variables, and CT texture parameters.
Methods
We enrolled 63 patients with small HCC. Two to four weeks after RFA, we performed DECT scanning to obtain DECT-quantitative parameters and to record the patients’ clinical baseline variables. DECT images were manually segmented, and 56 CT texture features were extracted. We used LASSO algorithm for feature selection and data dimensionality reduction; logistic regression analysis was used to build a clinical model with clinical variables and DECT-quantitative parameters; we then added texture features to build a clinical-texture model based on clinical model.
Results
A total of six optimal CT texture analysis (CTTA) features were selected, which were statistically different between patients with or without tumor progression (P < 0.05). When clinical variables and DECT-quantitative parameters were included, the clinical models showed that albumin-bilirubin grade (ALBI) [odds ratio (OR) = 2.77, 95% confidence interval (CI): 1.35-6.65, P = 0.010], λAP (40-100 keV) (OR = 3.21, 95% CI: 3.16-5.65, P = 0.045) and ICAP (OR = 1.25, 95% CI: 1.01-1.62, P = 0.028) were associated with tumor progression, while the clinical-texture models showed that ALBI (OR = 2.40, 95% CI: 1.19-5.68, P = 0.024), λAP (40-100 keV) (OR = 1.43, 95% CI: 1.10-2.07, P = 0.019), and CTTA-score (OR = 2.98, 95% CI: 1.68-6.66, P = 0.001) were independent risk factors for tumor progression. The clinical model, clinical-texture model, and CTTA-score all performed well in predicting tumor progression within 12 months after RFA (AUC = 0.917, 0.962, and 0.906, respectively), and the C-indexes of the clinical and clinical-texture models were 0.917 and 0.957, respectively.
Conclusions
DECT-quantitative parameters, CTTA, and clinical variables were helpful in predicting HCC progression after RFA. The constructed clinical prediction model can provide early warning of potential tumor progression risk for patients after RFA.}
}
@article{DEMIREZEN2021111237,
title = {Feasibility of Cloud Based Smart Dual Fuel Switching System (SDFSS) of Hybrid Residential Space Heating Systems for Simultaneous Reduction of Energy Cost and Greenhouse Gas Emission},
journal = {Energy and Buildings},
volume = {250},
pages = {111237},
year = {2021},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2021.111237},
url = {https://www.sciencedirect.com/science/article/pii/S0378778821005211},
author = {Gulsun Demirezen and Alan S. Fung},
keywords = {Greenhouse gas emissions, ASHP, Carbon tax, Natural gas, Internet-of-things, Sensors, Data and Measurement, Energy efficiency, Applied energy, Energy cost, Optimization, Fuel switching, Heating systems, North America, Hybrid heating systems, Cold climate, Global warming, Climate change, Smart dual fuel switching system, Data analysis, Algorithm},
abstract = {Through tests conducted at the Toronto and Region Conservation Authority’s (TRCA) Archetype Sustainable Houses, a hybrid residential space heating system with a supervisory controller was monitored and studied to evaluate its performance and effectiveness for the heating season. A high efficiency natural gas furnace (NGF) and an electric air source heat pump (ASHP) were coupled together to meet the space heating demand of the house. This integrated system is called the cloud based Smart Dual Fuel Switching System (SDFSS) that considers time-of-use (TOU) pricing, fuel cost, short-term weather forecast, and equipment efficiencies and capacities. This multi-variable decision-making process defines an optimal schedule for the hybrid system to run more cost effectively. This paper analyses two separate SDFSSs. According to these analyses, the SDFSS systems showed lower operating cost with respect to the furnace or the ASHP system alone with various carbon tax (CT) levels from $0 to $250/tonne of CT with an increment of $10/tonne of CT that were simulated, along with a significant GHG emission reduction relative to the conventional heating systems. Furthermore, with these technologies, Canada’s residential sector could potentially meet Canada’s Paris Agreement goals. The SDFSS technology is a is flexible, user friendly, ubiquitous technology for the smooth transition from today’s natural gas dominated space heating system to the future’s low carbon infrastructure powered by a heat pump and renewable energy.}
}
@article{DODIER2020e892,
title = {Novel Software-Derived Workflow in Extracranial–Intracranial Bypass Surgery Validated by Transdural Indocyanine Green Videoangiography},
journal = {World Neurosurgery},
volume = {134},
pages = {e892-e902},
year = {2020},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2019.11.038},
url = {https://www.sciencedirect.com/science/article/pii/S1878875019328748},
author = {Philippe Dodier and Thomas Auzinger and Gabriel Mistelbauer and Wei-Te Wang and Heber Ferraz-Leite and Andreas Gruber and Wolfgang Marik and Fabian Winter and Gerrit Fischer and Josa M. Frischer and Gerhard Bavinzski},
keywords = {Cerebral revascularization, EC–IC bypass surgery, Flow augmentation, Flow replacement, STA-MCA bypass, Transdural ICG-VA, Virtual planning},
abstract = {Background
The introduction of image-guided methods to bypass surgery has resulted in optimized preoperative identification of the recipients and excellent patency rates. However, the recently presented methods have also been resource-consuming. In the present study, we have reported a cost-efficient planning workflow for extracranial–intracranial (EC–IC) revascularization combined with transdural indocyanine green videoangiography (tICG-VA).
Methods
We performed a retrospective review at a single tertiary referral center from 2011 to 2018. A novel software-derived workflow was applied for 25 of 92 bypass procedures during the study period. The precision and accuracy were assessed using tICG-VA identification of the cortical recipients and a comparison of the virtual and actual data. The data from a control group of 25 traditionally planned procedures were also matched.
Results
The intraoperative transfer time of the calculated coordinates averaged 0.8 minute (range, 0.4–1.9 minutes). The definitive recipients matched the targeted branches in 80%, and a neighboring branch was used in 16%. Our workflow led to a significant craniotomy size reduction in the study group compared with that in the control group (P = 0.005). tICG-VA was successfully applied in 19 cases. An average of 2 potential recipient arteries were identified transdurally, resulting in tailored durotomy and 3 craniotomy adjustments. Follow-up patency results were available for 49 bypass surgeries, comprising 54 grafts. The overall patency rate was 91% at a median follow-up period of 26 months. No significant difference was found in the patency rate between the study and control groups (P = 0.317).
Conclusions
Our clinical results have validated the presented planning and surgical workflow and support the routine implementation of tICG-VA for recipient identification before durotomy.}
}
@article{ARORA2021102260,
title = {An intelligent energy efficient storage system for cloud based big data applications},
journal = {Simulation Modelling Practice and Theory},
volume = {108},
pages = {102260},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2020.102260},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X2030188X},
author = {Sumedha Arora and Anju Bala},
keywords = {Energy consumption, Prediction, Power, Arrival, Request rate, Disk},
abstract = {Storage technology has emerged as an indispensable paradigm for processing various applications in cloud data centers. The storage infrastructure consisting of Hard Disk Drives (HDDs) and Solid-State Drives (SSDs) accounts for high energy consumption. Also, the trade-offs between HDDs and SSDs in terms of cost and energy consumption are extremely high. Therefore, disk-based storage subsystems need to be more energy efficient. This paper proposes an intelligent energy-efficient hybrid disk storage system. The proposed system recognizes the frequently used data from traces of applications. Replica management along with data layout is used to allocate frequently used files in hot disks and the other files in cold disks. The request is executed using an intelligent scheduling technique that searches and selects the disk based on its states. The scheduling technique selects either an idle or active disk without spinning the standby disks. The selection procedure also considers minimum waiting time for the active disk and maximum remaining idle time for the idle disk. The proposed system has been implemented by adding disk management in the cloud environment, which proved to be highly effective in achieving a 39% power savings with an 18.26% decrease in execution time.}
}
@article{SAVOLDI2022103457,
title = {Thermal-hydraulic analysis of superconducting cables for energy applications with a novel open object-oriented software: OPENSC2},
journal = {Cryogenics},
volume = {124},
pages = {103457},
year = {2022},
issn = {0011-2275},
doi = {https://doi.org/10.1016/j.cryogenics.2022.103457},
url = {https://www.sciencedirect.com/science/article/pii/S001122752200039X},
author = {Laura Savoldi and Daniele Placido and Sofia Viarengo},
keywords = {Numerical modelling, Object-oriented model, Open-source software, Test-driven development, Thermal-hydraulic transients, Superconducting cables for energy applications},
abstract = {Super-conducting cables are an enabling technology for energy applications such as large magnetic-confinement nuclear fusion machine, and a promising key player in the power transmission of the next future, both in AC and DC conditions. While the thermal–hydraulic analysis of forced-flow superconducting cables for fusion application can only rely on commercial or proprietary numerical tools, such kind of tools for power transmission cables are not even available. Within the framework of Open Science, set as a priority by the European Commission in Horizon Europe, the novel software OPEN Super Conducting Cables (OPENSC2) has been developed to grant the entire research community the possibility to simulate thermal–hydraulic transients in forced-flow superconducting cables for energy applications. A Test-Driven Development has been adopted for the OPENSC2 within an object-oriented approach. Following the TDD approach, three test cases are considered of paramount interest for the OPENSC2 development, deriving the set of characteristics that the target object-oriented tool should comply with, and namely: 1) a heat slug propagation along an ITER-like 2-region cable-in-conduit conductor, with a thousand of mm-size low-critical-temperature superconducting (LTS) strands, cooled by supercritical helium (SHe); 2) the heat diffusion across the cross section of a twisted-slotted-core cable-in-conduit conductor, with high-critical-temperature (HTS) superconducting tapes, for fusion application, cooled by SHe and 3) the nominal operation of a single-phase HTS High-voltage, Direct Current power cable, with a 2-cryostat configuration and 2 different fluids adopted as primary coolant and thermal shield. In the object-oriented OPENSC2 the class “conductor” is defined, where each Conductor Object (CO) is the combination of different lower-level objects (both fluid and solid components) instantiated by the class. The choice of each component drives the automatic selection of the appropriate physical equation(s) in the code, as well as the possible interactions between them. Thermo-physical properties of different materials and cryogens can be attributed to the components of a conductor objects, taken form open datasets. A user-friendly GUI allows setting and monitoring the simulations while running. The software is tested in the three case studies targeted in the TDD, to show eventually how it allows modeling the three test cases presented here. The Verification and Validation of the CO methods performed through benchmarks against the 4C code is also presented and discussed.}
}
@article{LAGO2021102329,
title = {SinergyCloud: A simulator for evaluation of energy consumption in data centers and hybrid clouds},
journal = {Simulation Modelling Practice and Theory},
volume = {110},
pages = {102329},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102329},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21000472},
author = {Daniel G. Lago and Rodrigo A.C. {da Silva} and Edmundo R.M. Madeira and Nelson L.S. {da Fonseca} and Deep Medhi},
keywords = {Simulator, Cloud computing, Green computing, Data center, Virtual machine, Scheduling},
abstract = {One of the significant limitations in the evaluation of hybrid clouds is the difficulty in validating new solutions by their deployment in real systems since replication of tests on large environments is usually highly expensive. To overcome these limitations, simulators have become popular for conducting preliminary tests. However, there is still a lack of simulation tools with easy-to-use code that allows handling diverse cloud scenarios and the simulation of energy consumption of all devices (hosts, switches, routers, and storage). To fill this gap, the simulator SinergyCloud has been developed to evaluate data centers in hybrid clouds. SinergyCloud allows evaluating diverse cloud scenarios, including energy consumption, workflow makespan, the completion time of tasks, and migrations of virtual machines, with a fine granularity of abstraction. SinergyCloud can handle the simulation of hybrid clouds with multiple data centers composed of hundreds of thousands of devices. Moreover, it is a Java-based, event-driven, and packet-level simulator, having a less steep learning curve than do other simulators. To show the feasibility of SinergyCloud, we performed accuracy and scalability analyses. Metrics values obtained by another simulator had less than 1% difference compared to SinergyCloud, demonstrating its accuracy. In terms of scalability, a scenario with 10,000 hosts was simulated in about 7 h using a typical personal computer. A comprehensive analysis of algorithms for the scheduling of virtual machines in a hybrid cloud is also presented, showing how to perform various evaluations.}
}
@article{LIN2023295,
title = {Discordance in lumbar bone mineral density measurements by quantitative computed tomography and dual-energy X-ray absorptiometry in postmenopausal women: a prospective comparative study},
journal = {The Spine Journal},
volume = {23},
number = {2},
pages = {295-304},
year = {2023},
issn = {1529-9430},
doi = {https://doi.org/10.1016/j.spinee.2022.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S1529943022010002},
author = {Wentao Lin and Chaoqin He and Faqin Xie and Tao Chen and Guanghao Zheng and Houjie Yin and Haixiong Chen and Zhiyun Wang},
keywords = {Bone mineral density, Dual X-ray absorptiometry, DXA, Lumbar osteoporosis, Quantitative computed tomography, QCT, Specific level},
abstract = {Background Context
Level-specific lumbar bone mineral density (BMD) evaluation of a single vertebral body can provide useful surgical planning and osteoporosis management information. Previous comparative studies have primarily focused on detecting spinal osteoporosis but not at specific levels.
Purpose
To compare the detection rate of lumbar osteoporosis between quantitative computed tomography (QCT) and dual-energy X-ray absorptiometry (DXA); to explore and analyze the distribution models of QCT-derived BMD and DXA T-score at the specific levels; and to evaluate the diagnostic accuracy of level-specific BMD thresholds for the prediction of osteoporotic vertebral compression fracture (OVCF) in postmenopausal women.
Study Design/Setting
A comparative analysis of prospectively collected data comparing QCT-derived BMD with DXA T-score.
Patient Sample
A total of 296 postmenopausal women who were referred to the spine service of a single academic institution were enrolled.
Outcome Measures
QCT-derived BMD and DXA T-score at specific levels, with or without osteoporotic vertebral compression fracture.
Methods
Postmenopausal women who underwent QCT and DXA within a week of admission from May 2019 to June 2022 were enrolled. The diagnostic criteria for osteoporosis recommended by the World Health Organization and the American College of Radiology were used for lumbar osteoporotic diagnosis. To evaluate differences in lumbar BMD measurements at specific levels, a threshold of T score=-2.5 and QCT-derived BMD = 80 mg/cm3 were used to categorize level-specific lumbar BMD into low and high BMD. Disagreements in BMD categorization between DXA and QCT were classified as a minor or major discordance based on the definition by Woodson. Data between QCT and DXA were visualized in a stacked bar plot and analyzed. Correlations between DXA and QCT at the specific levels were evaluated using Pearson's linear correlation and scatter plots. Curve fitting of BMD distribution, receiver operating characteristic (ROC) and area under the curve (AUC) for each single vertebral level was performed.
Results
Of the 296 patients, QCT diagnosed 61.1% as osteoporosis, 30.4% as osteopenia and 8.4% as normal. For those screened with DXA, 54.1% of the patients had osteoporosis, 29.4% had osteopenia and 16.6% had normal BMD. Diagnoses were concordant for 194 (65.5%) patients. Of the other 102 discordant patients, 5 (1.7%) were major and 97 (32.8%) were minor. Significant correlations in level-specific BMD between DXA and QCT were observed (p<.001), with Pearson's correlation coefficients ranging from 0.662 to 0.728. The correlation strength was in the order of L1 > L2 > L3 > L4. The low BMD detection rate for QCT was significantly higher than that for DXA at the L3 and L4 levels (65% vs. 47.9% and 68.1% vs 43.7, respectively, p<.001). Patients with OVCF showed significantly lower QCT-derived BMD (47.2 mg/cm3 vs. 83.2 mg/cm3, p<.001) and T-score (-3.39 vs. -1.98, p<.001) than those without OVCF. Among these patients, 82.8% (101/122) were diagnosed with osteoporosis by QCT measurement, while only 74.6% (91/122) were diagnosed by DXA. For discrimination between patients with and without OVCF, QCT-derived BMD showed better diagnosed performance (AUC range from 0.769 to 0.801) than DXA T-score (AUC range from 0.696 to 0.753).
Conclusion
QCT provided a more accurate evaluation of lumbar osteoporosis than DXA. The QCT-derived BMD measurements at a specific lumbar level have a high diagnostic performance for OVCF.}
}
@article{ZHOU2022211,
title = {An intelligence energy consumption model based on BP neural network in mobile edge computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {167},
pages = {211-220},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2022.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0743731522001149},
author = {Zhou Zhou and Yangfan Li and Fangmin Li and Hongbing Cheng},
keywords = {BP neural network, Energy consumption model, Edge server, Power modeling, Principal component analysis (PCA)},
abstract = {Establishing an accurate edge server power model is helpful for resource providers to predict and optimize power consumption within edge data centers. Considering the fact that the accuracy of the previous energy consumption model is easily affected by the workload types, this paper develops an edge server power model based on BP (back propagation) neural network and feature selection, which is denoted by DSBF. For different task types, DSBF leverages “principal component analysis (PCA)” to analyze the contribution of each energy consumption parameter and selects “representative parameter”, and then builds a power model based on BP neural network. In contrast to other power models, DSBF can effectively handle the variable workload. To measure the effectiveness of the DSBF model, a series of experiments were conducted. The results suggest that compared with other energy consumption models, DSBF can better adapt to the changing workload and has advantages in predicting the accuracy of the energy consumption model.}
}
@article{BOCCALI2022167434,
title = {High Energy Physics computing for the next decade},
journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
volume = {1043},
pages = {167434},
year = {2022},
issn = {0168-9002},
doi = {https://doi.org/10.1016/j.nima.2022.167434},
url = {https://www.sciencedirect.com/science/article/pii/S0168900222007264},
author = {Tommaso Boccali},
keywords = {High Energy Physics, Computing, Big Data, Distributed computing},
abstract = {The next 10 years will be exciting for High Energy Physics, with new experiments entering data taking (High Luminosity LHC) or being designed and possibly approved (FCC, CEPC, ILC, MU_COLL). The computing infrastructure, including the software stacks for selection, simulation, reconstruction and analyses, will be crucial for the success of the physics programs. This contribution wants to address the landscape and the state-of-the-art in the field, highlighting the strong and weak points, and the aspects which still need sizeable R&D.}
}
@article{WEN2023473,
title = {Energy-efficient task allocation for reliable parallel computation of cluster-based wireless sensor network in edge computing},
journal = {Digital Communications and Networks},
volume = {9},
number = {2},
pages = {473-482},
year = {2023},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001365},
author = {Jiabao Wen and Jiachen Yang and Tianying Wang and Yang Li and Zhihan Lv},
keywords = {Wireless sensor network, Parallel computation, Task allocation, Genetic algorithm, Ant colony optimization algorithm, Energy-efficient, Load balancing},
abstract = {To efficiently complete a complex computation task, the complex task should be decomposed into sub-computation tasks that run parallel in edge computing. Wireless Sensor Network (WSN) is a typical application of parallel computation. To achieve highly reliable parallel computation for wireless sensor network, the network's lifetime needs to be extended. Therefore, a proper task allocation strategy is needed to reduce the energy consumption and balance the load of the network. This paper proposes a task model and a cluster-based WSN model in edge computing. In our model, different tasks require different types of resources and different sensors provide different types of resources, so our model is heterogeneous, which makes the model more practical. Then we propose a task allocation algorithm that combines the Genetic Algorithm (GA) and the Ant Colony Optimization (ACO) algorithm. The algorithm concentrates on energy conservation and load balancing so that the lifetime of the network can be extended. The experimental result shows the algorithm's effectiveness and advantages in energy conservation and load balancing.}
}
@article{ASKARI2022149,
title = {A parametric assessing and intelligent forecasting of the energy and exergy performances of a dish concentrating photovoltaic/thermal collector considering six different nanofluids and applying two meticulous soft computing paradigms},
journal = {Renewable Energy},
volume = {193},
pages = {149-166},
year = {2022},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2022.04.155},
url = {https://www.sciencedirect.com/science/article/pii/S0960148122006231},
author = {Ighball Baniasad Askari and Amin Shahsavar and Mehdi Jamei and Francesco Calise and Masoud Karbasi},
keywords = {Dish concentrating photovoltaic thermal system, Exergy, Multi-gene genetic optimization, Nanofluid, Thermodynamic analysis},
abstract = {In the present study, the application of six engine oil-based Nano fluids (NFs) in a solar concentrating photovoltaic thermal (CPVT) collector is investigated. The calculations were performed for different values of nanoparticle volume concentration, receiver tube diameter, concentrator surface area, receiver length, receiver actual to the maximum number of channels ratio, beam radiation, and a constant volumetric flow rate. Besides, two novel soft computing paradigms namely, the cascaded forward neural network (CFNN) and Multi-gene genetic programming (MGGP) were adopted to predict the first law efficiency (ηI) and second law efficiency (ηII) of the system based on the influential parameters, as the input features. It was found that the increase of nanoparticle concentration leads to an increase in ηI and a decrease in ηII. Moreover, the rise of both the concentrator surface area (from 5 m2 to 20 m2) and beam irradiance (from 150 W/m2 to 1000 W/m2) entails an increase in both the ηI (by 39% and 261%) and ηII (by 55% and 438%). Furthermore, it was reported that the pattern of changes in both ηI and ηII with serpentine tube diameter, receiver plate length, and absorber tube length is increasing-decreasing. The results of modeling demonstrated that the CFNN had superior performance than the MGGP model.}
}
@article{MEDARA2021102323,
title = {Energy-aware workflow task scheduling in clouds with virtual machine consolidation using discrete water wave optimization},
journal = {Simulation Modelling Practice and Theory},
volume = {110},
pages = {102323},
year = {2021},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2021.102323},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X21000447},
author = {Rambabu Medara and Ravi Shankar Singh and  Amit},
keywords = {Cloud computing, Workflow scheduling, VM consolidation, Water wave optimization, Energy-aware, Resource utilization},
abstract = {The scientific workflows are high-level complex applications that demand more computing power. The cloud data center (CDC) remains one of the essential models of economic infrastructure for workflow applications. These CDCs consume a lot of electric power while running workflow applications. Hence, efficient energy-aware scheduling techniques are required to perform the task to a virtual machine (VM) mapping. The existing researches overlooked to join the workflow scheduling and VM consolidation which addresses resource utilization and energy consumption effectively. In this article, we propose an energy-aware algorithm for workflow scheduling in cloud computing with VM consolidation called EASVMC. The proposed EASVMC approach is modeled to address the multi-objectives such as energy consumption, resource utilization, and VM migrations. The EASVMC algorithm runs in two phases task scheduling and VM consolidation (VMC). In the first phase, the task with maximum execution length is mapped to the virtual machine that will perform it with the minimum energy. The second phase contains VM consolidation is a prominent NP-hard problem. The VMC phase categorizes the physical hosts into the normal load, under-loaded and overloaded hosts based on CPU utilization. Double threshold values are used for this purpose. VMs from underloaded and overloaded hosts are migrated to normally loaded hosts. For the VMC phase, we used a nature-inspired meta-heuristic approach called the Water Wave Optimization (WWO) algorithm, which finds a suitable migration plan to reduce the energy consumption by increasing the overall resource utilization and switch off idle hosts after migrating its VMs to a suitable target host. The efficiency of our proposed method evaluated using the WorkflowSim simulation tool with five different real-world scientific workloads. The experimental results show that the EASVMC approach surpassed the similar works in stated objectives irrespective of diverse workloads.}
}
@article{WADA2022110461,
title = {A novel fast kilovoltage switching dual-energy computed tomography technique with deep learning: Utility for non-invasive assessments of liver fibrosis},
journal = {European Journal of Radiology},
volume = {155},
pages = {110461},
year = {2022},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2022.110461},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X22003114},
author = {Noriaki Wada and Nobuhiro Fujita and Keisuke Ishimatsu and Seiichiro Takao and Tomoharu Yoshizumi and Yoshiko Miyazaki and Yoshinao Oda and Akihiro Nishie and Kousei Ishigami and Yasuhiro Ushijima},
keywords = {Deep learning-based spectral CT, Iodine density, Extracellular volume, Liver fibrosis},
abstract = {Purpose
To investigate whether the iodine density of liver parenchyma in the equilibrium phase and extracellular volume fraction (ECV) measured by deep learning-based spectral computed tomography (CT) can enable noninvasive liver fibrosis staging.
Method
We retrospectively analyzed 63 patients who underwent dynamic CT using deep learning-based spectral CT before a hepatectomy or liver transplantation. The iodine densities of the liver parenchyma (I-liver) and abdominal aorta (I-aorta) were independently measured by two radiologists using iodine density images at the equilibrium phase. The iodine-density ratio (I-ratio: I-liver/I-aorta) and CT-ECV were calculated. Spearman's rank correlation analysis was used to evaluate the relationship between the I-ratio or CT-ECV and liver fibrosis stage, and receiver operating characteristic (ROC) analysis was used to evaluate the diagnostic performances of the I-ratio and CT-ECV.
Results
The I-ratio and CT-ECV showed significant positive correlations with liver fibrosis stage (ρ = 0.648, p < 0.0001 and ρ = 0.723, p < 0.0001, respectively). The areas under the ROC curve for the CT-ECV were 0.882 (F0 vs ≥ F1), 0.873 (≤F1 vs ≥ F2), 0.848 (≤F2 vs ≥ F3), and 0.891 (≤F3 vs F4).
Conclusions
Deep learning-based spectral CT may be useful for noninvasive assessments of liver fibrosis.}
}
@article{GAO2023110618,
title = {Association between extramural vascular invasion and iodine quantification using dual-energy computed tomography of rectal cancer: a preliminary study},
journal = {European Journal of Radiology},
volume = {158},
pages = {110618},
year = {2023},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2022.110618},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X22004685},
author = {Wei Gao and Yuqi Zhang and Yana Dou and Lei Zhao and Hui Wu and Zhenxing Yang and Aishi Liu and Lu Zhu and Fene Hao},
keywords = {Rectal cancer, Extramural vascular invasion, Dual-energy CT, Iodine quantification},
abstract = {Objective
This study aimed to investigate whether histopathological confirmed extramural vascular invasion (EMVI) is associated with quantitative parameters derived from dual-energy computed tomography (DECT) of rectal cancer.
Methods
This retrospective study included patients with rectal cancer who underwent rectal cancer surgery and DECT (including arterial-, venous-, and delay-phase scanning) between November 2019 and November 2020. The EMVI of rectal cancer was confirmed via postoperative pathological results. Iodine concentration (IC), IC normalized to the aorta (NIC), and CT attenuation values of the three phases were measured and compared between patients with and without EMVI. Receiver operating characteristic (ROC) curves were generated to determine the diagnostic performance of these DECT quantitative parameters.
Results
Herein, 36 patients (22 men and 14 women) with a mean age of 62 [range, 43–77] years) with (n = 13) and without (n = 23) EMVI were included. Patients with EMVI exhibited significantly higher IC in the venous and delay phases (venous-phase: 2.92 ± 0.6 vs 2.34 ± 0.48; delay-phase: 2.46 ± 0.47 vs 1.88 ± 0.35) and NIC in all the three phases (arterial-phase: 0.31 ± 0.12 vs 0.24 ± 0.06; venous-phase: 0.58 ± 0.11 vs 0.41 ± 0.07; delay-phase: 0.68 ± 0.10 vs 0.46 ± 0.08) than patients without EMVI. Among them, the highest area under the ROC curve (AUC) was obtained in the delay-phase NIC (AUC = 0.983). IC in the arterial-phase and CT attenuation in all the three phases did not significantly differ between patients with and without EMVI (p = 0.205–0.869).
Conclusion
Iodine quantification using dual-energy CT, especially the NIC of the tumor, differs between the EMVI-positive and EMVI-negative groups and seems to help predict the EMVI of rectal cancer in this preliminary study; however, a larger sample size study is warranted in the future.}
}
@article{LIANG2021107020,
title = {An energy-aware resource deployment algorithm for cloud data centers based on dynamic hybrid machine learning},
journal = {Knowledge-Based Systems},
volume = {222},
pages = {107020},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107020},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121002835},
author = {Bin Liang and Di Wu and Pengfei Wu and Yuanqi Su},
keywords = {Machine learning, Supervised learning, Unsupervised learning, Cloud data center, Energy consumption optimization},
abstract = {To meet the ever-increasing requirements of cloud users, cloud service providers have further increased the deployment of cloud data centers. Cloud users can freely choose the cloud data center that suits them according to their own business characteristics and budget expenditures. This requires cloud service providers to continuously improve service quality and reduce usage costs to expand their own user base. Mature cloud service providers will continuously optimize cloud tasks and virtual machine deployment methods to increase physical machine utilization and reduce cloud data center energy consumption. However, existing virtual machine deployment algorithms usually have low utilization of physical machines or high energy consumption of cloud data centers, thereby reducing the frequency of use by cloud users and the benefits of cloud service providers. This paper systematically analyzes virtual machine and physical machine models. At the same time, the K-means clustering algorithm for unsupervised learning and the KNN classification algorithm for supervised learning are expanded to establish a dynamic hybrid resource deployment rule. Then, an energy-aware resource deployment algorithm for cloud data centers based on dynamic hybrid machine learning (EHML) is proposed based on the theory of machine learning. This algorithm reduces energy consumption by increasing the average utilization of physical machines. Finally, the experimental test results show that the average utilization of physical machines and energy consumption of the algorithm are significantly better than those of the comparison algorithms.}
}
@article{HE202251,
title = {Online delay-guaranteed workload scheduling to minimize power cost in cloud data centers using renewable energy},
journal = {Journal of Parallel and Distributed Computing},
volume = {159},
pages = {51-64},
year = {2022},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2021.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0743731521001805},
author = {Huaiwen He and Hong Shen and Qing Hao and Hui Tian},
keywords = {Cloud data center, Delay tolerant scheduling, Smart grid, Renewable energy, Lyapunov optimization},
abstract = {More and more cloud data centers are turning to leverage on-site renewable energy to reduce power cost for sustainable development. But how to effectively coordinate the intermittent renewable energy with workload remains to be a great challenge. This paper investigates the problem of workload scheduling for power cost minimization under the constraints of different Service Level Agreements (SLAs) of delay tolerant workload and delay sensitive workload for green data centers in a smart grid. Different from the existing studies, we take into consideration of the impact of zero price in the smart grid and the cost of on-site renewable energy. To handle the randomness of workload, electricity price and renewable energy availability, we first formulate the problem as a constrained stochastic problem. Then we propose an efficient online control algorithm named ODGWS (Online Delay-Guaranteed Workload Scheduling) which makes online scheduling decisions achieve a bounded guarantee from the worst scheduling delay for delay tolerant workload. Compared with the existing solutions, our ODGWS decomposes the problem into that of solving a simple optimization problem within each time slot in O(1) time without needing any future information. The rigorous theoretical analysis demonstrates that our algorithm achieves a [O(1V),O(V)] cost-delay tradeoff, where V is a balance parameter between the cost optimality and service quality. Extensive simulations based on real-world traces are done to evaluate the performance of our algorithm. The results show that ODGWS saves about 5% average power cost compared with the baseline algorithms.}
}
@article{MUHAMMAD20222160,
title = {Comparative energy analysis of a laboratory building with different materials using eQUEST simulation software},
journal = {Materials Today: Proceedings},
volume = {52},
pages = {2160-2165},
year = {2022},
note = {International Conference on Smart and Sustainable Developments in Materials, Manufacturing and Energy Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.01.187},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322002206},
author = {Aqil Muhammad and Shashikantha Karinka},
keywords = {US-DOE-2, eQuest software, Building Energy, Modeling and Simulation, Room Insulators},
abstract = {In the context of global warming and climate change issues, the building sector is one of the major contributors to energy consumption and Green House Gas emissions. In view of this, the building concepts are shifting from the standard structures to the low-energy consumption structures through use of passive and active systems to save energy during the life expectancy of the buildingswithout compromising quality and comfort levels. The main aim of this study is to investigate the passive strategies (by using non-energy strategies) to achieve energy-efficient buildings. The work has been carried out for a laboratory building at Nitte, India. The laboratory with Galvanized Iron (GI) sheet roofing and concrete walls, due to which has an uncomfortable living space, has been used for the study. The US department of energy’s eQUEST (The Quick Energy Simulation Tool) software has been used for the simulations. The results show that it is possible to improve the human comfort to the acceptable standards by using the concept of economic thickness of insulation such as the use of 5 mm coconut pith insulation board on the wall and 5 mm jute fiber below the roof with 0.3 m air gap and also by using low emissivity glass window. The Temperature maintained is 75 to 80 ˚F (24 ˚C to 27 ˚C) for maximum number of hours and relative humidity is found to be in between 40 and 49 percent for maximum number of hours, both of which otherwise used to be much outside these comfortable limits.}
}
@article{MOHAMED2021106845,
title = {A novel fuzzy cloud stochastic framework for energy management of renewable microgrids based on maximum deployment of electric vehicles},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {129},
pages = {106845},
year = {2021},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2021.106845},
url = {https://www.sciencedirect.com/science/article/pii/S0142061521000855},
author = {Mohamed A. Mohamed and Heba M. Abdullah and Mohammed A. El-Meligy and Mohamed Sharaf and Ahmed T. Soliman and Ali Hajjiah},
keywords = {Renewable microgrid, Fuzzy cloud, Storage device, Electric vehicles, Honeybee mating algorithm, Energy policy},
abstract = {The high penetration of renewable energy sources in varied types along with the electric vehicles charging demand has created new challenges against the optimal operation and management of these systems. Therefore, this article attempts to formulate, model and operate the renewable microgrids considering different types of renewable sources including solar units, wind units and storages in the presence of electric vehicles. Due to the volatile and random nature of renewable sources and the charging demands by the electric vehicles, it is needed to find a solution for optimal control of these technologies. In this way, a new policy is devised for control of the charging demand in three different schemes, so-called the smart scheme, coordinated scheme and uncoordinated scheme. In order to capture the influence of uncertainties rooting from the renewable sources in the problem, a novel intelligent approach based on fuzzy cloud theory is used which generates an entropy-entropy concept for its task. Through this model, not only the uncertainty of the random variables is handled, but also the uncertainty of the probability density function (PDF) type is also deployed. In order to solve the problem, a swarm optimization based on honeybee mating algorithm (HMA) is proposed which is equipped by the polar search operators for making a symmetrical searching frame. The simulation results on a typical renewable microgrid test system advocate the quality and appropriate efficacy of the model.}
}
@article{LIN2021103335,
title = {A DAG-based cloud-fog layer architecture for distributed energy management in smart power grids in the presence of PHEVs},
journal = {Sustainable Cities and Society},
volume = {75},
pages = {103335},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103335},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721006119},
author = {Yubin Lin and Chenbing Cheng and Fen Xiao and Khalid Alsubhi and Hani Moaiteq Abdullah Aljahdali},
keywords = {Machine learning, Evolutionary computing, Smart power grid, Plug-in hybrid electric vehicle, DAG-based cloud-fog computing},
abstract = {In this paper, a new framework based on the directed acyclic graph (DAG) and distributed multi-layer cloud-fog computing to find the optimal energy management of the smart grids, considering high penetration of plug-in hybrid electric vehicles (PHEVs). The presented distributed structure lets neighboring agents make a consensus together. The uncertainties have been modeled according to the Monte Carlo simulations, due to wide usages of diverse renewable energy resources such as photovoltaic panels and wind turbines. Three diverse charging schemes have been considered in the smart grid test system which contains controlled, uncontrolled and smart chargings. The Whale Optimization Algorithm (WOA) has been used to solve the augmented Lagrangian function in each agent. The simulation results are shown that the suggested scheme is effective.}
}
@article{HSAYADNAVARD2022100995,
title = {A multi-objective approach for energy-efficient and reliable dynamic VM consolidation in cloud data centers},
journal = {Engineering Science and Technology, an International Journal},
volume = {26},
pages = {100995},
year = {2022},
issn = {2215-0986},
doi = {https://doi.org/10.1016/j.jestch.2021.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S221509862100104X},
author = {Monireh {H. Sayadnavard} and Abolfazl {Toroghi Haghighat} and Amir Masoud Rahmani},
keywords = {Cloud computing, VM consolidation, Reliability, Markov chain, Multi-objective optimization, -MOABC algorithm},
abstract = {The rapid growth of cloud computing in the last decade has led to an increasing concern about the energy requirement of cloud data centers. Dynamic virtual machine (VM) consolidation is an effective way to tackle this issue, where VMs are executed on as few physical machines (PMs) as possible. Meanwhile, VM placement must be performed strategically, by considering different factors of the available resources to optimal exploitation of them. Moreover, a major challenge is the system’s reliability degradation because of the high frequency of consolidation and placing VMs on unreliable PMs. In this paper, we address the problem by introducing a discrete-time Markov chain (DTMC) model to predict future resource usage. Using the DTMC model along with the reliability model of PMs leads to more accurate PMs categorization based on their status. Then, a multi-objective VM placement approach is proposed to achieve the optimal VMs to PMs mapping using the ε-dominance-based multi-objective artificial bee colony (ε-MOABC) algorithm which can efficiently balance the overall energy consumption, resource wastage, and the system reliability to meet SLA and QoS requirements. We have validated the effectiveness of our proposed approach by conducting a performance evaluation study using the CloudSim toolkit. Competitive analysis of the experimental results demonstrates that the proposed approach significantly improves energy consumption while avoiding the inefficient VM migrations.}
}
@article{SILVA202241,
title = {Computing the best-case energy complexity of satisfying assignments in monotone circuits},
journal = {Theoretical Computer Science},
volume = {932},
pages = {41-55},
year = {2022},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2022.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304397522004777},
author = {Janio Carlos Nascimento Silva and Uéverton S. Souza},
keywords = {Energy complexity, Monotone circuit, Planar, Genus, FPT},
abstract = {Measures of circuit complexity are usually analyzed to ensure the computation of Boolean functions with economy and efficiency. One of these measures is the energy complexity, which is related to the number of gates that output true in a circuit for an assignment. The idea behind energy complexity comes from the counting of ‘firing’ neurons in a natural neural network. The initial model is based on threshold circuits, but recent works also have analyzed the energy complexity of traditional Boolean circuits. In this work, we discuss the time complexity needed to compute the best-case energy complexity among satisfying assignments of a monotone Boolean circuit, and we call such a problem as MinECM+. In the MinECM+ problem, we are given a monotone Boolean circuit C, a positive integer k and asked to determine whether there is a satisfying assignment X for C such that EC(C,X)≤k, where EC(C,X) is the number of gates that output true in C according to the assignment X. We prove that MinECM+ is NP-complete even when the input monotone circuit is planar. Besides, we show that the problem is W[1]-hard but in XP when parameterized by the size of the solution. In contrast, we show that when the size of the solution and the genus of the input circuit are aggregated parameters, the MinECM+ problem becomes fixed-parameter tractable.}
}
@article{SAMARJI2022215,
title = {ESRA: Energy soaring-based routing algorithm for IoT applications in software-defined wireless sensor networks},
journal = {Egyptian Informatics Journal},
volume = {23},
number = {2},
pages = {215-224},
year = {2022},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2021.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1110866521000827},
author = {N. Samarji and M. Salamah},
keywords = {Controller Placement Problem, Energy Efficiency, Genetic Algorithm, Software-Defined Networking},
abstract = {Software-defined wireless sensor networking is an emerging networking architecture envisioned to play a critical role in the looming internet of things paradigm. Since energy is a scarce resource in wireless sensor networks, many energy-efficient routing algorithms were proposed to enhance the network lifetime. However, most of these algorithms lack network stability and reliability in the presence of dead nodes. This paper presents ESRA: Energy Soaring-based Routing Algorithm for IoT Applications in Software-Defined Wireless Sensor Networks, specifically for monitoring environment to address this shortcoming. The proposed ESRA algorithm efficiently selects the network cluster heads to be considered for solving the controller placement problem, intending to achieve network reliability and stability and enhance the network lifetime. The selection of controllers among the cluster heads is formulated as an NP-hard problem, considering the residual energy of the cluster heads, their spatial distance to the sink, and their load or density. To tackle this NP-hard problem, genetic algorithm is adopted to optimize the network lifetime, throughput, latency, and network reliability in the presence of different percentages of dead nodes. Simulation results showed that ESRA outperforms other three state-of-the art algorithms in terms of network lifetime and throughput by 15%, 20%, and 25%, in terms of energy savings by 10%, 20%, and 25%, and in terms of delay by 10%, 15%, and 20%. We also applied the proposed scheme on real networks adopted from the internet topology zoo, which showed promising results compared to other existing works.}
}
@article{SCHAEFER2021128853,
title = {An MCDM-based approach to evaluate the performance objectives for strategic management and development of Energy Cloud},
journal = {Journal of Cleaner Production},
volume = {320},
pages = {128853},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128853},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621030493},
author = {Jones Luís Schaefer and Julio Cezar Mairesse Siluk and Patrícia Stefan de Carvalho},
keywords = {Energy management, Energy cloud, Energy cloud management, Cloud computing, DEMATEL},
abstract = {The wide insertion of renewable energies from distributed sources, electric vehicles, energy storage systems, and the massive use of technologies such as Cloud Computing (CC), Big Data, and the Internet of Things, contributed to the development of energy management in CC environments. This way concerns are raised regarding performance objectives, which form the key points in the development of the Energy Cloud (EC). Thus, the purpose of this article is to identify what are the Fundamental Points of View (FPVs), connecting them to the challenges for EC, and structure a management model for the development and maturation of EC environments. FPVs were identified through a literature review and, with a survey with experts, their impact on each other was evaluated. The DEMATEL method was used to classify those FPVs as key, indirect, motivators, and independents. The results showed that cost, interoperability, and scalability are the key FPVs for the diffusion of EC. The article presents a management model with a broad scope which brings a managerial perspective for EC that can serve as a guiding thread for decisions related to technological, organizational, market, and regulatory aspects, being adapted according to the specifics and the breadth of the business ecosystem.}
}
@article{KAUR2021100496,
title = {Energy efficient cloud-assisted IoT-enabled architectural paradigm for drought prediction},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100496},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100496},
url = {https://www.sciencedirect.com/science/article/pii/S2210537920302183},
author = {Amandeep Kaur and Sandeep K. Sood},
keywords = {Internet of Things (ioT), Energy conserving, Fog computing, Sustainable sensor deployment, Cloud computing, Support Vector Machine, Kernel K-means clustering},
abstract = {Natural hazards like droughts give hard hit to economies, hydrological circle and human lives. Climatic fluctuations have exacerbate the chances of such calamities in future. The consequences can be abbreviated to some degree by predicting it and doing forward planning for such situations. Several drought indices evaluate the intensity of droughts but are incapable of covering most of the important drought evoking factors and lacks universality. The presented paper proposes an energy conserving cloud-assisted system for drought severity evaluation and prediction. The framework intelligently deploys sensor only to the locations that does not add redundant information. An energy conserving sleep scheduling algorithm is applied at fog layer that economize the power consumption but maintaining maximum accuracy of the system at the same time. Spatio-temporal analysis of wide spectrum of drought inciting factors by kernel based methods give the current and future degree of severity at the cloud layer. Kernel K-means clustering and Support Vector Regression are used to evaluate the situation for current and predict it for subsequent time periods respectively. Experimental results prove the system efficiency in assessing and predicting droughts with energy saving at the sensor node level.}
}
@article{SHAW2022101722,
title = {Applying Reinforcement Learning towards automating energy efficient virtual machine consolidation in cloud data centers},
journal = {Information Systems},
volume = {107},
pages = {101722},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101722},
url = {https://www.sciencedirect.com/science/article/pii/S030643792100003X},
author = {Rachael Shaw and Enda Howley and Enda Barrett},
keywords = {Energy efficiency, Virtual machine consolidation, Reinforcement learning, Artificial intelligence},
abstract = {Energy awareness presents an immense challenge for cloud computing infrastructure and the development of next generation data centers. Virtual Machine (VM) consolidation is one technique that can be harnessed to reduce energy related costs and environmental sustainability issues of data centers. In recent times intelligent learning approaches have proven to be effective for managing resources in cloud data centers. In this paper we explore the application of Reinforcement Learning (RL) algorithms for the VM consolidation problem demonstrating their capacity to optimize the distribution of virtual machines across the data center for improved resource management. Determining efficient policies in dynamic environments can be a difficult task, however, the proposed RL approach learns optimal behavior in the absence of complete knowledge due to its innate ability to reason under uncertainty. Using real workload data we provide a comparative analysis of popular RL algorithms including SARSA and Q-learning. Our empirical results demonstrate how our approach improves energy efficiency by 25% while also reducing service violations by 63% over the popular Power-Aware heuristic algorithm.}
}
@article{CHEN2022110359,
title = {Diagnostic accuracy of dual-energy computed tomography (DECT) to detect non-traumatic bone marrow edema: A systematic review and meta-analysis},
journal = {European Journal of Radiology},
volume = {153},
pages = {110359},
year = {2022},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2022.110359},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X22002091},
author = {Zheng Chen and Yingmin Chen and Hui Zhang and Xiuchuan Jia and Xuechao Zheng and Tianzi Zuo},
keywords = {Dual-energy computed tomography, Non-traumatic, Bone marrowedema, Meta-analysis, Systematic review},
abstract = {Purpose
This meta-analysis aimed to evaluate the diagnostic performance of dual-energy computed tomography (DECT) for detecting bone marrow edema (BME) in non-traumatic patients.
Methods
A systematic search of PubMed, EMBASE, and the Cochrane Library databases was performed up to October 1, 2021 for relevant original studies. Study details were extracted by two independent reviewers. A bivariate mixed-effects regression model was used to assess comprehensive diagnostic performance, and a subgroup analysis was performed to evaluate sources of variability. The risk of bias was evaluated with the QUADAS-2 tool.
Results
Ten studies involving 2463 regions, including hands, ankles, hips, and sacroiliac joints, were evaluated in this meta-analysis. Summary sensitivity, specificity, and area under the receiver operating characteristic curve values for BME were 88.4% (95% confidence interval (CI) 82.4%–92.5%), 96.1% (95% CI 94.4%–97.3%), and 0.98 (95% CI 96%–99%), respectively. The subgroup analysis showed that studies using a thicker slice (≥1 mm) had a higher sensitivity, and studies with older patients (≥60 years), fewer included patients (<40), and bones other than the pelvis had a higher specificity. Studies presented a generally low or unclear risk for bias and applicability concerns.
Conclusions
DECT has an excellent diagnostic performance for detecting BME in non-traumatic patients and may provide an alternative to magnetic resonance imaging (MRI) for the detection of non-traumatic BME in the future, especially when MRI is unavailable or contraindicated.}
}
@article{RAJPUT2022105569,
title = {Local bit-line shared pass-gate 8T SRAM based energy efficient and reliable In-Memory Computing architecture},
journal = {Microelectronics Journal},
volume = {129},
pages = {105569},
year = {2022},
issn = {0026-2692},
doi = {https://doi.org/10.1016/j.mejo.2022.105569},
url = {https://www.sciencedirect.com/science/article/pii/S0026269222001975},
author = {Anil Kumar Rajput and Manisha Pattanaik and Gaurav Kaushal},
keywords = {In-Memory Computing, Compute-disturbance, Half select issue, Energy efficiency, Static Random Access Memory (SRAM), Binary content-addressable memory (BCAM)},
abstract = {The In-Memory Computing (IMC) architecture based on Conventional 6T, 8T, and 10T SRAM suffers from compute disturbance, compute-failure, and half-select issues, which affect the reliability of In-Memory Boolean Computation (IMBC) operations. To overcome these problems, local bit-line Shared pass-gate Dual-Port 8T (SDP8T) SRAM-based IMC architecture is proposed to perform energy-efficient IMBC operations. The local bit-line shared pass-gate structure addresses the half select issues with a small area overhead and achieves higher array efficiency by incorporating the bit-interleave architecture. The virtual-VSS write-assist is used in SDP8T SRAM to improve the write-margin yield (μ−3σ) by 48.99%, and multi-VTH technique improves Decoupled Read-Margin (DRM) yield by 5.26% when compared to DP8T SRAM. The Dynamical Reset Word Line (DRWL) scheme is proposed to resolve the sneak current problem and make the proposed IMC architecture more resilient to compute disturbance at non-read decouple paths during IMBC operations. Further, the Reference-based Reconfigurable Sense Amplifier (RRCSA) scheme is proposed to achieve reliable (compute-failure free) sensing for IMBC on four operands simultaneously in a single cycle, normal read, and Binary Content Addressable Memory (BCAM) operations. The 4 Kb SRAM array is implemented in 65-nm CMOS technology to analyze the SDP8T-IMC architecture. The operating frequency of 1190 MHz and average-energy consumption of 18.56 fJ/bit are achieved during IMBC operation at 1V. For BCAM operations, it achieves 0.60 fJ/search/bit energy consumption in the worst case (i.e., all data mismatch) at 1V. Cumulatively, the proposed SDP8T-IMC architecture has the highest figure of merits than the recently reported IMC architecture.}
}
@article{SHOAIB202212211,
title = {Impact of thermal energy on MHD Casson fluid through a Forchheimer porous medium with inclined non-linear surface: A soft computing approach},
journal = {Alexandria Engineering Journal},
volume = {61},
number = {12},
pages = {12211-12228},
year = {2022},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822003878},
author = {Muhammad Shoaib and Mamoona Kausar and Kottakkaran Sooppy Nisar and Muhammad {Asif Zahoor Raja} and Ahmed Morsy},
keywords = {Heat source, Thermal radiation, Slip velocity, Porous medium, Casson fluid, Artificial neural networks, Soft computing},
abstract = {In this work, the influence of thermal energy in term of heat source, thermal radiation and chemical reaction on magneto hydrodynamic Casson fluid flow model (MHD-CFM) over a nonlinear slanted extending surface with slip velocity in a Forchheimer permeable medium is numerically studied using the Levenberg Marquardt methodology with backpropagated learning mechanism. It is valuable to evaluate the flow of Cason fluids based on materials (such as drilling muds, clay coatings, various suspensions and certain lubricating oils, polymeric melts, and a wide range of colloids) in the occurrence of heat transfer. Using efficient data, PDEs of (MHD-CFM) were converted to ordinary differential equations. These obtained non-linear ODEs are then rectified using the computational power of the Lobatto IIIA approach to obtain a dataset of Levenberg Marquardt algorithm based trained neural networks (LMA-TNN) for six scenarios of this presented model, which were graphically represented using nftool to obtain regression, efficiency, fit curve, error bars, and trained state analysis. The velocity, temperature, and concentration profiles were computed, and the findings were presented. Additionally, the skin friction coefficient, Nusselt number, & local Sherwood number are explored. The graphs show that when values of radiation parameter and the Forchheimer porous media parameter increase, the temperature of the plate drops. In the existence of a chemical process and a high Schmidt number, the concentration drops. The accuracy achieved in terms of relative error demonstrates the validity and significance of the solution process.}
}
@article{KUEH2023S98,
title = {Myocardial Characterisation Using Delayed Dual-Energy Cardiac Computed Tomography},
journal = {Heart, Lung and Circulation},
volume = {32},
pages = {S98},
year = {2023},
note = {Abstracts for the Cardiac Society of Australia and New Zealand Annual Scientific Meeting (New Zealand) 2023, 15 - 17 June 2023, Auckland, New Zealand},
issn = {1443-9506},
doi = {https://doi.org/10.1016/j.hlc.2023.04.262},
url = {https://www.sciencedirect.com/science/article/pii/S1443950623004390},
author = {S.-H. Kueh and J. Benatar and R. Stewart}
}